# -*- coding: utf-8 -*-
"""suc_transformers_serve.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i3JL9MKzLGx0QCE4pOpWLjtVx_jTWk7g
"""





"""https://huggingface.co/docs/transformers/main/en/serving"""

!pip install 'transformers[torch]'
#uv pip install 'transformers[torch]'

!python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('hugging face is the best'))"

[{'label': 'POSITIVE', 'score': 0.9998704791069031}]

!pip install git+https://github.com/huggingface/transformers

!transformers serve

!transformers serve -h

!transformers serve --force_model facebook/opt-125m



import requests
import json

# The URL of the local server
url = "http://localhost:8000/generate" # Or other endpoint depending on the task

# The data to send in the request (e.g., a prompt for text generation)
# This will vary depending on the specific model and task
data = {
    "inputs": "Once upon a time,",
    "parameters": {
        "max_new_tokens": 50
    }
}

try:
    # Send a POST request to the server
    response = requests.post(url, json=data)

    # Check if the request was successful
    if response.status_code == 200:
        # Print the response from the server
        print(response.json())
    else:
        print(f"Error: {response.status_code}")
        print(response.text)

except requests.exceptions.RequestException as e:
    print(f"Error sending request: {e}")

!pip install transformers[serving]

!pip install git+https://github.com/huggingface/transformers

!transformers-cli convert --model facebook/opt-125m --framework pt

!transformers chat Qwen/Qwen3-0.6B

import subprocess
import shlex

command = "transformers serve --force_model Qwen/Qwen3-0.6B"
process = subprocess.Popen(shlex.split(command))

# You can add code here to interact with the server
# For example, using the 'requests' library after the server has started

# To stop the server, you can use process.terminate() or process.kill()
# import time
# time.sleep(10) # Example: let the server run for 10 seconds
# process.terminate()

!curl -X POST http://localhost:8000/v1/chat/completions -H "Content-Type: application/json" -d '{"messages": [{"role": "system", "content": "hello"}], "temperature": 0.9, "max_tokens": 10, "stream": true, "model": "Qwen/Qwen3-0.6B"}'

!transformers chat localhost:8000 --model-name-or-path Qwen/Qwen3-0.6B

!curl -X POST http://localhost:8000/v1/chat/completions -H "Content-Type: application/json" -d '{"messages": [{"role": "system", "content": "hello"}], "temperature": 0.9, "max_tokens": 10, "stream": true, "model": "Qwen/Qwen3-0.6B"}'

!transformers chat --model-name-or-path Qwen/Qwen3-0.6B











"""### %%%%%%%%%%%%%%%%%%%%%%شغال"""

ف الترمنال
transformers serve

from openai import OpenAI

client = OpenAI(base_url="http://localhost:8000/v1", api_key="<random_string>")

completion = client.chat.completions.create(
    model="Qwen/Qwen2.5-0.5B-Instruct",
    messages=[
        {
            "role": "user",
            "content": "What is the Transformers library known for?"
        }
    ],
    stream=True
)

for chunk in completion:
    token = chunk.choices[0].delta.content
    if token:
        print(token, end='')

"""%%%%%%%%%%%%%%%"""

