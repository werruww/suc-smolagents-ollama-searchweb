{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fe73d0109e41494bae91e212dabffbad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_4f57ca14e56540149d878d9dd377ebc8"
          }
        },
        "d0f3960896924e2e86ade89b4468e6bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f830bd792f840aca2da59476563944a",
            "placeholder": "​",
            "style": "IPY_MODEL_37d762e6a04449f7bf33356802a25aae",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "d46432cd0a5a464db927b106df6c9cb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_5ffd819f0855436892389af98b220694",
            "placeholder": "​",
            "style": "IPY_MODEL_f2ef61f8bcc245cb93ea5b4174a57002",
            "value": ""
          }
        },
        "9eaf075d76994d6a881851c034fed55a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_a252a7b671f440d98b5ef1ab9bad1b1b",
            "style": "IPY_MODEL_65d801176295482c891e0def56fc127a",
            "value": true
          }
        },
        "f1796191a6904f89949493e87acd34a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_242cab9c69f8434594e7954818f5be94",
            "style": "IPY_MODEL_61c06fa4914a4e73b2c387d3263dfedc",
            "tooltip": ""
          }
        },
        "6716f6ba3d414711ac344f5953e94b6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16c6c1a42ca64751aeb754af195cd0cc",
            "placeholder": "​",
            "style": "IPY_MODEL_d4e882bbc1a84c62a7ec85cd3cfd70e6",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "4f57ca14e56540149d878d9dd377ebc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "7f830bd792f840aca2da59476563944a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37d762e6a04449f7bf33356802a25aae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ffd819f0855436892389af98b220694": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2ef61f8bcc245cb93ea5b4174a57002": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a252a7b671f440d98b5ef1ab9bad1b1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65d801176295482c891e0def56fc127a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "242cab9c69f8434594e7954818f5be94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61c06fa4914a4e73b2c387d3263dfedc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "16c6c1a42ca64751aeb754af195cd0cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4e882bbc1a84c62a7ec85cd3cfd70e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40cbe9f91d454517a2f5b9893f9e8b2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d33ecc5c3bd14bdfa45e83f01fb56c0b",
            "placeholder": "​",
            "style": "IPY_MODEL_d434a75f102746398d5e354176267620",
            "value": "Connecting..."
          }
        },
        "d33ecc5c3bd14bdfa45e83f01fb56c0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d434a75f102746398d5e354176267620": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "568d07e1d4f046cfa0db1ff0d9943dfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c528273683034d798b495087486e1c0e",
              "IPY_MODEL_4c3aff1c083b4ab6b0f4592c1a2a26c5",
              "IPY_MODEL_a107ac91a9ce4a81b5659538f38e8afa"
            ],
            "layout": "IPY_MODEL_bb29b09f34ea4cdd9aae5a956dd474c0"
          }
        },
        "c528273683034d798b495087486e1c0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d58999b0e15489c99c9e8e730b6b6ea",
            "placeholder": "​",
            "style": "IPY_MODEL_f6a8812784e74b69b518629555fdb47f",
            "value": "Processing Files (0 / 1)                : 100%"
          }
        },
        "4c3aff1c083b4ab6b0f4592c1a2a26c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ee145396dc64c589009fb1fc81adc96",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a72cae119831444db2a2e9ae8784bb1c",
            "value": 1
          }
        },
        "a107ac91a9ce4a81b5659538f38e8afa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3f31f1610784f2b9f9befcc211eb7a3",
            "placeholder": "​",
            "style": "IPY_MODEL_12803aa60f994727851638ec26ea9d7b",
            "value": " 14.5GB / 14.5GB, 99.2MB/s  "
          }
        },
        "bb29b09f34ea4cdd9aae5a956dd474c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d58999b0e15489c99c9e8e730b6b6ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6a8812784e74b69b518629555fdb47f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ee145396dc64c589009fb1fc81adc96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a72cae119831444db2a2e9ae8784bb1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d3f31f1610784f2b9f9befcc211eb7a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12803aa60f994727851638ec26ea9d7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b85ac5e8173c4535991450c7985f284f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_907629901ce14622805bd041a141884d",
              "IPY_MODEL_8d0caf4bdee646e99bc3349a6c2a7f2f",
              "IPY_MODEL_a18e6294c58b42d3afe32e9fd2fc5b36"
            ],
            "layout": "IPY_MODEL_f25f4db80db94dfe8bcac353e0cc3050"
          }
        },
        "907629901ce14622805bd041a141884d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbddf3f81947479090ae5ed856e2c095",
            "placeholder": "​",
            "style": "IPY_MODEL_7a2f094dfb78401aabc9bec54f2896d1",
            "value": "New Data Upload                         : 100%"
          }
        },
        "8d0caf4bdee646e99bc3349a6c2a7f2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_470311e80211489fabc3967d877fa8b9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d008ec433cf44541b63ffd8d45fa7070",
            "value": 1
          }
        },
        "a18e6294c58b42d3afe32e9fd2fc5b36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_195fc0f6b6b6459e8c279a0297b9d1e2",
            "placeholder": "​",
            "style": "IPY_MODEL_19f476c3a3654203b42f862efcd9ab04",
            "value": "  195MB /  195MB, 6.04MB/s  "
          }
        },
        "f25f4db80db94dfe8bcac353e0cc3050": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbddf3f81947479090ae5ed856e2c095": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a2f094dfb78401aabc9bec54f2896d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "470311e80211489fabc3967d877fa8b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "d008ec433cf44541b63ffd8d45fa7070": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "195fc0f6b6b6459e8c279a0297b9d1e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19f476c3a3654203b42f862efcd9ab04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7dc9083b47ca4fd3be40f42d498a33da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_574129adf4694852a6b29abd21dff802",
              "IPY_MODEL_ce530f17e8d2403e80f8aa3861b30f33",
              "IPY_MODEL_6d881a8da60d45cd8b8514d392423b11"
            ],
            "layout": "IPY_MODEL_ca29f44c9723409d96006d5c0f002521"
          }
        },
        "574129adf4694852a6b29abd21dff802": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43f8b9bbb1c647349f6aed3b13ea5377",
            "placeholder": "​",
            "style": "IPY_MODEL_ed8b63e763fb4bc9b4dddae9b71dcaa5",
            "value": "  .../Mistral-7B-Instruct-v0.3-bf16.gguf: 100%"
          }
        },
        "ce530f17e8d2403e80f8aa3861b30f33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_124a040fef964e2c818fa9f09025ca64",
            "max": 14497337280,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_323f915406a848bfa33f01c04319593f",
            "value": 14485278656
          }
        },
        "6d881a8da60d45cd8b8514d392423b11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09da28009f694ae3a5af63d6968dbfca",
            "placeholder": "​",
            "style": "IPY_MODEL_82e147850a014f77bd5c2291608ccf60",
            "value": " 14.5GB / 14.5GB            "
          }
        },
        "ca29f44c9723409d96006d5c0f002521": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43f8b9bbb1c647349f6aed3b13ea5377": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed8b63e763fb4bc9b4dddae9b71dcaa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "124a040fef964e2c818fa9f09025ca64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "323f915406a848bfa33f01c04319593f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "09da28009f694ae3a5af63d6968dbfca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82e147850a014f77bd5c2291608ccf60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "198a66ea2d40420b9dd8c091a2a25e89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9d9d8cbbe8e4cfd9a0d4eef9461efed",
              "IPY_MODEL_5d91c0987ec14356be9678c41573444a",
              "IPY_MODEL_7cd932a8f7714de9b461c5cad9f150a2"
            ],
            "layout": "IPY_MODEL_cbae615bd1c44c439f147096247a9e49"
          }
        },
        "f9d9d8cbbe8e4cfd9a0d4eef9461efed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d44ec3189ccc45158967821b6c22799b",
            "placeholder": "​",
            "style": "IPY_MODEL_33307f202a134e73aa48b6a40db1fc77",
            "value": "Processing Files (3 / 3)                : 100%"
          }
        },
        "5d91c0987ec14356be9678c41573444a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4096f3f39c54888ae4b938d028dabc8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8fa9eadb8e7d48ce8fa7212ce5eee6de",
            "value": 1
          }
        },
        "7cd932a8f7714de9b461c5cad9f150a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21646d243fd645709e5d3f57d2eb4b99",
            "placeholder": "​",
            "style": "IPY_MODEL_471ab2fe3384483995d186af4d108d30",
            "value": " 14.5GB / 14.5GB,  107MB/s  "
          }
        },
        "cbae615bd1c44c439f147096247a9e49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d44ec3189ccc45158967821b6c22799b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33307f202a134e73aa48b6a40db1fc77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4096f3f39c54888ae4b938d028dabc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8fa9eadb8e7d48ce8fa7212ce5eee6de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "21646d243fd645709e5d3f57d2eb4b99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "471ab2fe3384483995d186af4d108d30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27a128514d2a441ba02dfc127eb97b63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ed957d349214fa0925ec09175a6a550",
              "IPY_MODEL_bf5f4e4be09143ab8b1f27c73ca6616d",
              "IPY_MODEL_59d9434e46dd442cbb07ee0614b278cf"
            ],
            "layout": "IPY_MODEL_440bbd6c209f4bcbae0c869d9761452b"
          }
        },
        "2ed957d349214fa0925ec09175a6a550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41be7cbd31364ff2bd85a5fd02aa22d2",
            "placeholder": "​",
            "style": "IPY_MODEL_b3dc66f7b5da4bc3954cb287cb6d6cf1",
            "value": "New Data Upload                         : 100%"
          }
        },
        "bf5f4e4be09143ab8b1f27c73ca6616d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d50a816b99f4909b05c99984c82c22c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_783603f3e19f407ab801e9b0768dae71",
            "value": 1
          }
        },
        "59d9434e46dd442cbb07ee0614b278cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ebbec5cbebb4cc799316acc8b87a0d9",
            "placeholder": "​",
            "style": "IPY_MODEL_267dd506b6764e0ebb61dce18fb4aa84",
            "value": "  704kB /  704kB, 70.4kB/s  "
          }
        },
        "440bbd6c209f4bcbae0c869d9761452b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41be7cbd31364ff2bd85a5fd02aa22d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3dc66f7b5da4bc3954cb287cb6d6cf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d50a816b99f4909b05c99984c82c22c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "783603f3e19f407ab801e9b0768dae71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3ebbec5cbebb4cc799316acc8b87a0d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "267dd506b6764e0ebb61dce18fb4aa84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e845db03803b4eee86ff42998b983973": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d0820382a1354af2b58f5be2114a9dbe",
              "IPY_MODEL_d045db4be798425987bc05439ae29546",
              "IPY_MODEL_d39a33d063e24551b795f1402c95b6ca"
            ],
            "layout": "IPY_MODEL_3efb0e01abe24059918491a04b78d536"
          }
        },
        "d0820382a1354af2b58f5be2114a9dbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1496807481a44899de3e2b293ea12e0",
            "placeholder": "​",
            "style": "IPY_MODEL_a743766b157f40dc88cf3d55411ce89b",
            "value": "  ...bf16-split.gguf-00001-of-00003.gguf: 100%"
          }
        },
        "d045db4be798425987bc05439ae29546": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b18b988c4adf492a9101d98ae084c648",
            "max": 4983939840,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1bce2d793124bc2a60f877525ecbe60",
            "value": 4983939840
          }
        },
        "d39a33d063e24551b795f1402c95b6ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11ee59d85f3146d284c2046d8121ae05",
            "placeholder": "​",
            "style": "IPY_MODEL_ef7acae7af0e4d75a9098ff815f7ede2",
            "value": " 4.98GB / 4.98GB            "
          }
        },
        "3efb0e01abe24059918491a04b78d536": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1496807481a44899de3e2b293ea12e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a743766b157f40dc88cf3d55411ce89b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b18b988c4adf492a9101d98ae084c648": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1bce2d793124bc2a60f877525ecbe60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "11ee59d85f3146d284c2046d8121ae05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef7acae7af0e4d75a9098ff815f7ede2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1bb7736e886541eeb5fff077a8cc415c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6712b6bdd3cb4b0aa90d1834f99b77a4",
              "IPY_MODEL_af5cfcf1d20d46dc945fb4eb4592545e",
              "IPY_MODEL_727e2062a2714f64a0dca5f76f6c4f9b"
            ],
            "layout": "IPY_MODEL_6ff7776a70ab420f89e539a20cc9d4be"
          }
        },
        "6712b6bdd3cb4b0aa90d1834f99b77a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29f9f01278a948b29732b104262f0257",
            "placeholder": "​",
            "style": "IPY_MODEL_bfd607f5d54f4e9484b7eff0f2c44872",
            "value": "  ...bf16-split.gguf-00003-of-00003.gguf: 100%"
          }
        },
        "af5cfcf1d20d46dc945fb4eb4592545e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85327870d8a9455c9a952f5d8442b957",
            "max": 4597306912,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_064464417d754fcfb45f3b4b495fb416",
            "value": 4597306912
          }
        },
        "727e2062a2714f64a0dca5f76f6c4f9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_635b297eea104754860029f054b9d00f",
            "placeholder": "​",
            "style": "IPY_MODEL_41c5b24cf90f4824ab8812114944690d",
            "value": " 4.60GB / 4.60GB            "
          }
        },
        "6ff7776a70ab420f89e539a20cc9d4be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29f9f01278a948b29732b104262f0257": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfd607f5d54f4e9484b7eff0f2c44872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85327870d8a9455c9a952f5d8442b957": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "064464417d754fcfb45f3b4b495fb416": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "635b297eea104754860029f054b9d00f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41c5b24cf90f4824ab8812114944690d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "622437811763467b9d12f05410245b39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e432de25bf44f1092ed3c06dbe02f53",
              "IPY_MODEL_51ca705eaefb4edc860d6eb379dee768",
              "IPY_MODEL_41c36992b6274536a0983c88551bb74b"
            ],
            "layout": "IPY_MODEL_a73f8fd2c86e4f05812bea261e2404e9"
          }
        },
        "4e432de25bf44f1092ed3c06dbe02f53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_479c2bb199e04ddebcf35ae741ac7e7d",
            "placeholder": "​",
            "style": "IPY_MODEL_2c4717c22ec34daba0bad3bfa29efa7a",
            "value": "  ...bf16-split.gguf-00002-of-00003.gguf: 100%"
          }
        },
        "51ca705eaefb4edc860d6eb379dee768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d718aa3fcc6a4b8cb4f6c97153fc49c2",
            "max": 4916090848,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8688b431e10b40e7a55d3e5a9174be99",
            "value": 4916090848
          }
        },
        "41c36992b6274536a0983c88551bb74b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fea1d6aac394033bdcba72ba1012202",
            "placeholder": "​",
            "style": "IPY_MODEL_3dc8acc54a354ff1b00fe53de50aac1d",
            "value": " 4.92GB / 4.92GB            "
          }
        },
        "a73f8fd2c86e4f05812bea261e2404e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "479c2bb199e04ddebcf35ae741ac7e7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c4717c22ec34daba0bad3bfa29efa7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d718aa3fcc6a4b8cb4f6c97153fc49c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8688b431e10b40e7a55d3e5a9174be99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9fea1d6aac394033bdcba72ba1012202": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dc8acc54a354ff1b00fe53de50aac1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6-c6mkcN8e8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUkz3r5MN8__",
        "outputId": "e3264084-91e9-4c92-ff83-ed0710d70150"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.15.tar.gz (50.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.14.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.15-cp311-cp311-linux_x86_64.whl size=4397839 sha256=bf30e35991e8fd165b0c4e6f8f53d007f1f956cb164bdd8bc80e1b67ad13f2ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/62/22/63039c8c4c8da7ff68e81b8357e64c199faff9df9a2b5e5e3b\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install llama-cpp-python\n",
        "\n",
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama.from_pretrained(\n",
        "\trepo_id=\"rakmik/Mistral-7B-Instruct-v0.3-Q8_0-GGUF\",\n",
        "\tfilename=\"mistral-7b-instruct-v0.3-q8_0-00001-of-00002.gguf\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "FeQkeVr8N9Cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.create_chat_completion(\n",
        "\tmessages = \"No input example has been defined for this model task.\"\n",
        ")"
      ],
      "metadata": {
        "id": "5KfVwZ8IOB1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/rakmik/Mistral-7B-Instruct-v0.3-Q8_0-GGUF/resolve/main/mistral-7b-instruct-v0.3-q8_0-00001-of-00002.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcIfFa3QOeVG",
        "outputId": "d609ba85-6499-40b5-d985-31aed59abeec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-09 19:09:16--  https://huggingface.co/rakmik/Mistral-7B-Instruct-v0.3-Q8_0-GGUF/resolve/main/mistral-7b-instruct-v0.3-q8_0-00001-of-00002.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.88, 18.172.134.4, 18.172.134.24, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/689799a58ce64adb4e678c88/085e026cc70306e56356edd579d6e269862852ae7ec4540129d7f9b4ac5ef01f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250809%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250809T190916Z&X-Amz-Expires=3600&X-Amz-Signature=d0755c0aef0c4662cc07af8d81f8e14d6e8eb3806c6cdcb4d891e5166c560d90&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27mistral-7b-instruct-v0.3-q8_0-00001-of-00002.gguf%3B+filename%3D%22mistral-7b-instruct-v0.3-q8_0-00001-of-00002.gguf%22%3B&x-id=GetObject&Expires=1754770156&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NDc3MDE1Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODk3OTlhNThjZTY0YWRiNGU2NzhjODgvMDg1ZTAyNmNjNzAzMDZlNTYzNTZlZGQ1NzlkNmUyNjk4NjI4NTJhZTdlYzQ1NDAxMjlkN2Y5YjRhYzVlZjAxZioifV19&Signature=ZQ270VvBTLFyNwyThmV4NbFJiXuzcdl6DH%7EyQ9R-IC8XeLbQfbj3q8rvsGZCf4IbWkL%7E67ypQihdT2rZWy3NurVp5vZHrqLDQeai-M9FBpk9So44XhjtZWzqx5ZGTbI07O91y7Z6CILsdvRoZXvDkWTP4flZiXOiFK8QJ-hurKR5f2yXiYEq85LM%7EtNkwFGCGfrji1B41vo372CW2k2Mh8swwdBzQvbfv3ADdG4sW2YbDHxDJEo2qb0Sd3XsXH2jYHX6YmzYDjsZABOVcxDGqj7inm0VJMfdfjsfvsV1IKCe6YcmIdM0wLiwP8QLlUCjwJHbUHp8mfi62RkEhWw37A__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-08-09 19:09:16--  https://cas-bridge.xethub.hf.co/xet-bridge-us/689799a58ce64adb4e678c88/085e026cc70306e56356edd579d6e269862852ae7ec4540129d7f9b4ac5ef01f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250809%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250809T190916Z&X-Amz-Expires=3600&X-Amz-Signature=d0755c0aef0c4662cc07af8d81f8e14d6e8eb3806c6cdcb4d891e5166c560d90&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27mistral-7b-instruct-v0.3-q8_0-00001-of-00002.gguf%3B+filename%3D%22mistral-7b-instruct-v0.3-q8_0-00001-of-00002.gguf%22%3B&x-id=GetObject&Expires=1754770156&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NDc3MDE1Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODk3OTlhNThjZTY0YWRiNGU2NzhjODgvMDg1ZTAyNmNjNzAzMDZlNTYzNTZlZGQ1NzlkNmUyNjk4NjI4NTJhZTdlYzQ1NDAxMjlkN2Y5YjRhYzVlZjAxZioifV19&Signature=ZQ270VvBTLFyNwyThmV4NbFJiXuzcdl6DH%7EyQ9R-IC8XeLbQfbj3q8rvsGZCf4IbWkL%7E67ypQihdT2rZWy3NurVp5vZHrqLDQeai-M9FBpk9So44XhjtZWzqx5ZGTbI07O91y7Z6CILsdvRoZXvDkWTP4flZiXOiFK8QJ-hurKR5f2yXiYEq85LM%7EtNkwFGCGfrji1B41vo372CW2k2Mh8swwdBzQvbfv3ADdG4sW2YbDHxDJEo2qb0Sd3XsXH2jYHX6YmzYDjsZABOVcxDGqj7inm0VJMfdfjsfvsV1IKCe6YcmIdM0wLiwP8QLlUCjwJHbUHp8mfi62RkEhWw37A__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 3.162.163.110, 3.162.163.68, 3.162.163.41, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|3.162.163.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4965927040 (4.6G)\n",
            "Saving to: ‘mistral-7b-instruct-v0.3-q8_0-00001-of-00002.gguf’\n",
            "\n",
            "mistral-7b-instruct 100%[===================>]   4.62G  61.5MB/s    in 89s     \n",
            "\n",
            "2025-08-09 19:10:45 (53.3 MB/s) - ‘mistral-7b-instruct-v0.3-q8_0-00001-of-00002.gguf’ saved [4965927040/4965927040]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/rakmik/Mistral-7B-Instruct-v0.3-Q8_0-GGUF/resolve/main/mistral-7b-instruct-v0.3-q8_0-00002-of-00002.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1vn3Ms_OirO",
        "outputId": "0b4e2838-445a-43b4-aa68-e0511bf0b23d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-09 19:10:47--  https://huggingface.co/rakmik/Mistral-7B-Instruct-v0.3-Q8_0-GGUF/resolve/main/mistral-7b-instruct-v0.3-q8_0-00002-of-00002.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.88, 18.172.134.124, 18.172.134.4, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/689799a58ce64adb4e678c88/101e8fff800e8d4d4b0592fa7daf1e4e24b04c632a2c1b35e2c635dbedaa7392?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250809%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250809T191047Z&X-Amz-Expires=3600&X-Amz-Signature=cb20ee2f64b48599b7f7ae9aca556757ea71463c807c0fb38d4618c1855d219d&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27mistral-7b-instruct-v0.3-q8_0-00002-of-00002.gguf%3B+filename%3D%22mistral-7b-instruct-v0.3-q8_0-00002-of-00002.gguf%22%3B&x-id=GetObject&Expires=1754770247&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NDc3MDI0N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODk3OTlhNThjZTY0YWRiNGU2NzhjODgvMTAxZThmZmY4MDBlOGQ0ZDRiMDU5MmZhN2RhZjFlNGUyNGIwNGM2MzJhMmMxYjM1ZTJjNjM1ZGJlZGFhNzM5MioifV19&Signature=caFvHJGKrgT8sf43tCFPWF%7Es-BYOXdT1LUyJhf5Dnn4ltaAG2Ky8TP0b%7EwKBm0Y5WNe3Ggj4vTn2zUnfWMXPm7pH1QZX2SkUA-NImN3UXg8Cu3R%7EfbpIm-11nUxkjxqmrdUkGArL4UrdvtG2497AzJzjhUyoPbxYify8QBHhuXd0%7E4Xxa41T%7E4gXq4xTniJnlAikvWdXD6fKV54eeGJwzhtnfyN6849N8Qy8aSaElp1ENUkierc-dFQGRQ1yIJCmPWgXr-QC7KBx69gutTQT2IP2KJZ26wklYea%7ETHSXX43FWx856zfLk2TC4STYGlb01YgjdIJVa0c36dY5GWDOXA__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-08-09 19:10:47--  https://cas-bridge.xethub.hf.co/xet-bridge-us/689799a58ce64adb4e678c88/101e8fff800e8d4d4b0592fa7daf1e4e24b04c632a2c1b35e2c635dbedaa7392?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250809%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250809T191047Z&X-Amz-Expires=3600&X-Amz-Signature=cb20ee2f64b48599b7f7ae9aca556757ea71463c807c0fb38d4618c1855d219d&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27mistral-7b-instruct-v0.3-q8_0-00002-of-00002.gguf%3B+filename%3D%22mistral-7b-instruct-v0.3-q8_0-00002-of-00002.gguf%22%3B&x-id=GetObject&Expires=1754770247&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NDc3MDI0N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODk3OTlhNThjZTY0YWRiNGU2NzhjODgvMTAxZThmZmY4MDBlOGQ0ZDRiMDU5MmZhN2RhZjFlNGUyNGIwNGM2MzJhMmMxYjM1ZTJjNjM1ZGJlZGFhNzM5MioifV19&Signature=caFvHJGKrgT8sf43tCFPWF%7Es-BYOXdT1LUyJhf5Dnn4ltaAG2Ky8TP0b%7EwKBm0Y5WNe3Ggj4vTn2zUnfWMXPm7pH1QZX2SkUA-NImN3UXg8Cu3R%7EfbpIm-11nUxkjxqmrdUkGArL4UrdvtG2497AzJzjhUyoPbxYify8QBHhuXd0%7E4Xxa41T%7E4gXq4xTniJnlAikvWdXD6fKV54eeGJwzhtnfyN6849N8Qy8aSaElp1ENUkierc-dFQGRQ1yIJCmPWgXr-QC7KBx69gutTQT2IP2KJZ26wklYea%7ETHSXX43FWx856zfLk2TC4STYGlb01YgjdIJVa0c36dY5GWDOXA__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 3.162.163.41, 3.162.163.68, 3.162.163.2, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|3.162.163.41|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2736642176 (2.5G)\n",
            "Saving to: ‘mistral-7b-instruct-v0.3-q8_0-00002-of-00002.gguf’\n",
            "\n",
            "mistral-7b-instruct 100%[===================>]   2.55G  62.4MB/s    in 45s     \n",
            "\n",
            "2025-08-09 19:11:33 (57.9 MB/s) - ‘mistral-7b-instruct-v0.3-q8_0-00002-of-00002.gguf’ saved [2736642176/2736642176]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(model_path=\"/content/mistral-7b-instruct-v0.3-q8_0-00001-of-00002.gguf\")\n",
        "response = llm.create_chat_completion(\n",
        "  messages=[\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"how big is the sky\"\n",
        "    }\n",
        "])\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TV_Xe3o2Otqo",
        "outputId": "2ec25117-0062-44b2-afeb-dff143a9f9b5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: additional 1 GGUFs metadata loaded.\n",
            "llama_model_loader: loaded meta data with 42 key-value pairs and 291 tensors from /content/mistral-7b-instruct-v0.3-q8_0-00001-of-00002.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Mistral 7B Instruct v0.3\n",
            "llama_model_loader: - kv   3:                            general.version str              = v0.3\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 7B\n",
            "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
            "llama_model_loader: - kv   8:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   9:                  general.base_model.0.name str              = Mistral 7B v0.3\n",
            "llama_model_loader: - kv  10:               general.base_model.0.version str              = v0.3\n",
            "llama_model_loader: - kv  11:          general.base_model.0.organization str              = Mistralai\n",
            "llama_model_loader: - kv  12:              general.base_model.0.repo_url str              = https://huggingface.co/mistralai/Mist...\n",
            "llama_model_loader: - kv  13:                               general.tags arr[str,2]       = [\"vllm\", \"mistral-common\"]\n",
            "llama_model_loader: - kv  14:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  15:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv  16:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  17:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  18:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  19:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  20:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  21:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  22:                           llama.vocab_size u32              = 32768\n",
            "llama_model_loader: - kv  23:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\n",
            "llama_model_loader: - kv  27:                      tokenizer.ggml.scores arr[f32,32768]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  28:                  tokenizer.ggml.token_type arr[i32,32768]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  29:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  31:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  33:               tokenizer.ggml.add_sep_token bool             = false\n",
            "llama_model_loader: - kv  34:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  35:                    tokenizer.chat_template str              = {%- if messages[0][\"role\"] == \"system...\n",
            "llama_model_loader: - kv  36:            tokenizer.ggml.add_space_prefix bool             = true\n",
            "llama_model_loader: - kv  37:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  38:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  39:                                   split.no u16              = 0\n",
            "llama_model_loader: - kv  40:                        split.tensors.count i32              = 291\n",
            "llama_model_loader: - kv  41:                                split.count u16              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q8_0:  226 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q8_0\n",
            "print_info: file size   = 7.17 GiB (8.50 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control token:    468 '[control_466]' is not marked as EOG\n",
            "load: control token:    464 '[control_462]' is not marked as EOG\n",
            "load: control token:    727 '[control_725]' is not marked as EOG\n",
            "load: control token:    343 '[control_341]' is not marked as EOG\n",
            "load: control token:    603 '[control_601]' is not marked as EOG\n",
            "load: control token:    332 '[control_330]' is not marked as EOG\n",
            "load: control token:     34 '[control_32]' is not marked as EOG\n",
            "load: control token:    412 '[control_410]' is not marked as EOG\n",
            "load: control token:    675 '[control_673]' is not marked as EOG\n",
            "load: control token:    177 '[control_175]' is not marked as EOG\n",
            "load: control token:    434 '[control_432]' is not marked as EOG\n",
            "load: control token:    166 '[control_164]' is not marked as EOG\n",
            "load: control token:    199 '[control_197]' is not marked as EOG\n",
            "load: control token:    252 '[control_250]' is not marked as EOG\n",
            "load: control token:    742 '[control_740]' is not marked as EOG\n",
            "load: control token:    611 '[control_609]' is not marked as EOG\n",
            "load: control token:    573 '[control_571]' is not marked as EOG\n",
            "load: control token:    455 '[control_453]' is not marked as EOG\n",
            "load: control token:    701 '[control_699]' is not marked as EOG\n",
            "load: control token:    583 '[control_581]' is not marked as EOG\n",
            "load: control token:    433 '[control_431]' is not marked as EOG\n",
            "load: control token:    301 '[control_299]' is not marked as EOG\n",
            "load: control token:    617 '[control_615]' is not marked as EOG\n",
            "load: control token:    536 '[control_534]' is not marked as EOG\n",
            "load: control token:    285 '[control_283]' is not marked as EOG\n",
            "load: control token:    376 '[control_374]' is not marked as EOG\n",
            "load: control token:     79 '[control_77]' is not marked as EOG\n",
            "load: control token:    671 '[control_669]' is not marked as EOG\n",
            "load: control token:    391 '[control_389]' is not marked as EOG\n",
            "load: control token:    174 '[control_172]' is not marked as EOG\n",
            "load: control token:    708 '[control_706]' is not marked as EOG\n",
            "load: control token:    729 '[control_727]' is not marked as EOG\n",
            "load: control token:    311 '[control_309]' is not marked as EOG\n",
            "load: control token:    384 '[control_382]' is not marked as EOG\n",
            "load: control token:    766 '[control_764]' is not marked as EOG\n",
            "load: control token:     94 '[control_92]' is not marked as EOG\n",
            "load: control token:    304 '[control_302]' is not marked as EOG\n",
            "load: control token:    197 '[control_195]' is not marked as EOG\n",
            "load: control token:     17 '[control_15]' is not marked as EOG\n",
            "load: control token:    604 '[control_602]' is not marked as EOG\n",
            "load: control token:    576 '[control_574]' is not marked as EOG\n",
            "load: control token:    359 '[control_357]' is not marked as EOG\n",
            "load: control token:    302 '[control_300]' is not marked as EOG\n",
            "load: control token:    453 '[control_451]' is not marked as EOG\n",
            "load: control token:    374 '[control_372]' is not marked as EOG\n",
            "load: control token:    207 '[control_205]' is not marked as EOG\n",
            "load: control token:    739 '[control_737]' is not marked as EOG\n",
            "load: control token:    689 '[control_687]' is not marked as EOG\n",
            "load: control token:    335 '[control_333]' is not marked as EOG\n",
            "load: control token:    485 '[control_483]' is not marked as EOG\n",
            "load: control token:    283 '[control_281]' is not marked as EOG\n",
            "load: control token:    249 '[control_247]' is not marked as EOG\n",
            "load: control token:    752 '[control_750]' is not marked as EOG\n",
            "load: control token:    330 '[control_328]' is not marked as EOG\n",
            "load: control token:    635 '[control_633]' is not marked as EOG\n",
            "load: control token:    161 '[control_159]' is not marked as EOG\n",
            "load: control token:    731 '[control_729]' is not marked as EOG\n",
            "load: control token:    631 '[control_629]' is not marked as EOG\n",
            "load: control token:    323 '[control_321]' is not marked as EOG\n",
            "load: control token:     48 '[control_46]' is not marked as EOG\n",
            "load: control token:    544 '[control_542]' is not marked as EOG\n",
            "load: control token:    687 '[control_685]' is not marked as EOG\n",
            "load: control token:    463 '[control_461]' is not marked as EOG\n",
            "load: control token:     47 '[control_45]' is not marked as EOG\n",
            "load: control token:     86 '[control_84]' is not marked as EOG\n",
            "load: control token:    331 '[control_329]' is not marked as EOG\n",
            "load: control token:     13 '[control_11]' is not marked as EOG\n",
            "load: control token:    760 '[control_758]' is not marked as EOG\n",
            "load: control token:    612 '[control_610]' is not marked as EOG\n",
            "load: control token:    473 '[control_471]' is not marked as EOG\n",
            "load: control token:    601 '[control_599]' is not marked as EOG\n",
            "load: control token:    146 '[control_144]' is not marked as EOG\n",
            "load: control token:    734 '[control_732]' is not marked as EOG\n",
            "load: control token:    141 '[control_139]' is not marked as EOG\n",
            "load: control token:    138 '[control_136]' is not marked as EOG\n",
            "load: control token:     61 '[control_59]' is not marked as EOG\n",
            "load: control token:     46 '[control_44]' is not marked as EOG\n",
            "load: control token:    173 '[control_171]' is not marked as EOG\n",
            "load: control token:    367 '[control_365]' is not marked as EOG\n",
            "load: control token:    417 '[control_415]' is not marked as EOG\n",
            "load: control token:    740 '[control_738]' is not marked as EOG\n",
            "load: control token:    216 '[control_214]' is not marked as EOG\n",
            "load: control token:    498 '[control_496]' is not marked as EOG\n",
            "load: control token:    512 '[control_510]' is not marked as EOG\n",
            "load: control token:    122 '[control_120]' is not marked as EOG\n",
            "load: control token:    246 '[control_244]' is not marked as EOG\n",
            "load: control token:    745 '[control_743]' is not marked as EOG\n",
            "load: control token:    451 '[control_449]' is not marked as EOG\n",
            "load: control token:    280 '[control_278]' is not marked as EOG\n",
            "load: control token:    654 '[control_652]' is not marked as EOG\n",
            "load: control token:    679 '[control_677]' is not marked as EOG\n",
            "load: control token:     77 '[control_75]' is not marked as EOG\n",
            "load: control token:      0 '<unk>' is not marked as EOG\n",
            "load: control token:    638 '[control_636]' is not marked as EOG\n",
            "load: control token:    472 '[control_470]' is not marked as EOG\n",
            "load: control token:     59 '[control_57]' is not marked as EOG\n",
            "load: control token:    145 '[control_143]' is not marked as EOG\n",
            "load: control token:    318 '[control_316]' is not marked as EOG\n",
            "load: control token:    640 '[control_638]' is not marked as EOG\n",
            "load: control token:    690 '[control_688]' is not marked as EOG\n",
            "load: control token:    256 '[control_254]' is not marked as EOG\n",
            "load: control token:    476 '[control_474]' is not marked as EOG\n",
            "load: control token:     21 '[control_19]' is not marked as EOG\n",
            "load: control token:    288 '[control_286]' is not marked as EOG\n",
            "load: control token:    255 '[control_253]' is not marked as EOG\n",
            "load: control token:    113 '[control_111]' is not marked as EOG\n",
            "load: control token:    190 '[control_188]' is not marked as EOG\n",
            "load: control token:    108 '[control_106]' is not marked as EOG\n",
            "load: control token:    211 '[control_209]' is not marked as EOG\n",
            "load: control token:    551 '[control_549]' is not marked as EOG\n",
            "load: control token:     14 '[control_12]' is not marked as EOG\n",
            "load: control token:    737 '[control_735]' is not marked as EOG\n",
            "load: control token:    555 '[control_553]' is not marked as EOG\n",
            "load: control token:    227 '[control_225]' is not marked as EOG\n",
            "load: control token:    210 '[control_208]' is not marked as EOG\n",
            "load: control token:    230 '[control_228]' is not marked as EOG\n",
            "load: control token:      7 '[/AVAILABLE_TOOLS]' is not marked as EOG\n",
            "load: control token:     55 '[control_53]' is not marked as EOG\n",
            "load: control token:    525 '[control_523]' is not marked as EOG\n",
            "load: control token:    533 '[control_531]' is not marked as EOG\n",
            "load: control token:    195 '[control_193]' is not marked as EOG\n",
            "load: control token:    584 '[control_582]' is not marked as EOG\n",
            "load: control token:    203 '[control_201]' is not marked as EOG\n",
            "load: control token:    322 '[control_320]' is not marked as EOG\n",
            "load: control token:    757 '[control_755]' is not marked as EOG\n",
            "load: control token:    642 '[control_640]' is not marked as EOG\n",
            "load: control token:    168 '[control_166]' is not marked as EOG\n",
            "load: control token:    389 '[control_387]' is not marked as EOG\n",
            "load: control token:    368 '[control_366]' is not marked as EOG\n",
            "load: control token:    767 '[control_765]' is not marked as EOG\n",
            "load: control token:    222 '[control_220]' is not marked as EOG\n",
            "load: control token:    420 '[control_418]' is not marked as EOG\n",
            "load: control token:    530 '[control_528]' is not marked as EOG\n",
            "load: control token:    535 '[control_533]' is not marked as EOG\n",
            "load: control token:    765 '[control_763]' is not marked as EOG\n",
            "load: control token:    661 '[control_659]' is not marked as EOG\n",
            "load: control token:     88 '[control_86]' is not marked as EOG\n",
            "load: control token:    581 '[control_579]' is not marked as EOG\n",
            "load: control token:    327 '[control_325]' is not marked as EOG\n",
            "load: control token:    201 '[control_199]' is not marked as EOG\n",
            "load: control token:    115 '[control_113]' is not marked as EOG\n",
            "load: control token:    709 '[control_707]' is not marked as EOG\n",
            "load: control token:    291 '[control_289]' is not marked as EOG\n",
            "load: control token:    265 '[control_263]' is not marked as EOG\n",
            "load: control token:    148 '[control_146]' is not marked as EOG\n",
            "load: control token:    185 '[control_183]' is not marked as EOG\n",
            "load: control token:    574 '[control_572]' is not marked as EOG\n",
            "load: control token:    360 '[control_358]' is not marked as EOG\n",
            "load: control token:    127 '[control_125]' is not marked as EOG\n",
            "load: control token:    325 '[control_323]' is not marked as EOG\n",
            "load: control token:    183 '[control_181]' is not marked as EOG\n",
            "load: control token:    116 '[control_114]' is not marked as EOG\n",
            "load: control token:    540 '[control_538]' is not marked as EOG\n",
            "load: control token:    293 '[control_291]' is not marked as EOG\n",
            "load: control token:    559 '[control_557]' is not marked as EOG\n",
            "load: control token:    527 '[control_525]' is not marked as EOG\n",
            "load: control token:    156 '[control_154]' is not marked as EOG\n",
            "load: control token:    338 '[control_336]' is not marked as EOG\n",
            "load: control token:    519 '[control_517]' is not marked as EOG\n",
            "load: control token:    516 '[control_514]' is not marked as EOG\n",
            "load: control token:     10 '[control_8]' is not marked as EOG\n",
            "load: control token:      8 '[TOOL_RESULTS]' is not marked as EOG\n",
            "load: control token:    505 '[control_503]' is not marked as EOG\n",
            "load: control token:    503 '[control_501]' is not marked as EOG\n",
            "load: control token:    500 '[control_498]' is not marked as EOG\n",
            "load: control token:    496 '[control_494]' is not marked as EOG\n",
            "load: control token:    492 '[control_490]' is not marked as EOG\n",
            "load: control token:    489 '[control_487]' is not marked as EOG\n",
            "load: control token:    538 '[control_536]' is not marked as EOG\n",
            "load: control token:    596 '[control_594]' is not marked as EOG\n",
            "load: control token:    481 '[control_479]' is not marked as EOG\n",
            "load: control token:    475 '[control_473]' is not marked as EOG\n",
            "load: control token:    336 '[control_334]' is not marked as EOG\n",
            "load: control token:    670 '[control_668]' is not marked as EOG\n",
            "load: control token:     50 '[control_48]' is not marked as EOG\n",
            "load: control token:    456 '[control_454]' is not marked as EOG\n",
            "load: control token:    105 '[control_103]' is not marked as EOG\n",
            "load: control token:    421 '[control_419]' is not marked as EOG\n",
            "load: control token:    430 '[control_428]' is not marked as EOG\n",
            "load: control token:    429 '[control_427]' is not marked as EOG\n",
            "load: control token:    575 '[control_573]' is not marked as EOG\n",
            "load: control token:    425 '[control_423]' is not marked as EOG\n",
            "load: control token:    424 '[control_422]' is not marked as EOG\n",
            "load: control token:    680 '[control_678]' is not marked as EOG\n",
            "load: control token:     57 '[control_55]' is not marked as EOG\n",
            "load: control token:    356 '[control_354]' is not marked as EOG\n",
            "load: control token:    458 '[control_456]' is not marked as EOG\n",
            "load: control token:    313 '[control_311]' is not marked as EOG\n",
            "load: control token:    418 '[control_416]' is not marked as EOG\n",
            "load: control token:     70 '[control_68]' is not marked as EOG\n",
            "load: control token:    759 '[control_757]' is not marked as EOG\n",
            "load: control token:    416 '[control_414]' is not marked as EOG\n",
            "load: control token:    238 '[control_236]' is not marked as EOG\n",
            "load: control token:    568 '[control_566]' is not marked as EOG\n",
            "load: control token:    409 '[control_407]' is not marked as EOG\n",
            "load: control token:    550 '[control_548]' is not marked as EOG\n",
            "load: control token:    571 '[control_569]' is not marked as EOG\n",
            "load: control token:    618 '[control_616]' is not marked as EOG\n",
            "load: control token:    623 '[control_621]' is not marked as EOG\n",
            "load: control token:    247 '[control_245]' is not marked as EOG\n",
            "load: control token:    400 '[control_398]' is not marked as EOG\n",
            "load: control token:    396 '[control_394]' is not marked as EOG\n",
            "load: control token:    392 '[control_390]' is not marked as EOG\n",
            "load: control token:    552 '[control_550]' is not marked as EOG\n",
            "load: control token:    651 '[control_649]' is not marked as EOG\n",
            "load: control token:    390 '[control_388]' is not marked as EOG\n",
            "load: control token:    186 '[control_184]' is not marked as EOG\n",
            "load: control token:    565 '[control_563]' is not marked as EOG\n",
            "load: control token:     92 '[control_90]' is not marked as EOG\n",
            "load: control token:    508 '[control_506]' is not marked as EOG\n",
            "load: control token:    373 '[control_371]' is not marked as EOG\n",
            "load: control token:    299 '[control_297]' is not marked as EOG\n",
            "load: control token:    562 '[control_560]' is not marked as EOG\n",
            "load: control token:    537 '[control_535]' is not marked as EOG\n",
            "load: control token:    372 '[control_370]' is not marked as EOG\n",
            "load: control token:    366 '[control_364]' is not marked as EOG\n",
            "load: control token:    196 '[control_194]' is not marked as EOG\n",
            "load: control token:    478 '[control_476]' is not marked as EOG\n",
            "load: control token:    621 '[control_619]' is not marked as EOG\n",
            "load: control token:     53 '[control_51]' is not marked as EOG\n",
            "load: control token:     40 '[control_38]' is not marked as EOG\n",
            "load: control token:    347 '[control_345]' is not marked as EOG\n",
            "load: control token:    345 '[control_343]' is not marked as EOG\n",
            "load: control token:    339 '[control_337]' is not marked as EOG\n",
            "load: control token:    234 '[control_232]' is not marked as EOG\n",
            "load: control token:    770 '[control_768]' is not marked as EOG\n",
            "load: control token:    444 '[control_442]' is not marked as EOG\n",
            "load: control token:    317 '[control_315]' is not marked as EOG\n",
            "load: control token:    693 '[control_691]' is not marked as EOG\n",
            "load: control token:    321 '[control_319]' is not marked as EOG\n",
            "load: control token:    320 '[control_318]' is not marked as EOG\n",
            "load: control token:    179 '[control_177]' is not marked as EOG\n",
            "load: control token:    214 '[control_212]' is not marked as EOG\n",
            "load: control token:    666 '[control_664]' is not marked as EOG\n",
            "load: control token:    712 '[control_710]' is not marked as EOG\n",
            "load: control token:    303 '[control_301]' is not marked as EOG\n",
            "load: control token:    440 '[control_438]' is not marked as EOG\n",
            "load: control token:    314 '[control_312]' is not marked as EOG\n",
            "load: control token:    397 '[control_395]' is not marked as EOG\n",
            "load: control token:    312 '[control_310]' is not marked as EOG\n",
            "load: control token:    129 '[control_127]' is not marked as EOG\n",
            "load: control token:    545 '[control_543]' is not marked as EOG\n",
            "load: control token:     58 '[control_56]' is not marked as EOG\n",
            "load: control token:    509 '[control_507]' is not marked as EOG\n",
            "load: control token:    541 '[control_539]' is not marked as EOG\n",
            "load: control token:    443 '[control_441]' is not marked as EOG\n",
            "load: control token:     45 '[control_43]' is not marked as EOG\n",
            "load: control token:    469 '[control_467]' is not marked as EOG\n",
            "load: control token:    532 '[control_530]' is not marked as EOG\n",
            "load: control token:     64 '[control_62]' is not marked as EOG\n",
            "load: control token:    365 '[control_363]' is not marked as EOG\n",
            "load: control token:    563 '[control_561]' is not marked as EOG\n",
            "load: control token:    346 '[control_344]' is not marked as EOG\n",
            "load: control token:    282 '[control_280]' is not marked as EOG\n",
            "load: control token:    450 '[control_448]' is not marked as EOG\n",
            "load: control token:    526 '[control_524]' is not marked as EOG\n",
            "load: control token:    672 '[control_670]' is not marked as EOG\n",
            "load: control token:    232 '[control_230]' is not marked as EOG\n",
            "load: control token:    273 '[control_271]' is not marked as EOG\n",
            "load: control token:    272 '[control_270]' is not marked as EOG\n",
            "load: control token:    636 '[control_634]' is not marked as EOG\n",
            "load: control token:    260 '[control_258]' is not marked as EOG\n",
            "load: control token:    518 '[control_516]' is not marked as EOG\n",
            "load: control token:    353 '[control_351]' is not marked as EOG\n",
            "load: control token:    257 '[control_255]' is not marked as EOG\n",
            "load: control token:    126 '[control_124]' is not marked as EOG\n",
            "load: control token:    124 '[control_122]' is not marked as EOG\n",
            "load: control token:    751 '[control_749]' is not marked as EOG\n",
            "load: control token:    383 '[control_381]' is not marked as EOG\n",
            "load: control token:    175 '[control_173]' is not marked as EOG\n",
            "load: control token:     75 '[control_73]' is not marked as EOG\n",
            "load: control token:     72 '[control_70]' is not marked as EOG\n",
            "load: control token:    614 '[control_612]' is not marked as EOG\n",
            "load: control token:    167 '[control_165]' is not marked as EOG\n",
            "load: control token:    465 '[control_463]' is not marked as EOG\n",
            "load: control token:    341 '[control_339]' is not marked as EOG\n",
            "load: control token:    703 '[control_701]' is not marked as EOG\n",
            "load: control token:    501 '[control_499]' is not marked as EOG\n",
            "load: control token:     90 '[control_88]' is not marked as EOG\n",
            "load: control token:     89 '[control_87]' is not marked as EOG\n",
            "load: control token:    593 '[control_591]' is not marked as EOG\n",
            "load: control token:    107 '[control_105]' is not marked as EOG\n",
            "load: control token:    241 '[control_239]' is not marked as EOG\n",
            "load: control token:    713 '[control_711]' is not marked as EOG\n",
            "load: control token:    212 '[control_210]' is not marked as EOG\n",
            "load: control token:     80 '[control_78]' is not marked as EOG\n",
            "load: control token:    707 '[control_705]' is not marked as EOG\n",
            "load: control token:    371 '[control_369]' is not marked as EOG\n",
            "load: control token:    628 '[control_626]' is not marked as EOG\n",
            "load: control token:     15 '[control_13]' is not marked as EOG\n",
            "load: control token:    354 '[control_352]' is not marked as EOG\n",
            "load: control token:     25 '[control_23]' is not marked as EOG\n",
            "load: control token:    261 '[control_259]' is not marked as EOG\n",
            "load: control token:    435 '[control_433]' is not marked as EOG\n",
            "load: control token:    307 '[control_305]' is not marked as EOG\n",
            "load: control token:    305 '[control_303]' is not marked as EOG\n",
            "load: control token:    656 '[control_654]' is not marked as EOG\n",
            "load: control token:    702 '[control_700]' is not marked as EOG\n",
            "load: control token:    428 '[control_426]' is not marked as EOG\n",
            "load: control token:    499 '[control_497]' is not marked as EOG\n",
            "load: control token:    154 '[control_152]' is not marked as EOG\n",
            "load: control token:    308 '[control_306]' is not marked as EOG\n",
            "load: control token:    286 '[control_284]' is not marked as EOG\n",
            "load: control token:    634 '[control_632]' is not marked as EOG\n",
            "load: control token:     51 '[control_49]' is not marked as EOG\n",
            "load: control token:    599 '[control_597]' is not marked as EOG\n",
            "load: control token:    606 '[control_604]' is not marked as EOG\n",
            "load: control token:     30 '[control_28]' is not marked as EOG\n",
            "load: control token:    250 '[control_248]' is not marked as EOG\n",
            "load: control token:    714 '[control_712]' is not marked as EOG\n",
            "load: control token:    270 '[control_268]' is not marked as EOG\n",
            "load: control token:     83 '[control_81]' is not marked as EOG\n",
            "load: control token:    730 '[control_728]' is not marked as EOG\n",
            "load: control token:    743 '[control_741]' is not marked as EOG\n",
            "load: control token:    484 '[control_482]' is not marked as EOG\n",
            "load: control token:    758 '[control_756]' is not marked as EOG\n",
            "load: control token:    271 '[control_269]' is not marked as EOG\n",
            "load: control token:    279 '[control_277]' is not marked as EOG\n",
            "load: control token:     73 '[control_71]' is not marked as EOG\n",
            "load: control token:    363 '[control_361]' is not marked as EOG\n",
            "load: control token:     60 '[control_58]' is not marked as EOG\n",
            "load: control token:    405 '[control_403]' is not marked as EOG\n",
            "load: control token:    220 '[control_218]' is not marked as EOG\n",
            "load: control token:    436 '[control_434]' is not marked as EOG\n",
            "load: control token:     27 '[control_25]' is not marked as EOG\n",
            "load: control token:    652 '[control_650]' is not marked as EOG\n",
            "load: control token:    646 '[control_644]' is not marked as EOG\n",
            "load: control token:    395 '[control_393]' is not marked as EOG\n",
            "load: control token:    549 '[control_547]' is not marked as EOG\n",
            "load: control token:     12 '[control_10]' is not marked as EOG\n",
            "load: control token:    229 '[control_227]' is not marked as EOG\n",
            "load: control token:    162 '[control_160]' is not marked as EOG\n",
            "load: control token:    497 '[control_495]' is not marked as EOG\n",
            "load: control token:     31 '[control_29]' is not marked as EOG\n",
            "load: control token:     98 '[control_96]' is not marked as EOG\n",
            "load: control token:    686 '[control_684]' is not marked as EOG\n",
            "load: control token:      3 '[INST]' is not marked as EOG\n",
            "load: control token:    155 '[control_153]' is not marked as EOG\n",
            "load: control token:    470 '[control_468]' is not marked as EOG\n",
            "load: control token:     69 '[control_67]' is not marked as EOG\n",
            "load: control token:     93 '[control_91]' is not marked as EOG\n",
            "load: control token:     71 '[control_69]' is not marked as EOG\n",
            "load: control token:     11 '[control_9]' is not marked as EOG\n",
            "load: control token:     43 '[control_41]' is not marked as EOG\n",
            "load: control token:     22 '[control_20]' is not marked as EOG\n",
            "load: control token:     35 '[control_33]' is not marked as EOG\n",
            "load: control token:    706 '[control_704]' is not marked as EOG\n",
            "load: control token:    511 '[control_509]' is not marked as EOG\n",
            "load: control token:    491 '[control_489]' is not marked as EOG\n",
            "load: control token:    324 '[control_322]' is not marked as EOG\n",
            "load: control token:    655 '[control_653]' is not marked as EOG\n",
            "load: control token:    117 '[control_115]' is not marked as EOG\n",
            "load: control token:    665 '[control_663]' is not marked as EOG\n",
            "load: control token:      9 '[/TOOL_RESULTS]' is not marked as EOG\n",
            "load: control token:     29 '[control_27]' is not marked as EOG\n",
            "load: control token:     44 '[control_42]' is not marked as EOG\n",
            "load: control token:    310 '[control_308]' is not marked as EOG\n",
            "load: control token:    157 '[control_155]' is not marked as EOG\n",
            "load: control token:    515 '[control_513]' is not marked as EOG\n",
            "load: control token:    388 '[control_386]' is not marked as EOG\n",
            "load: control token:    438 '[control_436]' is not marked as EOG\n",
            "load: control token:    486 '[control_484]' is not marked as EOG\n",
            "load: control token:    139 '[control_137]' is not marked as EOG\n",
            "load: control token:    543 '[control_541]' is not marked as EOG\n",
            "load: control token:    622 '[control_620]' is not marked as EOG\n",
            "load: control token:    598 '[control_596]' is not marked as EOG\n",
            "load: control token:     16 '[control_14]' is not marked as EOG\n",
            "load: control token:     39 '[control_37]' is not marked as EOG\n",
            "load: control token:    102 '[control_100]' is not marked as EOG\n",
            "load: control token:    673 '[control_671]' is not marked as EOG\n",
            "load: control token:    287 '[control_285]' is not marked as EOG\n",
            "load: control token:    244 '[control_242]' is not marked as EOG\n",
            "load: control token:    446 '[control_444]' is not marked as EOG\n",
            "load: control token:    483 '[control_481]' is not marked as EOG\n",
            "load: control token:    467 '[control_465]' is not marked as EOG\n",
            "load: control token:    264 '[control_262]' is not marked as EOG\n",
            "load: control token:    176 '[control_174]' is not marked as EOG\n",
            "load: control token:    625 '[control_623]' is not marked as EOG\n",
            "load: control token:    160 '[control_158]' is not marked as EOG\n",
            "load: control token:     20 '[control_18]' is not marked as EOG\n",
            "load: control token:    641 '[control_639]' is not marked as EOG\n",
            "load: control token:     99 '[control_97]' is not marked as EOG\n",
            "load: control token:     54 '[control_52]' is not marked as EOG\n",
            "load: control token:     37 '[control_35]' is not marked as EOG\n",
            "load: control token:    401 '[control_399]' is not marked as EOG\n",
            "load: control token:    130 '[control_128]' is not marked as EOG\n",
            "load: control token:    218 '[control_216]' is not marked as EOG\n",
            "load: control token:    243 '[control_241]' is not marked as EOG\n",
            "load: control token:    753 '[control_751]' is not marked as EOG\n",
            "load: control token:     74 '[control_72]' is not marked as EOG\n",
            "load: control token:    171 '[control_169]' is not marked as EOG\n",
            "load: control token:    151 '[control_149]' is not marked as EOG\n",
            "load: control token:    364 '[control_362]' is not marked as EOG\n",
            "load: control token:    495 '[control_493]' is not marked as EOG\n",
            "load: control token:    119 '[control_117]' is not marked as EOG\n",
            "load: control token:    217 '[control_215]' is not marked as EOG\n",
            "load: control token:     84 '[control_82]' is not marked as EOG\n",
            "load: control token:      5 '[TOOL_CALLS]' is not marked as EOG\n",
            "load: control token:    531 '[control_529]' is not marked as EOG\n",
            "load: control token:     33 '[control_31]' is not marked as EOG\n",
            "load: control token:     82 '[control_80]' is not marked as EOG\n",
            "load: control token:    100 '[control_98]' is not marked as EOG\n",
            "load: control token:    125 '[control_123]' is not marked as EOG\n",
            "load: control token:    520 '[control_518]' is not marked as EOG\n",
            "load: control token:    144 '[control_142]' is not marked as EOG\n",
            "load: control token:    152 '[control_150]' is not marked as EOG\n",
            "load: control token:    592 '[control_590]' is not marked as EOG\n",
            "load: control token:    461 '[control_459]' is not marked as EOG\n",
            "load: control token:    624 '[control_622]' is not marked as EOG\n",
            "load: control token:     65 '[control_63]' is not marked as EOG\n",
            "load: control token:    172 '[control_170]' is not marked as EOG\n",
            "load: control token:    178 '[control_176]' is not marked as EOG\n",
            "load: control token:    258 '[control_256]' is not marked as EOG\n",
            "load: control token:    524 '[control_522]' is not marked as EOG\n",
            "load: control token:    657 '[control_655]' is not marked as EOG\n",
            "load: control token:    184 '[control_182]' is not marked as EOG\n",
            "load: control token:    441 '[control_439]' is not marked as EOG\n",
            "load: control token:    315 '[control_313]' is not marked as EOG\n",
            "load: control token:    662 '[control_660]' is not marked as EOG\n",
            "load: control token:      6 '[AVAILABLE_TOOLS]' is not marked as EOG\n",
            "load: control token:    517 '[control_515]' is not marked as EOG\n",
            "load: control token:    194 '[control_192]' is not marked as EOG\n",
            "load: control token:    447 '[control_445]' is not marked as EOG\n",
            "load: control token:     52 '[control_50]' is not marked as EOG\n",
            "load: control token:    718 '[control_716]' is not marked as EOG\n",
            "load: control token:    692 '[control_690]' is not marked as EOG\n",
            "load: control token:    290 '[control_288]' is not marked as EOG\n",
            "load: control token:    763 '[control_761]' is not marked as EOG\n",
            "load: control token:    362 '[control_360]' is not marked as EOG\n",
            "load: control token:    200 '[control_198]' is not marked as EOG\n",
            "load: control token:    204 '[control_202]' is not marked as EOG\n",
            "load: control token:    722 '[control_720]' is not marked as EOG\n",
            "load: control token:    351 '[control_349]' is not marked as EOG\n",
            "load: control token:    349 '[control_347]' is not marked as EOG\n",
            "load: control token:     38 '[control_36]' is not marked as EOG\n",
            "load: control token:    134 '[control_132]' is not marked as EOG\n",
            "load: control token:     67 '[control_65]' is not marked as EOG\n",
            "load: control token:    239 '[control_237]' is not marked as EOG\n",
            "load: control token:    570 '[control_568]' is not marked as EOG\n",
            "load: control token:    253 '[control_251]' is not marked as EOG\n",
            "load: control token:    221 '[control_219]' is not marked as EOG\n",
            "load: control token:    664 '[control_662]' is not marked as EOG\n",
            "load: control token:    225 '[control_223]' is not marked as EOG\n",
            "load: control token:    226 '[control_224]' is not marked as EOG\n",
            "load: control token:    553 '[control_551]' is not marked as EOG\n",
            "load: control token:    242 '[control_240]' is not marked as EOG\n",
            "load: control token:    633 '[control_631]' is not marked as EOG\n",
            "load: control token:    580 '[control_578]' is not marked as EOG\n",
            "load: control token:    245 '[control_243]' is not marked as EOG\n",
            "load: control token:    620 '[control_618]' is not marked as EOG\n",
            "load: control token:    191 '[control_189]' is not marked as EOG\n",
            "load: control token:     87 '[control_85]' is not marked as EOG\n",
            "load: control token:    189 '[control_187]' is not marked as EOG\n",
            "load: control token:    542 '[control_540]' is not marked as EOG\n",
            "load: control token:    548 '[control_546]' is not marked as EOG\n",
            "load: control token:    558 '[control_556]' is not marked as EOG\n",
            "load: control token:    560 '[control_558]' is not marked as EOG\n",
            "load: control token:    564 '[control_562]' is not marked as EOG\n",
            "load: control token:    410 '[control_408]' is not marked as EOG\n",
            "load: control token:    566 '[control_564]' is not marked as EOG\n",
            "load: control token:    649 '[control_647]' is not marked as EOG\n",
            "load: control token:    414 '[control_412]' is not marked as EOG\n",
            "load: control token:    567 '[control_565]' is not marked as EOG\n",
            "load: control token:    569 '[control_567]' is not marked as EOG\n",
            "load: control token:    761 '[control_759]' is not marked as EOG\n",
            "load: control token:    408 '[control_406]' is not marked as EOG\n",
            "load: control token:    579 '[control_577]' is not marked as EOG\n",
            "load: control token:    147 '[control_145]' is not marked as EOG\n",
            "load: control token:    236 '[control_234]' is not marked as EOG\n",
            "load: control token:    588 '[control_586]' is not marked as EOG\n",
            "load: control token:    292 '[control_290]' is not marked as EOG\n",
            "load: control token:    591 '[control_589]' is not marked as EOG\n",
            "load: control token:    645 '[control_643]' is not marked as EOG\n",
            "load: control token:    140 '[control_138]' is not marked as EOG\n",
            "load: control token:    422 '[control_420]' is not marked as EOG\n",
            "load: control token:    595 '[control_593]' is not marked as EOG\n",
            "load: control token:    597 '[control_595]' is not marked as EOG\n",
            "load: control token:    754 '[control_752]' is not marked as EOG\n",
            "load: control token:    159 '[control_157]' is not marked as EOG\n",
            "load: control token:    344 '[control_342]' is not marked as EOG\n",
            "load: control token:    393 '[control_391]' is not marked as EOG\n",
            "load: control token:    691 '[control_689]' is not marked as EOG\n",
            "load: control token:    608 '[control_606]' is not marked as EOG\n",
            "load: control token:    663 '[control_661]' is not marked as EOG\n",
            "load: control token:    613 '[control_611]' is not marked as EOG\n",
            "load: control token:    340 '[control_338]' is not marked as EOG\n",
            "load: control token:    616 '[control_614]' is not marked as EOG\n",
            "load: control token:    719 '[control_717]' is not marked as EOG\n",
            "load: control token:    521 '[control_519]' is not marked as EOG\n",
            "load: control token:    619 '[control_617]' is not marked as EOG\n",
            "load: control token:    523 '[control_521]' is not marked as EOG\n",
            "load: control token:    626 '[control_624]' is not marked as EOG\n",
            "load: control token:    695 '[control_693]' is not marked as EOG\n",
            "load: control token:    637 '[control_635]' is not marked as EOG\n",
            "load: control token:    235 '[control_233]' is not marked as EOG\n",
            "load: control token:    659 '[control_657]' is not marked as EOG\n",
            "load: control token:    268 '[control_266]' is not marked as EOG\n",
            "load: control token:    406 '[control_404]' is not marked as EOG\n",
            "load: control token:    735 '[control_733]' is not marked as EOG\n",
            "load: control token:    398 '[control_396]' is not marked as EOG\n",
            "load: control token:    674 '[control_672]' is not marked as EOG\n",
            "load: control token:    684 '[control_682]' is not marked as EOG\n",
            "load: control token:    462 '[control_460]' is not marked as EOG\n",
            "load: control token:    382 '[control_380]' is not marked as EOG\n",
            "load: control token:    697 '[control_695]' is not marked as EOG\n",
            "load: control token:    698 '[control_696]' is not marked as EOG\n",
            "load: control token:    699 '[control_697]' is not marked as EOG\n",
            "load: control token:    688 '[control_686]' is not marked as EOG\n",
            "load: control token:    704 '[control_702]' is not marked as EOG\n",
            "load: control token:    705 '[control_703]' is not marked as EOG\n",
            "load: control token:    205 '[control_203]' is not marked as EOG\n",
            "load: control token:    726 '[control_724]' is not marked as EOG\n",
            "load: control token:    180 '[control_178]' is not marked as EOG\n",
            "load: control token:    716 '[control_714]' is not marked as EOG\n",
            "load: control token:    590 '[control_588]' is not marked as EOG\n",
            "load: control token:    454 '[control_452]' is not marked as EOG\n",
            "load: control token:    728 '[control_726]' is not marked as EOG\n",
            "load: control token:    738 '[control_736]' is not marked as EOG\n",
            "load: control token:    744 '[control_742]' is not marked as EOG\n",
            "load: control token:    403 '[control_401]' is not marked as EOG\n",
            "load: control token:    764 '[control_762]' is not marked as EOG\n",
            "load: control token:    676 '[control_674]' is not marked as EOG\n",
            "load: control token:    756 '[control_754]' is not marked as EOG\n",
            "load: control token:    380 '[control_378]' is not marked as EOG\n",
            "load: control token:    319 '[control_317]' is not marked as EOG\n",
            "load: control token:    600 '[control_598]' is not marked as EOG\n",
            "load: control token:    297 '[control_295]' is not marked as EOG\n",
            "load: control token:    259 '[control_257]' is not marked as EOG\n",
            "load: control token:    639 '[control_637]' is not marked as EOG\n",
            "load: control token:    529 '[control_527]' is not marked as EOG\n",
            "load: control token:    101 '[control_99]' is not marked as EOG\n",
            "load: control token:    522 '[control_520]' is not marked as EOG\n",
            "load: control token:    723 '[control_721]' is not marked as EOG\n",
            "load: control token:    254 '[control_252]' is not marked as EOG\n",
            "load: control token:    513 '[control_511]' is not marked as EOG\n",
            "load: control token:    667 '[control_665]' is not marked as EOG\n",
            "load: control token:    267 '[control_265]' is not marked as EOG\n",
            "load: control token:    732 '[control_730]' is not marked as EOG\n",
            "load: control token:     24 '[control_22]' is not marked as EOG\n",
            "load: control token:    650 '[control_648]' is not marked as EOG\n",
            "load: control token:    528 '[control_526]' is not marked as EOG\n",
            "load: control token:    749 '[control_747]' is not marked as EOG\n",
            "load: control token:    721 '[control_719]' is not marked as EOG\n",
            "load: control token:    103 '[control_101]' is not marked as EOG\n",
            "load: control token:    294 '[control_292]' is not marked as EOG\n",
            "load: control token:     41 '[control_39]' is not marked as EOG\n",
            "load: control token:    133 '[control_131]' is not marked as EOG\n",
            "load: control token:    188 '[control_186]' is not marked as EOG\n",
            "load: control token:    276 '[control_274]' is not marked as EOG\n",
            "load: control token:    644 '[control_642]' is not marked as EOG\n",
            "load: control token:    648 '[control_646]' is not marked as EOG\n",
            "load: control token:    459 '[control_457]' is not marked as EOG\n",
            "load: control token:    112 '[control_110]' is not marked as EOG\n",
            "load: control token:    300 '[control_298]' is not marked as EOG\n",
            "load: control token:    206 '[control_204]' is not marked as EOG\n",
            "load: control token:    121 '[control_119]' is not marked as EOG\n",
            "load: control token:    750 '[control_748]' is not marked as EOG\n",
            "load: control token:    131 '[control_129]' is not marked as EOG\n",
            "load: control token:    474 '[control_472]' is not marked as EOG\n",
            "load: control token:    685 '[control_683]' is not marked as EOG\n",
            "load: control token:    251 '[control_249]' is not marked as EOG\n",
            "load: control token:    402 '[control_400]' is not marked as EOG\n",
            "load: control token:    281 '[control_279]' is not marked as EOG\n",
            "load: control token:    192 '[control_190]' is not marked as EOG\n",
            "load: control token:    387 '[control_385]' is not marked as EOG\n",
            "load: control token:    587 '[control_585]' is not marked as EOG\n",
            "load: control token:     66 '[control_64]' is not marked as EOG\n",
            "load: control token:    394 '[control_392]' is not marked as EOG\n",
            "load: control token:     49 '[control_47]' is not marked as EOG\n",
            "load: control token:     42 '[control_40]' is not marked as EOG\n",
            "load: control token:     32 '[control_30]' is not marked as EOG\n",
            "load: control token:    399 '[control_397]' is not marked as EOG\n",
            "load: control token:    448 '[control_446]' is not marked as EOG\n",
            "load: control token:    479 '[control_477]' is not marked as EOG\n",
            "load: control token:    493 '[control_491]' is not marked as EOG\n",
            "load: control token:    736 '[control_734]' is not marked as EOG\n",
            "load: control token:    348 '[control_346]' is not marked as EOG\n",
            "load: control token:    193 '[control_191]' is not marked as EOG\n",
            "load: control token:    231 '[control_229]' is not marked as EOG\n",
            "load: control token:    609 '[control_607]' is not marked as EOG\n",
            "load: control token:    213 '[control_211]' is not marked as EOG\n",
            "load: control token:    128 '[control_126]' is not marked as EOG\n",
            "load: control token:    118 '[control_116]' is not marked as EOG\n",
            "load: control token:    369 '[control_367]' is not marked as EOG\n",
            "load: control token:    630 '[control_628]' is not marked as EOG\n",
            "load: control token:    755 '[control_753]' is not marked as EOG\n",
            "load: control token:    747 '[control_745]' is not marked as EOG\n",
            "load: control token:    289 '[control_287]' is not marked as EOG\n",
            "load: control token:     26 '[control_24]' is not marked as EOG\n",
            "load: control token:     63 '[control_61]' is not marked as EOG\n",
            "load: control token:    284 '[control_282]' is not marked as EOG\n",
            "load: control token:    164 '[control_162]' is not marked as EOG\n",
            "load: control token:    404 '[control_402]' is not marked as EOG\n",
            "load: control token:    506 '[control_504]' is not marked as EOG\n",
            "load: control token:    123 '[control_121]' is not marked as EOG\n",
            "load: control token:    237 '[control_235]' is not marked as EOG\n",
            "load: control token:     62 '[control_60]' is not marked as EOG\n",
            "load: control token:    510 '[control_508]' is not marked as EOG\n",
            "load: control token:    233 '[control_231]' is not marked as EOG\n",
            "load: control token:     68 '[control_66]' is not marked as EOG\n",
            "load: control token:    487 '[control_485]' is not marked as EOG\n",
            "load: control token:    471 '[control_469]' is not marked as EOG\n",
            "load: control token:    415 '[control_413]' is not marked as EOG\n",
            "load: control token:     78 '[control_76]' is not marked as EOG\n",
            "load: control token:    610 '[control_608]' is not marked as EOG\n",
            "load: control token:    137 '[control_135]' is not marked as EOG\n",
            "load: control token:    539 '[control_537]' is not marked as EOG\n",
            "load: control token:    658 '[control_656]' is not marked as EOG\n",
            "load: control token:    158 '[control_156]' is not marked as EOG\n",
            "load: control token:    585 '[control_583]' is not marked as EOG\n",
            "load: control token:    215 '[control_213]' is not marked as EOG\n",
            "load: control token:    554 '[control_552]' is not marked as EOG\n",
            "load: control token:    762 '[control_760]' is not marked as EOG\n",
            "load: control token:    717 '[control_715]' is not marked as EOG\n",
            "load: control token:    228 '[control_226]' is not marked as EOG\n",
            "load: control token:    442 '[control_440]' is not marked as EOG\n",
            "load: control token:    494 '[control_492]' is not marked as EOG\n",
            "load: control token:     97 '[control_95]' is not marked as EOG\n",
            "load: control token:    683 '[control_681]' is not marked as EOG\n",
            "load: control token:    629 '[control_627]' is not marked as EOG\n",
            "load: control token:    725 '[control_723]' is not marked as EOG\n",
            "load: control token:    432 '[control_430]' is not marked as EOG\n",
            "load: control token:    477 '[control_475]' is not marked as EOG\n",
            "load: control token:    355 '[control_353]' is not marked as EOG\n",
            "load: control token:     36 '[control_34]' is not marked as EOG\n",
            "load: control token:    534 '[control_532]' is not marked as EOG\n",
            "load: control token:    120 '[control_118]' is not marked as EOG\n",
            "load: control token:     28 '[control_26]' is not marked as EOG\n",
            "load: control token:    306 '[control_304]' is not marked as EOG\n",
            "load: control token:    407 '[control_405]' is not marked as EOG\n",
            "load: control token:    696 '[control_694]' is not marked as EOG\n",
            "load: control token:    274 '[control_272]' is not marked as EOG\n",
            "load: control token:    142 '[control_140]' is not marked as EOG\n",
            "load: control token:    136 '[control_134]' is not marked as EOG\n",
            "load: control token:    309 '[control_307]' is not marked as EOG\n",
            "load: control token:    423 '[control_421]' is not marked as EOG\n",
            "load: control token:    724 '[control_722]' is not marked as EOG\n",
            "load: control token:    262 '[control_260]' is not marked as EOG\n",
            "load: control token:    419 '[control_417]' is not marked as EOG\n",
            "load: control token:    329 '[control_327]' is not marked as EOG\n",
            "load: control token:    427 '[control_425]' is not marked as EOG\n",
            "load: control token:    370 '[control_368]' is not marked as EOG\n",
            "load: control token:    266 '[control_264]' is not marked as EOG\n",
            "load: control token:    710 '[control_708]' is not marked as EOG\n",
            "load: control token:    602 '[control_600]' is not marked as EOG\n",
            "load: control token:    277 '[control_275]' is not marked as EOG\n",
            "load: control token:    298 '[control_296]' is not marked as EOG\n",
            "load: control token:    378 '[control_376]' is not marked as EOG\n",
            "load: control token:    198 '[control_196]' is not marked as EOG\n",
            "load: control token:     76 '[control_74]' is not marked as EOG\n",
            "load: control token:    607 '[control_605]' is not marked as EOG\n",
            "load: control token:    682 '[control_680]' is not marked as EOG\n",
            "load: control token:    333 '[control_331]' is not marked as EOG\n",
            "load: control token:    660 '[control_658]' is not marked as EOG\n",
            "load: control token:    163 '[control_161]' is not marked as EOG\n",
            "load: control token:     95 '[control_93]' is not marked as EOG\n",
            "load: control token:    377 '[control_375]' is not marked as EOG\n",
            "load: control token:    223 '[control_221]' is not marked as EOG\n",
            "load: control token:    326 '[control_324]' is not marked as EOG\n",
            "load: control token:    111 '[control_109]' is not marked as EOG\n",
            "load: control token:    187 '[control_185]' is not marked as EOG\n",
            "load: control token:    678 '[control_676]' is not marked as EOG\n",
            "load: control token:    514 '[control_512]' is not marked as EOG\n",
            "load: control token:    431 '[control_429]' is not marked as EOG\n",
            "load: control token:      4 '[/INST]' is not marked as EOG\n",
            "load: control token:    342 '[control_340]' is not marked as EOG\n",
            "load: control token:    594 '[control_592]' is not marked as EOG\n",
            "load: control token:    768 '[control_766]' is not marked as EOG\n",
            "load: control token:    143 '[control_141]' is not marked as EOG\n",
            "load: control token:    445 '[control_443]' is not marked as EOG\n",
            "load: control token:    165 '[control_163]' is not marked as EOG\n",
            "load: control token:    439 '[control_437]' is not marked as EOG\n",
            "load: control token:    181 '[control_179]' is not marked as EOG\n",
            "load: control token:    352 '[control_350]' is not marked as EOG\n",
            "load: control token:    643 '[control_641]' is not marked as EOG\n",
            "load: control token:    182 '[control_180]' is not marked as EOG\n",
            "load: control token:     19 '[control_17]' is not marked as EOG\n",
            "load: control token:    269 '[control_267]' is not marked as EOG\n",
            "load: control token:    677 '[control_675]' is not marked as EOG\n",
            "load: control token:    615 '[control_613]' is not marked as EOG\n",
            "load: control token:    170 '[control_168]' is not marked as EOG\n",
            "load: control token:     56 '[control_54]' is not marked as EOG\n",
            "load: control token:    582 '[control_580]' is not marked as EOG\n",
            "load: control token:    700 '[control_698]' is not marked as EOG\n",
            "load: control token:    561 '[control_559]' is not marked as EOG\n",
            "load: control token:    381 '[control_379]' is not marked as EOG\n",
            "load: control token:    647 '[control_645]' is not marked as EOG\n",
            "load: control token:    437 '[control_435]' is not marked as EOG\n",
            "load: control token:    507 '[control_505]' is not marked as EOG\n",
            "load: control token:    490 '[control_488]' is not marked as EOG\n",
            "load: control token:    586 '[control_584]' is not marked as EOG\n",
            "load: control token:     23 '[control_21]' is not marked as EOG\n",
            "load: control token:    452 '[control_450]' is not marked as EOG\n",
            "load: control token:    605 '[control_603]' is not marked as EOG\n",
            "load: control token:    426 '[control_424]' is not marked as EOG\n",
            "load: control token:    769 '[control_767]' is not marked as EOG\n",
            "load: control token:     81 '[control_79]' is not marked as EOG\n",
            "load: control token:    386 '[control_384]' is not marked as EOG\n",
            "load: control token:    627 '[control_625]' is not marked as EOG\n",
            "load: control token:     18 '[control_16]' is not marked as EOG\n",
            "load: control token:    741 '[control_739]' is not marked as EOG\n",
            "load: control token:     96 '[control_94]' is not marked as EOG\n",
            "load: control token:    502 '[control_500]' is not marked as EOG\n",
            "load: control token:    482 '[control_480]' is not marked as EOG\n",
            "load: control token:    480 '[control_478]' is not marked as EOG\n",
            "load: control token:    557 '[control_555]' is not marked as EOG\n",
            "load: control token:    263 '[control_261]' is not marked as EOG\n",
            "load: control token:    668 '[control_666]' is not marked as EOG\n",
            "load: control token:    466 '[control_464]' is not marked as EOG\n",
            "load: control token:    334 '[control_332]' is not marked as EOG\n",
            "load: control token:    413 '[control_411]' is not marked as EOG\n",
            "load: control token:    572 '[control_570]' is not marked as EOG\n",
            "load: control token:    295 '[control_293]' is not marked as EOG\n",
            "load: control token:    150 '[control_148]' is not marked as EOG\n",
            "load: control token:    275 '[control_273]' is not marked as EOG\n",
            "load: control token:    748 '[control_746]' is not marked as EOG\n",
            "load: control token:    135 '[control_133]' is not marked as EOG\n",
            "load: control token:    357 '[control_355]' is not marked as EOG\n",
            "load: control token:    577 '[control_575]' is not marked as EOG\n",
            "load: control token:    578 '[control_576]' is not marked as EOG\n",
            "load: control token:    556 '[control_554]' is not marked as EOG\n",
            "load: control token:    375 '[control_373]' is not marked as EOG\n",
            "load: control token:    589 '[control_587]' is not marked as EOG\n",
            "load: control token:    202 '[control_200]' is not marked as EOG\n",
            "load: control token:     85 '[control_83]' is not marked as EOG\n",
            "load: control token:    278 '[control_276]' is not marked as EOG\n",
            "load: control token:    169 '[control_167]' is not marked as EOG\n",
            "load: control token:    208 '[control_206]' is not marked as EOG\n",
            "load: control token:    488 '[control_486]' is not marked as EOG\n",
            "load: control token:    337 '[control_335]' is not marked as EOG\n",
            "load: control token:    733 '[control_731]' is not marked as EOG\n",
            "load: control token:    109 '[control_107]' is not marked as EOG\n",
            "load: control token:    547 '[control_545]' is not marked as EOG\n",
            "load: control token:    350 '[control_348]' is not marked as EOG\n",
            "load: control token:    132 '[control_130]' is not marked as EOG\n",
            "load: control token:    114 '[control_112]' is not marked as EOG\n",
            "load: control token:    248 '[control_246]' is not marked as EOG\n",
            "load: control token:    328 '[control_326]' is not marked as EOG\n",
            "load: control token:    104 '[control_102]' is not marked as EOG\n",
            "load: control token:    449 '[control_447]' is not marked as EOG\n",
            "load: control token:    711 '[control_709]' is not marked as EOG\n",
            "load: control token:    715 '[control_713]' is not marked as EOG\n",
            "load: control token:    358 '[control_356]' is not marked as EOG\n",
            "load: control token:    746 '[control_744]' is not marked as EOG\n",
            "load: control token:    385 '[control_383]' is not marked as EOG\n",
            "load: control token:    720 '[control_718]' is not marked as EOG\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: control token:    460 '[control_458]' is not marked as EOG\n",
            "load: control token:    632 '[control_630]' is not marked as EOG\n",
            "load: control token:    296 '[control_294]' is not marked as EOG\n",
            "load: control token:    240 '[control_238]' is not marked as EOG\n",
            "load: control token:    361 '[control_359]' is not marked as EOG\n",
            "load: control token:    504 '[control_502]' is not marked as EOG\n",
            "load: control token:    149 '[control_147]' is not marked as EOG\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: control token:    219 '[control_217]' is not marked as EOG\n",
            "load: control token:    106 '[control_104]' is not marked as EOG\n",
            "load: control token:    669 '[control_667]' is not marked as EOG\n",
            "load: control token:    411 '[control_409]' is not marked as EOG\n",
            "load: control token:    546 '[control_544]' is not marked as EOG\n",
            "load: control token:    379 '[control_377]' is not marked as EOG\n",
            "load: control token:    209 '[control_207]' is not marked as EOG\n",
            "load: control token:    110 '[control_108]' is not marked as EOG\n",
            "load: control token:    681 '[control_679]' is not marked as EOG\n",
            "load: control token:    153 '[control_151]' is not marked as EOG\n",
            "load: control token:    457 '[control_455]' is not marked as EOG\n",
            "load: control token:     91 '[control_89]' is not marked as EOG\n",
            "load: control token:    224 '[control_222]' is not marked as EOG\n",
            "load: control token:    653 '[control_651]' is not marked as EOG\n",
            "load: control token:    694 '[control_692]' is not marked as EOG\n",
            "load: control token:    316 '[control_314]' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: printing all EOG tokens:\n",
            "load:   - 2 ('</s>')\n",
            "load: special tokens cache size = 771\n",
            "load: token to piece cache size = 0.1731 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 14336\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 7B\n",
            "print_info: model params     = 7.25 B\n",
            "print_info: general.name     = Mistral 7B Instruct v0.3\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32768\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: LF token         = 781 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q8_0) (and 290 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size =  4735.16 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =  2609.86 MiB\n",
            "...................................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 512\n",
            "llama_context: n_ctx_per_seq = 512\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 1000000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (512) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.12 MiB\n",
            "create_memory: n_ctx = 512 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified: layer  28: dev = CPU\n",
            "llama_kv_cache_unified: layer  29: dev = CPU\n",
            "llama_kv_cache_unified: layer  30: dev = CPU\n",
            "llama_kv_cache_unified: layer  31: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =    64.00 MiB\n",
            "llama_kv_cache_unified: size =   64.00 MiB (   512 cells,  32 layers,  1/1 seqs), K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 1\n",
            "llama_context: max_nodes = 2328\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:        CPU compute buffer size =   113.01 MiB\n",
            "llama_context: graph nodes  = 1126\n",
            "llama_context: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'split.count': '2', 'split.tensors.count': '291', 'split.no': '0', 'tokenizer.chat_template': '{%- if messages[0][\"role\"] == \"system\" %}\\n    {%- set system_message = messages[0][\"content\"] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n{%- set user_messages = loop_messages | selectattr(\"role\", \"equalto\", \"user\") | list %}\\n\\n{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\\n{%- set ns = namespace() %}\\n{%- set ns.index = 0 %}\\n{%- for message in loop_messages %}\\n    {%- if not (message.role == \"tool\" or message.role == \"tool_results\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\\n        {%- if (message[\"role\"] == \"user\") != (ns.index % 2 == 0) %}\\n            {{- raise_exception(\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\") }}\\n        {%- endif %}\\n        {%- set ns.index = ns.index + 1 %}\\n    {%- endif %}\\n{%- endfor %}\\n\\n{{- bos_token }}\\n{%- for message in loop_messages %}\\n    {%- if message[\"role\"] == \"user\" %}\\n        {%- if tools is not none and (message == user_messages[-1]) %}\\n            {{- \"[AVAILABLE_TOOLS] [\" }}\\n            {%- for tool in tools %}\\n                {%- set tool = tool.function %}\\n                {{- \\'{\"type\": \"function\", \"function\": {\\' }}\\n                {%- for key, val in tool.items() if key != \"return\" %}\\n                    {%- if val is string %}\\n                        {{- \\'\"\\' + key + \\'\": \"\\' + val + \\'\"\\' }}\\n                    {%- else %}\\n                        {{- \\'\"\\' + key + \\'\": \\' + val|tojson }}\\n                    {%- endif %}\\n                    {%- if not loop.last %}\\n                        {{- \", \" }}\\n                    {%- endif %}\\n                {%- endfor %}\\n                {{- \"}}\" }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- else %}\\n                    {{- \"]\" }}\\n                {%- endif %}\\n            {%- endfor %}\\n            {{- \"[/AVAILABLE_TOOLS]\" }}\\n            {%- endif %}\\n        {%- if loop.last and system_message is defined %}\\n            {{- \"[INST] \" + system_message + \"\\\\n\\\\n\" + message[\"content\"] + \"[/INST]\" }}\\n        {%- else %}\\n            {{- \"[INST] \" + message[\"content\"] + \"[/INST]\" }}\\n        {%- endif %}\\n    {%- elif message.tool_calls is defined and message.tool_calls is not none %}\\n        {{- \"[TOOL_CALLS] [\" }}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- set out = tool_call.function|tojson %}\\n            {{- out[:-1] }}\\n            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\\n                {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\\n            {%- endif %}\\n            {{- \\', \"id\": \"\\' + tool_call.id + \\'\"}\\' }}\\n            {%- if not loop.last %}\\n                {{- \", \" }}\\n            {%- else %}\\n                {{- \"]\" + eos_token }}\\n            {%- endif %}\\n        {%- endfor %}\\n    {%- elif message[\"role\"] == \"assistant\" %}\\n        {{- \" \" + message[\"content\"]|trim + eos_token}}\\n    {%- elif message[\"role\"] == \"tool_results\" or message[\"role\"] == \"tool\" %}\\n        {%- if message.content is defined and message.content.content is defined %}\\n            {%- set content = message.content.content %}\\n        {%- else %}\\n            {%- set content = message.content %}\\n        {%- endif %}\\n        {{- \\'[TOOL_RESULTS] {\"content\": \\' + content|string + \", \" }}\\n        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\\n            {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\\n        {%- endif %}\\n        {{- \\'\"call_id\": \"\\' + message.tool_call_id + \\'\"}[/TOOL_RESULTS]\\' }}\\n    {%- else %}\\n        {{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}\\n    {%- endif %}\\n{%- endfor %}\\n', 'tokenizer.ggml.add_eos_token': 'false', 'general.quantization_version': '2', 'tokenizer.ggml.add_sep_token': 'false', 'general.license': 'apache-2.0', 'general.type': 'model', 'general.file_type': '7', 'general.finetune': 'Instruct', 'general.base_model.0.repo_url': 'https://huggingface.co/mistralai/Mistral-7B-v0.3', 'general.version': 'v0.3', 'general.base_model.0.name': 'Mistral 7B v0.3', 'tokenizer.ggml.add_space_prefix': 'true', 'llama.rope.dimension_count': '128', 'general.base_model.0.version': 'v0.3', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.embedding_length': '4096', 'general.size_label': '7B', 'tokenizer.ggml.add_bos_token': 'true', 'general.basename': 'Mistral', 'general.architecture': 'llama', 'general.base_model.count': '1', 'general.base_model.0.organization': 'Mistralai', 'llama.feed_forward_length': '14336', 'llama.block_count': '32', 'llama.attention.head_count': '32', 'general.name': 'Mistral 7B Instruct v0.3', 'tokenizer.ggml.bos_token_id': '1', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.pre': 'default', 'llama.vocab_size': '32768', 'tokenizer.ggml.model': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {%- if messages[0][\"role\"] == \"system\" %}\n",
            "    {%- set system_message = messages[0][\"content\"] %}\n",
            "    {%- set loop_messages = messages[1:] %}\n",
            "{%- else %}\n",
            "    {%- set loop_messages = messages %}\n",
            "{%- endif %}\n",
            "{%- if not tools is defined %}\n",
            "    {%- set tools = none %}\n",
            "{%- endif %}\n",
            "{%- set user_messages = loop_messages | selectattr(\"role\", \"equalto\", \"user\") | list %}\n",
            "\n",
            "{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\n",
            "{%- set ns = namespace() %}\n",
            "{%- set ns.index = 0 %}\n",
            "{%- for message in loop_messages %}\n",
            "    {%- if not (message.role == \"tool\" or message.role == \"tool_results\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n",
            "        {%- if (message[\"role\"] == \"user\") != (ns.index % 2 == 0) %}\n",
            "            {{- raise_exception(\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
            "        {%- endif %}\n",
            "        {%- set ns.index = ns.index + 1 %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "\n",
            "{{- bos_token }}\n",
            "{%- for message in loop_messages %}\n",
            "    {%- if message[\"role\"] == \"user\" %}\n",
            "        {%- if tools is not none and (message == user_messages[-1]) %}\n",
            "            {{- \"[AVAILABLE_TOOLS] [\" }}\n",
            "            {%- for tool in tools %}\n",
            "                {%- set tool = tool.function %}\n",
            "                {{- '{\"type\": \"function\", \"function\": {' }}\n",
            "                {%- for key, val in tool.items() if key != \"return\" %}\n",
            "                    {%- if val is string %}\n",
            "                        {{- '\"' + key + '\": \"' + val + '\"' }}\n",
            "                    {%- else %}\n",
            "                        {{- '\"' + key + '\": ' + val|tojson }}\n",
            "                    {%- endif %}\n",
            "                    {%- if not loop.last %}\n",
            "                        {{- \", \" }}\n",
            "                    {%- endif %}\n",
            "                {%- endfor %}\n",
            "                {{- \"}}\" }}\n",
            "                {%- if not loop.last %}\n",
            "                    {{- \", \" }}\n",
            "                {%- else %}\n",
            "                    {{- \"]\" }}\n",
            "                {%- endif %}\n",
            "            {%- endfor %}\n",
            "            {{- \"[/AVAILABLE_TOOLS]\" }}\n",
            "            {%- endif %}\n",
            "        {%- if loop.last and system_message is defined %}\n",
            "            {{- \"[INST] \" + system_message + \"\\n\\n\" + message[\"content\"] + \"[/INST]\" }}\n",
            "        {%- else %}\n",
            "            {{- \"[INST] \" + message[\"content\"] + \"[/INST]\" }}\n",
            "        {%- endif %}\n",
            "    {%- elif message.tool_calls is defined and message.tool_calls is not none %}\n",
            "        {{- \"[TOOL_CALLS] [\" }}\n",
            "        {%- for tool_call in message.tool_calls %}\n",
            "            {%- set out = tool_call.function|tojson %}\n",
            "            {{- out[:-1] }}\n",
            "            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\n",
            "                {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n",
            "            {%- endif %}\n",
            "            {{- ', \"id\": \"' + tool_call.id + '\"}' }}\n",
            "            {%- if not loop.last %}\n",
            "                {{- \", \" }}\n",
            "            {%- else %}\n",
            "                {{- \"]\" + eos_token }}\n",
            "            {%- endif %}\n",
            "        {%- endfor %}\n",
            "    {%- elif message[\"role\"] == \"assistant\" %}\n",
            "        {{- \" \" + message[\"content\"]|trim + eos_token}}\n",
            "    {%- elif message[\"role\"] == \"tool_results\" or message[\"role\"] == \"tool\" %}\n",
            "        {%- if message.content is defined and message.content.content is defined %}\n",
            "            {%- set content = message.content.content %}\n",
            "        {%- else %}\n",
            "            {%- set content = message.content %}\n",
            "        {%- endif %}\n",
            "        {{- '[TOOL_RESULTS] {\"content\": ' + content|string + \", \" }}\n",
            "        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\n",
            "            {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n",
            "        {%- endif %}\n",
            "        {{- '\"call_id\": \"' + message.tool_call_id + '\"}[/TOOL_RESULTS]' }}\n",
            "    {%- else %}\n",
            "        {{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "\n",
            "Using chat eos_token: </s>\n",
            "Using chat bos_token: <s>\n",
            "llama_perf_context_print:        load time =    3313.70 ms\n",
            "llama_perf_context_print: prompt eval time =    3313.31 ms /     9 tokens (  368.15 ms per token,     2.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =   96199.39 ms /    98 runs   (  981.63 ms per token,     1.02 tokens per second)\n",
            "llama_perf_context_print:       total time =   99571.56 ms /   107 tokens\n",
            "llama_perf_context_print:    graphs reused =         94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'chatcmpl-1d741f17-21f2-4513-b12b-4f60f4b6003b', 'object': 'chat.completion', 'created': 1754766737, 'model': '/content/mistral-7b-instruct-v0.3-q8_0-00001-of-00002.gguf', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \" The size of the sky is infinite, as it encompasses all of space. It extends infinitely in all directions, beyond the visible horizon, and contains all the stars, galaxies, and other celestial bodies that make up the universe. However, it's important to note that the visible part of the sky that we can see from Earth is limited by factors such as the curvature of the Earth and the presence of the atmosphere, which blocks our view of distant objects.\"}, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 9, 'completion_tokens': 98, 'total_tokens': 107}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mqwUdlTGQUiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(\n",
        "      model_path=\"./models/7B/llama-model.gguf\",\n",
        "      # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
        "      # seed=1337, # Uncomment to set a specific seed\n",
        "      # n_ctx=2048, # Uncomment to increase the context window\n",
        ")\n",
        "output = llm(\n",
        "      \"Q: Name the planets in the solar system? A: \", # Prompt\n",
        "      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
        "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
        "      echo=True # Echo the prompt back in the output\n",
        ") # Generate a completion, can also call create_completion\n",
        "print(output)"
      ],
      "metadata": {
        "id": "p7gxLSOuPY63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3-f32.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0PAem9eRE-G",
        "outputId": "53782675-9112-4e73-cdf1-ab991590de26"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-09 19:17:37--  https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3-f32.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.124, 18.172.134.4, 18.172.134.24, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.124|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/664e3ef36bc1025819154a01/5603ad244b9097bb89ae6a4c715687733668c19c4b129fd5397c2fefad33c551?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250809%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250809T191737Z&X-Amz-Expires=3600&X-Amz-Signature=ed9552a0efc4dd51786b93298d4d54735aa1383a62a768f116390e13ced2893b&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Mistral-7B-Instruct-v0.3-f32.gguf%3B+filename%3D%22Mistral-7B-Instruct-v0.3-f32.gguf%22%3B&x-id=GetObject&Expires=1754770657&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NDc3MDY1N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjRlM2VmMzZiYzEwMjU4MTkxNTRhMDEvNTYwM2FkMjQ0YjkwOTdiYjg5YWU2YTRjNzE1Njg3NzMzNjY4YzE5YzRiMTI5ZmQ1Mzk3YzJmZWZhZDMzYzU1MSoifV19&Signature=BTzxV7-aaA4sX5iDpM9EldbSN9o0Bg2idLgBWo-vtH9kB3Hh06MZDoTdGBvZW9xMAklXN8ntejU64bssixlrLoy-MWFt3gPHPCo03VtGlXPcmCUZBz-mrwof9K%7EejZP6av19x-2SJQKkdyx8J8PfXXRnzeHVgRDxtZnVcNR8QhcpvhTWLQLbE13U2nZk3kbcCJ8YGkANhOl-4WHLLgojJK8CVzW5%7EfCTNhnabGIwVRFLWQ0SLVOsA0GoxLsoXAWBRxfi2Jy8tPbM5zQM--RgBHNrQLJz4etbKoXwIanL6mRSAJ-wVdpQ2eSjfhvthB93Y31pT58e0kyWB3DL7x3xnA__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-08-09 19:17:37--  https://cas-bridge.xethub.hf.co/xet-bridge-us/664e3ef36bc1025819154a01/5603ad244b9097bb89ae6a4c715687733668c19c4b129fd5397c2fefad33c551?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250809%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250809T191737Z&X-Amz-Expires=3600&X-Amz-Signature=ed9552a0efc4dd51786b93298d4d54735aa1383a62a768f116390e13ced2893b&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Mistral-7B-Instruct-v0.3-f32.gguf%3B+filename%3D%22Mistral-7B-Instruct-v0.3-f32.gguf%22%3B&x-id=GetObject&Expires=1754770657&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NDc3MDY1N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjRlM2VmMzZiYzEwMjU4MTkxNTRhMDEvNTYwM2FkMjQ0YjkwOTdiYjg5YWU2YTRjNzE1Njg3NzMzNjY4YzE5YzRiMTI5ZmQ1Mzk3YzJmZWZhZDMzYzU1MSoifV19&Signature=BTzxV7-aaA4sX5iDpM9EldbSN9o0Bg2idLgBWo-vtH9kB3Hh06MZDoTdGBvZW9xMAklXN8ntejU64bssixlrLoy-MWFt3gPHPCo03VtGlXPcmCUZBz-mrwof9K%7EejZP6av19x-2SJQKkdyx8J8PfXXRnzeHVgRDxtZnVcNR8QhcpvhTWLQLbE13U2nZk3kbcCJ8YGkANhOl-4WHLLgojJK8CVzW5%7EfCTNhnabGIwVRFLWQ0SLVOsA0GoxLsoXAWBRxfi2Jy8tPbM5zQM--RgBHNrQLJz4etbKoXwIanL6mRSAJ-wVdpQ2eSjfhvthB93Y31pT58e0kyWB3DL7x3xnA__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 3.162.163.41, 3.162.163.68, 3.162.163.2, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|3.162.163.41|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28992851904 (27G)\n",
            "Saving to: ‘Mistral-7B-Instruct-v0.3-f32.gguf’\n",
            "\n",
            "Mistral-7B-Instruct 100%[===================>]  27.00G  82.6MB/s    in 6m 36s  \n",
            "\n",
            "2025-08-09 19:24:14 (69.7 MB/s) - ‘Mistral-7B-Instruct-v0.3-f32.gguf’ saved [28992851904/28992851904]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/ggml-org/llama.cpp/releases/download/b6123/llama-b6123-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyTtHUU0RGMB",
        "outputId": "06ad2d13-f280-4ad0-ce9b-3ad0351ca56b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-09 19:24:19--  https://github.com/ggml-org/llama.cpp/releases/download/b6123/llama-b6123-bin-ubuntu-x64.zip\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/612354784/ca452524-a49c-474c-b92a-a8e8622c057c?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-08-09T19%3A59%3A41Z&rscd=attachment%3B+filename%3Dllama-b6123-bin-ubuntu-x64.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-08-09T18%3A59%3A07Z&ske=2025-08-09T19%3A59%3A41Z&sks=b&skv=2018-11-09&sig=EibVEvNLseRGNYIB5J03nNLOLwAJDJ%2BEF6O5zaEd4L4%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1NDc2Nzc2MCwibmJmIjoxNzU0NzY3NDYwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.BqxpvSLrsHytBkAgb9G3__3ZwII_H8j_Y5Bg5g1TZ98&response-content-disposition=attachment%3B%20filename%3Dllama-b6123-bin-ubuntu-x64.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-08-09 19:24:20--  https://release-assets.githubusercontent.com/github-production-release-asset/612354784/ca452524-a49c-474c-b92a-a8e8622c057c?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-08-09T19%3A59%3A41Z&rscd=attachment%3B+filename%3Dllama-b6123-bin-ubuntu-x64.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-08-09T18%3A59%3A07Z&ske=2025-08-09T19%3A59%3A41Z&sks=b&skv=2018-11-09&sig=EibVEvNLseRGNYIB5J03nNLOLwAJDJ%2BEF6O5zaEd4L4%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1NDc2Nzc2MCwibmJmIjoxNzU0NzY3NDYwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.BqxpvSLrsHytBkAgb9G3__3ZwII_H8j_Y5Bg5g1TZ98&response-content-disposition=attachment%3B%20filename%3Dllama-b6123-bin-ubuntu-x64.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13339223 (13M) [application/octet-stream]\n",
            "Saving to: ‘llama-b6123-bin-ubuntu-x64.zip’\n",
            "\n",
            "llama-b6123-bin-ubu 100%[===================>]  12.72M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-08-09 19:24:20 (100 MB/s) - ‘llama-b6123-bin-ubuntu-x64.zip’ saved [13339223/13339223]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip llama-b6123-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MCyGa2ZSpQX",
        "outputId": "1bee9dad-8596-4cd9-ef02-d905bb014b71"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  llama-b6123-bin-ubuntu-x64.zip\n",
            "  inflating: build/bin/LICENSE       \n",
            "  inflating: build/bin/LICENSE-curl  \n",
            "  inflating: build/bin/LICENSE-httplib  \n",
            "  inflating: build/bin/LICENSE-jsonhpp  \n",
            "  inflating: build/bin/LICENSE-linenoise  \n",
            "  inflating: build/bin/libggml-base.so  \n",
            "  inflating: build/bin/libggml-cpu-alderlake.so  \n",
            "  inflating: build/bin/libggml-cpu-haswell.so  \n",
            "  inflating: build/bin/libggml-cpu-icelake.so  \n",
            "  inflating: build/bin/libggml-cpu-sandybridge.so  \n",
            "  inflating: build/bin/libggml-cpu-sapphirerapids.so  \n",
            "  inflating: build/bin/libggml-cpu-skylakex.so  \n",
            "  inflating: build/bin/libggml-cpu-sse42.so  \n",
            "  inflating: build/bin/libggml-cpu-x64.so  \n",
            "  inflating: build/bin/libggml-rpc.so  \n",
            "  inflating: build/bin/libggml.so    \n",
            "  inflating: build/bin/libllama.so   \n",
            "  inflating: build/bin/libmtmd.so    \n",
            "  inflating: build/bin/llama-batched-bench  \n",
            "  inflating: build/bin/llama-bench   \n",
            "  inflating: build/bin/llama-cli     \n",
            "  inflating: build/bin/llama-gemma3-cli  \n",
            "  inflating: build/bin/llama-gguf-split  \n",
            "  inflating: build/bin/llama-imatrix  \n",
            "  inflating: build/bin/llama-llava-cli  \n",
            "  inflating: build/bin/llama-minicpmv-cli  \n",
            "  inflating: build/bin/llama-mtmd-cli  \n",
            "  inflating: build/bin/llama-perplexity  \n",
            "  inflating: build/bin/llama-quantize  \n",
            "  inflating: build/bin/llama-qwen2vl-cli  \n",
            "  inflating: build/bin/llama-run     \n",
            "  inflating: build/bin/llama-server  \n",
            "  inflating: build/bin/llama-tokenize  \n",
            "  inflating: build/bin/llama-tts     \n",
            "  inflating: build/bin/rpc-server    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./llama-quantize ./models/mymodel/ggml-model-f16.gguf ./models/mymodel/ggml-model-Q4_K_M.gguf Q4_K_M\n"
      ],
      "metadata": {
        "id": "6lsF0mJ5Scbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "./llama-quantize ./models/mymodel/ggml-model-f16.gguf ./models/mymodel/ggml-model-Q4_K_M.gguf Q4_K_M\n"
      ],
      "metadata": {
        "id": "-tA6Y7cjSfAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-quantize -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHiFfzaSS2Mf",
        "outputId": "65d8742c-0e6d-4622-93f2-9d26485bb90b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: build/bin/llama-quantize [--help] [--allow-requantize] [--leave-output-tensor] [--pure] [--imatrix] [--include-weights]\n",
            "       [--exclude-weights] [--output-tensor-type] [--token-embedding-type] [--tensor-type] [--prune-layers] [--keep-split] [--override-kv]\n",
            "       model-f32.gguf [model-quant.gguf] type [nthreads]\n",
            "\n",
            "  --allow-requantize: Allows requantizing tensors that have already been quantized. Warning: This can severely reduce quality compared to quantizing from 16bit or 32bit\n",
            "  --leave-output-tensor: Will leave output.weight un(re)quantized. Increases model size but may also increase quality, especially when requantizing\n",
            "  --pure: Disable k-quant mixtures and quantize all tensors to the same type\n",
            "  --imatrix file_name: use data in file_name as importance matrix for quant optimizations\n",
            "  --include-weights tensor_name: use importance matrix for this/these tensor(s)\n",
            "  --exclude-weights tensor_name: use importance matrix for this/these tensor(s)\n",
            "  --output-tensor-type ggml_type: use this ggml_type for the output.weight tensor\n",
            "  --token-embedding-type ggml_type: use this ggml_type for the token embeddings tensor\n",
            "  --tensor-type TENSOR=TYPE: quantize this tensor to this ggml_type. example: --tensor-type attn_q=q8_0\n",
            "      Advanced option to selectively quantize tensors. May be specified multiple times.\n",
            "  --prune-layers L0,L1,L2...comma-separated list of layer numbers to prune from the model\n",
            "      Advanced option to remove all tensors from the given layers\n",
            "  --keep-split: will generate quantized model in the same shards as input\n",
            "  --override-kv KEY=TYPE:VALUE\n",
            "      Advanced option to override model metadata by key in the quantized model. May be specified multiple times.\n",
            "Note: --include-weights and --exclude-weights cannot be used together\n",
            "\n",
            "Allowed quantization types:\n",
            "   2  or  Q4_0    :  4.34G, +0.4685 ppl @ Llama-3-8B\n",
            "   3  or  Q4_1    :  4.78G, +0.4511 ppl @ Llama-3-8B\n",
            "  38  or  MXFP4_MOE :  MXFP4 MoE\n",
            "   8  or  Q5_0    :  5.21G, +0.1316 ppl @ Llama-3-8B\n",
            "   9  or  Q5_1    :  5.65G, +0.1062 ppl @ Llama-3-8B\n",
            "  19  or  IQ2_XXS :  2.06 bpw quantization\n",
            "  20  or  IQ2_XS  :  2.31 bpw quantization\n",
            "  28  or  IQ2_S   :  2.5  bpw quantization\n",
            "  29  or  IQ2_M   :  2.7  bpw quantization\n",
            "  24  or  IQ1_S   :  1.56 bpw quantization\n",
            "  31  or  IQ1_M   :  1.75 bpw quantization\n",
            "  36  or  TQ1_0   :  1.69 bpw ternarization\n",
            "  37  or  TQ2_0   :  2.06 bpw ternarization\n",
            "  10  or  Q2_K    :  2.96G, +3.5199 ppl @ Llama-3-8B\n",
            "  21  or  Q2_K_S  :  2.96G, +3.1836 ppl @ Llama-3-8B\n",
            "  23  or  IQ3_XXS :  3.06 bpw quantization\n",
            "  26  or  IQ3_S   :  3.44 bpw quantization\n",
            "  27  or  IQ3_M   :  3.66 bpw quantization mix\n",
            "  12  or  Q3_K    : alias for Q3_K_M\n",
            "  22  or  IQ3_XS  :  3.3 bpw quantization\n",
            "  11  or  Q3_K_S  :  3.41G, +1.6321 ppl @ Llama-3-8B\n",
            "  12  or  Q3_K_M  :  3.74G, +0.6569 ppl @ Llama-3-8B\n",
            "  13  or  Q3_K_L  :  4.03G, +0.5562 ppl @ Llama-3-8B\n",
            "  25  or  IQ4_NL  :  4.50 bpw non-linear quantization\n",
            "  30  or  IQ4_XS  :  4.25 bpw non-linear quantization\n",
            "  15  or  Q4_K    : alias for Q4_K_M\n",
            "  14  or  Q4_K_S  :  4.37G, +0.2689 ppl @ Llama-3-8B\n",
            "  15  or  Q4_K_M  :  4.58G, +0.1754 ppl @ Llama-3-8B\n",
            "  17  or  Q5_K    : alias for Q5_K_M\n",
            "  16  or  Q5_K_S  :  5.21G, +0.1049 ppl @ Llama-3-8B\n",
            "  17  or  Q5_K_M  :  5.33G, +0.0569 ppl @ Llama-3-8B\n",
            "  18  or  Q6_K    :  6.14G, +0.0217 ppl @ Llama-3-8B\n",
            "   7  or  Q8_0    :  7.96G, +0.0026 ppl @ Llama-3-8B\n",
            "   1  or  F16     : 14.00G, +0.0020 ppl @ Mistral-7B\n",
            "  32  or  BF16    : 14.00G, -0.0050 ppl @ Mistral-7B\n",
            "   0  or  F32     : 26.00G              @ 7B\n",
            "          COPY    : only copy tensors, no quantizing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-quantize /content/Mistral-7B-Instruct-v0.3-f32.gguf /content/Mistral-7B-Instruct-v0.3-bf16.gguf BF16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOQ4VurnS3PH",
        "outputId": "39c608fe-af45-4b7d-f8d2-f918aa6b26ec"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 6123 (79c1160b)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/Mistral-7B-Instruct-v0.3-f32.gguf' to '/content/Mistral-7B-Instruct-v0.3-bf16.gguf' as BF16\n",
            "llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from /content/Mistral-7B-Instruct-v0.3-f32.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 0\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  291 tensors\n",
            "[   1/ 291]                        output.weight - [ 4096, 32768,     1,     1], type =    f32, converting to bf16 .. size =   512.00 MiB ->   256.00 MiB\n",
            "[   2/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   3/ 291]                    token_embd.weight - [ 4096, 32768,     1,     1], type =    f32, converting to bf16 .. size =   512.00 MiB ->   256.00 MiB\n",
            "[   4/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[   5/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   6/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[   7/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[   8/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[   9/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  10/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  11/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  12/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  13/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[  14/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  15/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[  16/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[  17/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[  18/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  19/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  20/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  21/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  22/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[  23/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  24/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[  25/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[  26/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[  27/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  28/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  29/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  30/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  31/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[  32/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  33/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[  34/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[  35/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[  36/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  37/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  38/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  39/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  40/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[  41/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  42/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[  43/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[  44/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[  45/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  46/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  47/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  48/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  49/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[  50/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  51/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[  52/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[  53/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[  54/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  55/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  56/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  57/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  58/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[  59/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  60/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[  61/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[  62/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[  63/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  64/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  65/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  66/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  67/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[  68/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  69/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[  70/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[  71/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[  72/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  73/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  74/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  75/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  76/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[  77/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  78/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[  79/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[  80/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[  81/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  82/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  83/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  84/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  85/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[  86/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  87/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[  88/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[  89/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[  90/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  91/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  92/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  93/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[  94/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[  95/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  96/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[  97/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[  98/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[  99/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 100/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 101/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 102/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 103/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 104/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 105/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 106/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 107/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 108/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 109/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 110/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 111/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 112/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 113/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 114/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 115/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 116/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 117/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 118/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 119/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 120/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 121/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 122/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 123/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 124/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 125/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 126/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 127/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 128/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 129/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 130/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 131/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 132/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 133/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 134/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 135/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 136/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 137/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 138/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 139/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 140/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 141/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 142/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 143/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 144/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 145/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 146/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 147/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 148/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 149/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 150/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 151/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 152/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 153/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 154/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 155/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 156/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 157/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 158/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 159/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 160/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 161/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 162/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 163/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 164/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 165/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 166/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 167/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 168/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 169/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 170/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 171/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 172/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 173/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 174/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 175/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 176/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 177/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 178/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 179/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 180/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 181/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 182/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 183/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 184/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 185/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 186/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 187/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 188/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 189/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 190/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 191/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 192/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 193/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 194/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 195/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 196/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 197/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 198/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 199/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 200/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 201/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 202/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 203/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 204/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 205/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 206/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 207/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 208/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 209/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 210/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 211/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 212/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 213/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 214/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 215/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 216/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 217/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 218/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 219/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 220/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 221/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 222/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 223/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 224/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 225/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 226/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 227/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 228/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 229/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 230/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 231/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 232/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 233/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 234/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 235/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 236/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 237/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 238/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 239/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 240/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 241/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 242/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 243/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 244/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 245/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 246/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 247/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 248/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 249/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 250/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 251/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 252/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 253/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 254/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 255/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 256/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 257/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 258/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 259/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 260/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 261/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 262/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 263/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 264/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 265/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 266/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 267/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 268/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 269/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 270/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 271/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 272/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 273/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 274/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 275/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 276/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 277/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 278/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 279/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 280/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 281/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 282/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 283/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 284/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 285/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 286/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to bf16 .. size =    64.00 MiB ->    32.00 MiB\n",
            "[ 287/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to bf16 .. size =    16.00 MiB ->     8.00 MiB\n",
            "[ 288/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 289/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "[ 290/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 291/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to bf16 .. size =   224.00 MiB ->   112.00 MiB\n",
            "llama_model_quantize_impl: model size  = 27649.02 MB\n",
            "llama_model_quantize_impl: quant size  = 13825.02 MB\n",
            "\n",
            "main: quantize time = 219309.24 ms\n",
            "main:    total time = 219309.25 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/build/bin/llama-gguf-split -h"
      ],
      "metadata": {
        "id": "3X8PVVYwT1Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-cli -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQyrNJzNUtj4",
        "outputId": "b603cdff-79ec-4859-8387-135def6e33d0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_backend: loaded RPC backend from /content/build/bin/libggml-rpc.so\n",
            "load_backend: loaded CPU backend from /content/build/bin/libggml-cpu-haswell.so\n",
            "----- common params -----\n",
            "\n",
            "-h,    --help, --usage                  print usage and exit\n",
            "--version                               show version and build info\n",
            "--completion-bash                       print source-able bash completion script for llama.cpp\n",
            "--verbose-prompt                        print a verbose prompt before generation (default: false)\n",
            "-t,    --threads N                      number of threads to use during generation (default: -1)\n",
            "                                        (env: LLAMA_ARG_THREADS)\n",
            "-tb,   --threads-batch N                number of threads to use during batch and prompt processing (default:\n",
            "                                        same as --threads)\n",
            "-C,    --cpu-mask M                     CPU affinity mask: arbitrarily long hex. Complements cpu-range\n",
            "                                        (default: \"\")\n",
            "-Cr,   --cpu-range lo-hi                range of CPUs for affinity. Complements --cpu-mask\n",
            "--cpu-strict <0|1>                      use strict CPU placement (default: 0)\n",
            "--prio N                                set process/thread priority : low(-1), normal(0), medium(1), high(2),\n",
            "                                        realtime(3) (default: 0)\n",
            "--poll <0...100>                        use polling level to wait for work (0 - no polling, default: 50)\n",
            "-Cb,   --cpu-mask-batch M               CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch\n",
            "                                        (default: same as --cpu-mask)\n",
            "-Crb,  --cpu-range-batch lo-hi          ranges of CPUs for affinity. Complements --cpu-mask-batch\n",
            "--cpu-strict-batch <0|1>                use strict CPU placement (default: same as --cpu-strict)\n",
            "--prio-batch N                          set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
            "                                        (default: 0)\n",
            "--poll-batch <0|1>                      use polling to wait for work (default: same as --poll)\n",
            "-c,    --ctx-size N                     size of the prompt context (default: 4096, 0 = loaded from model)\n",
            "                                        (env: LLAMA_ARG_CTX_SIZE)\n",
            "-n,    --predict, --n-predict N         number of tokens to predict (default: -1, -1 = infinity, -2 = until\n",
            "                                        context filled)\n",
            "                                        (env: LLAMA_ARG_N_PREDICT)\n",
            "-b,    --batch-size N                   logical maximum batch size (default: 2048)\n",
            "                                        (env: LLAMA_ARG_BATCH)\n",
            "-ub,   --ubatch-size N                  physical maximum batch size (default: 512)\n",
            "                                        (env: LLAMA_ARG_UBATCH)\n",
            "--keep N                                number of tokens to keep from the initial prompt (default: 0, -1 =\n",
            "                                        all)\n",
            "--swa-full                              use full-size SWA cache (default: false)\n",
            "                                        [(more\n",
            "                                        info)](https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n",
            "                                        (env: LLAMA_ARG_SWA_FULL)\n",
            "--kv-unified, -kvu                      use single unified KV buffer for the KV cache of all sequences\n",
            "                                        (default: false)\n",
            "                                        [(more info)](https://github.com/ggml-org/llama.cpp/pull/14363)\n",
            "                                        (env: LLAMA_ARG_KV_SPLIT)\n",
            "-fa,   --flash-attn                     enable Flash Attention (default: disabled)\n",
            "                                        (env: LLAMA_ARG_FLASH_ATTN)\n",
            "-p,    --prompt PROMPT                  prompt to start generation with; for system message, use -sys\n",
            "--no-perf                               disable internal libllama performance timings (default: false)\n",
            "                                        (env: LLAMA_ARG_NO_PERF)\n",
            "-f,    --file FNAME                     a file containing the prompt (default: none)\n",
            "-bf,   --binary-file FNAME              binary file containing the prompt (default: none)\n",
            "-e,    --escape                         process escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\) (default: true)\n",
            "--no-escape                             do not process escape sequences\n",
            "--rope-scaling {none,linear,yarn}       RoPE frequency scaling method, defaults to linear unless specified by\n",
            "                                        the model\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALING_TYPE)\n",
            "--rope-scale N                          RoPE context scaling factor, expands context by a factor of N\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALE)\n",
            "--rope-freq-base N                      RoPE base frequency, used by NTK-aware scaling (default: loaded from\n",
            "                                        model)\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_BASE)\n",
            "--rope-freq-scale N                     RoPE frequency scaling factor, expands context by a factor of 1/N\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_SCALE)\n",
            "--yarn-orig-ctx N                       YaRN: original context size of model (default: 0 = model training\n",
            "                                        context size)\n",
            "                                        (env: LLAMA_ARG_YARN_ORIG_CTX)\n",
            "--yarn-ext-factor N                     YaRN: extrapolation mix factor (default: -1.0, 0.0 = full\n",
            "                                        interpolation)\n",
            "                                        (env: LLAMA_ARG_YARN_EXT_FACTOR)\n",
            "--yarn-attn-factor N                    YaRN: scale sqrt(t) or attention magnitude (default: 1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_ATTN_FACTOR)\n",
            "--yarn-beta-slow N                      YaRN: high correction dim or alpha (default: 1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_SLOW)\n",
            "--yarn-beta-fast N                      YaRN: low correction dim or beta (default: 32.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_FAST)\n",
            "-nkvo, --no-kv-offload                  disable KV offload\n",
            "                                        (env: LLAMA_ARG_NO_KV_OFFLOAD)\n",
            "-nr,   --no-repack                      disable weight repacking\n",
            "                                        (env: LLAMA_ARG_NO_REPACK)\n",
            "-ctk,  --cache-type-k TYPE              KV cache data type for K\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_K)\n",
            "-ctv,  --cache-type-v TYPE              KV cache data type for V\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_V)\n",
            "-dt,   --defrag-thold N                 KV cache defragmentation threshold (default: 0.1, < 0 - disabled)\n",
            "                                        (env: LLAMA_ARG_DEFRAG_THOLD)\n",
            "-np,   --parallel N                     number of parallel sequences to decode (default: 1)\n",
            "                                        (env: LLAMA_ARG_N_PARALLEL)\n",
            "--rpc SERVERS                           comma separated list of RPC servers\n",
            "                                        (env: LLAMA_ARG_RPC)\n",
            "--mlock                                 force system to keep model in RAM rather than swapping or compressing\n",
            "                                        (env: LLAMA_ARG_MLOCK)\n",
            "--no-mmap                               do not memory-map model (slower load but may reduce pageouts if not\n",
            "                                        using mlock)\n",
            "                                        (env: LLAMA_ARG_NO_MMAP)\n",
            "--numa TYPE                             attempt optimizations that help on some NUMA systems\n",
            "                                        - distribute: spread execution evenly over all nodes\n",
            "                                        - isolate: only spawn threads on CPUs on the node that execution\n",
            "                                        started on\n",
            "                                        - numactl: use the CPU map provided by numactl\n",
            "                                        if run without this previously, it is recommended to drop the system\n",
            "                                        page cache before using this\n",
            "                                        see https://github.com/ggml-org/llama.cpp/issues/1437\n",
            "                                        (env: LLAMA_ARG_NUMA)\n",
            "-dev,  --device <dev1,dev2,..>          comma-separated list of devices to use for offloading (none = don't\n",
            "                                        offload)\n",
            "                                        use --list-devices to see a list of available devices\n",
            "                                        (env: LLAMA_ARG_DEVICE)\n",
            "--list-devices                          print list of available devices and exit\n",
            "--override-tensor, -ot <tensor name pattern>=<buffer type>,...\n",
            "                                        override tensor buffer type\n",
            "--cpu-moe, -cmoe                        keep all Mixture of Experts (MoE) weights in the CPU\n",
            "                                        (env: LLAMA_ARG_CPU_MOE)\n",
            "--n-cpu-moe, -ncmoe N                   keep the Mixture of Experts (MoE) weights of the first N layers in the\n",
            "                                        CPU\n",
            "                                        (env: LLAMA_ARG_N_CPU_MOE)\n",
            "-ngl,  --gpu-layers, --n-gpu-layers N   number of layers to store in VRAM\n",
            "                                        (env: LLAMA_ARG_N_GPU_LAYERS)\n",
            "-sm,   --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:\n",
            "                                        - none: use one GPU only\n",
            "                                        - layer (default): split layers and KV across GPUs\n",
            "                                        - row: split rows across GPUs\n",
            "                                        (env: LLAMA_ARG_SPLIT_MODE)\n",
            "-ts,   --tensor-split N0,N1,N2,...      fraction of the model to offload to each GPU, comma-separated list of\n",
            "                                        proportions, e.g. 3,1\n",
            "                                        (env: LLAMA_ARG_TENSOR_SPLIT)\n",
            "-mg,   --main-gpu INDEX                 the GPU to use for the model (with split-mode = none), or for\n",
            "                                        intermediate results and KV (with split-mode = row) (default: 0)\n",
            "                                        (env: LLAMA_ARG_MAIN_GPU)\n",
            "--check-tensors                         check model tensor data for invalid values (default: false)\n",
            "--override-kv KEY=TYPE:VALUE            advanced option to override model metadata by key. may be specified\n",
            "                                        multiple times.\n",
            "                                        types: int, float, bool, str. example: --override-kv\n",
            "                                        tokenizer.ggml.add_bos_token=bool:false\n",
            "--no-op-offload                         disable offloading host tensor operations to device (default: false)\n",
            "--lora FNAME                            path to LoRA adapter (can be repeated to use multiple adapters)\n",
            "--lora-scaled FNAME SCALE               path to LoRA adapter with user defined scaling (can be repeated to use\n",
            "                                        multiple adapters)\n",
            "--control-vector FNAME                  add a control vector\n",
            "                                        note: this argument can be repeated to add multiple control vectors\n",
            "--control-vector-scaled FNAME SCALE     add a control vector with user defined scaling SCALE\n",
            "                                        note: this argument can be repeated to add multiple scaled control\n",
            "                                        vectors\n",
            "--control-vector-layer-range START END\n",
            "                                        layer range to apply the control vector(s) to, start and end inclusive\n",
            "-m,    --model FNAME                    model path (default: `models/$filename` with filename from `--hf-file`\n",
            "                                        or `--model-url` if set, otherwise models/7B/ggml-model-f16.gguf)\n",
            "                                        (env: LLAMA_ARG_MODEL)\n",
            "-mu,   --model-url MODEL_URL            model download url (default: unused)\n",
            "                                        (env: LLAMA_ARG_MODEL_URL)\n",
            "-hf,   -hfr, --hf-repo <user>/<model>[:quant]\n",
            "                                        Hugging Face model repository; quant is optional, case-insensitive,\n",
            "                                        default to Q4_K_M, or falls back to the first file in the repo if\n",
            "                                        Q4_K_M doesn't exist.\n",
            "                                        mmproj is also downloaded automatically if available. to disable, add\n",
            "                                        --no-mmproj\n",
            "                                        example: unsloth/phi-4-GGUF:q4_k_m\n",
            "                                        (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_REPO)\n",
            "-hfd,  -hfrd, --hf-repo-draft <user>/<model>[:quant]\n",
            "                                        Same as --hf-repo, but for the draft model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HFD_REPO)\n",
            "-hff,  --hf-file FILE                   Hugging Face model file. If specified, it will override the quant in\n",
            "                                        --hf-repo (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_FILE)\n",
            "-hfv,  -hfrv, --hf-repo-v <user>/<model>[:quant]\n",
            "                                        Hugging Face model repository for the vocoder model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_REPO_V)\n",
            "-hffv, --hf-file-v FILE                 Hugging Face model file for the vocoder model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_FILE_V)\n",
            "-hft,  --hf-token TOKEN                 Hugging Face access token (default: value from HF_TOKEN environment\n",
            "                                        variable)\n",
            "                                        (env: HF_TOKEN)\n",
            "--log-disable                           Log disable\n",
            "--log-file FNAME                        Log to file\n",
            "--log-colors                            Enable colored logging\n",
            "                                        (env: LLAMA_LOG_COLORS)\n",
            "-v,    --verbose, --log-verbose         Set verbosity level to infinity (i.e. log all messages, useful for\n",
            "                                        debugging)\n",
            "--offline                               Offline mode: forces use of cache, prevents network access\n",
            "                                        (env: LLAMA_OFFLINE)\n",
            "-lv,   --verbosity, --log-verbosity N   Set the verbosity threshold. Messages with a higher verbosity will be\n",
            "                                        ignored.\n",
            "                                        (env: LLAMA_LOG_VERBOSITY)\n",
            "--log-prefix                            Enable prefix in log messages\n",
            "                                        (env: LLAMA_LOG_PREFIX)\n",
            "--log-timestamps                        Enable timestamps in log messages\n",
            "                                        (env: LLAMA_LOG_TIMESTAMPS)\n",
            "-ctkd, --cache-type-k-draft TYPE        KV cache data type for K for the draft model\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_K_DRAFT)\n",
            "-ctvd, --cache-type-v-draft TYPE        KV cache data type for V for the draft model\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_V_DRAFT)\n",
            "\n",
            "\n",
            "----- sampling params -----\n",
            "\n",
            "--samplers SAMPLERS                     samplers that will be used for generation in the order, separated by\n",
            "                                        ';'\n",
            "                                        (default:\n",
            "                                        penalties;dry;top_n_sigma;top_k;typ_p;top_p;min_p;xtc;temperature)\n",
            "-s,    --seed SEED                      RNG seed (default: -1, use random seed for -1)\n",
            "--sampling-seq, --sampler-seq SEQUENCE\n",
            "                                        simplified sequence for samplers that will be used (default:\n",
            "                                        edskypmxt)\n",
            "--ignore-eos                            ignore end of stream token and continue generating (implies\n",
            "                                        --logit-bias EOS-inf)\n",
            "--temp N                                temperature (default: 0.8)\n",
            "--top-k N                               top-k sampling (default: 40, 0 = disabled)\n",
            "--top-p N                               top-p sampling (default: 0.9, 1.0 = disabled)\n",
            "--min-p N                               min-p sampling (default: 0.1, 0.0 = disabled)\n",
            "--top-nsigma N                          top-n-sigma sampling (default: -1.0, -1.0 = disabled)\n",
            "--xtc-probability N                     xtc probability (default: 0.0, 0.0 = disabled)\n",
            "--xtc-threshold N                       xtc threshold (default: 0.1, 1.0 = disabled)\n",
            "--typical N                             locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)\n",
            "--repeat-last-n N                       last n tokens to consider for penalize (default: 64, 0 = disabled, -1\n",
            "                                        = ctx_size)\n",
            "--repeat-penalty N                      penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)\n",
            "--presence-penalty N                    repeat alpha presence penalty (default: 0.0, 0.0 = disabled)\n",
            "--frequency-penalty N                   repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)\n",
            "--dry-multiplier N                      set DRY sampling multiplier (default: 0.0, 0.0 = disabled)\n",
            "--dry-base N                            set DRY sampling base value (default: 1.75)\n",
            "--dry-allowed-length N                  set allowed length for DRY sampling (default: 2)\n",
            "--dry-penalty-last-n N                  set DRY penalty for the last n tokens (default: -1, 0 = disable, -1 =\n",
            "                                        context size)\n",
            "--dry-sequence-breaker STRING           add sequence breaker for DRY sampling, clearing out default breakers\n",
            "                                        ('\\n', ':', '\"', '*') in the process; use \"none\" to not use any\n",
            "                                        sequence breakers\n",
            "--dynatemp-range N                      dynamic temperature range (default: 0.0, 0.0 = disabled)\n",
            "--dynatemp-exp N                        dynamic temperature exponent (default: 1.0)\n",
            "--mirostat N                            use Mirostat sampling.\n",
            "                                        Top K, Nucleus and Locally Typical samplers are ignored if used.\n",
            "                                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n",
            "--mirostat-lr N                         Mirostat learning rate, parameter eta (default: 0.1)\n",
            "--mirostat-ent N                        Mirostat target entropy, parameter tau (default: 5.0)\n",
            "-l,    --logit-bias TOKEN_ID(+/-)BIAS   modifies the likelihood of token appearing in the completion,\n",
            "                                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',\n",
            "                                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'\n",
            "--grammar GRAMMAR                       BNF-like grammar to constrain generations (see samples in grammars/\n",
            "                                        dir) (default: '')\n",
            "--grammar-file FNAME                    file to read grammar from\n",
            "-j,    --json-schema SCHEMA             JSON schema to constrain generations (https://json-schema.org/), e.g.\n",
            "                                        `{}` for any JSON object\n",
            "                                        For schemas w/ external $refs, use --grammar +\n",
            "                                        example/json_schema_to_grammar.py instead\n",
            "-jf,   --json-schema-file FILE          File containing a JSON schema to constrain generations\n",
            "                                        (https://json-schema.org/), e.g. `{}` for any JSON object\n",
            "                                        For schemas w/ external $refs, use --grammar +\n",
            "                                        example/json_schema_to_grammar.py instead\n",
            "\n",
            "\n",
            "----- example-specific params -----\n",
            "\n",
            "--no-display-prompt                     don't print prompt at generation (default: false)\n",
            "-co,   --color                          colorise output to distinguish prompt and user input from generations\n",
            "                                        (default: false)\n",
            "--no-context-shift                      disables context shift on infinite text generation (default: disabled)\n",
            "                                        (env: LLAMA_ARG_NO_CONTEXT_SHIFT)\n",
            "-sys,  --system-prompt PROMPT           system prompt to use with model (if applicable, depending on chat\n",
            "                                        template)\n",
            "-sysf, --system-prompt-file FNAME       a file containing the system prompt (default: none)\n",
            "-ptc,  --print-token-count N            print token count every N tokens (default: -1)\n",
            "--prompt-cache FNAME                    file to cache prompt state for faster startup (default: none)\n",
            "--prompt-cache-all                      if specified, saves user input and generations to cache as well\n",
            "--prompt-cache-ro                       if specified, uses the prompt cache but does not update it\n",
            "-r,    --reverse-prompt PROMPT          halt generation at PROMPT, return control in interactive mode\n",
            "-sp,   --special                        special tokens output enabled (default: false)\n",
            "-cnv,  --conversation                   run in conversation mode:\n",
            "                                        - does not print special tokens and suffix/prefix\n",
            "                                        - interactive mode is also enabled\n",
            "                                        (default: auto enabled if chat template is available)\n",
            "-no-cnv, --no-conversation              force disable conversation mode (default: false)\n",
            "-st,   --single-turn                    run conversation for a single turn only, then exit when done\n",
            "                                        will not be interactive if first turn is predefined with --prompt\n",
            "                                        (default: false)\n",
            "-i,    --interactive                    run in interactive mode (default: false)\n",
            "-if,   --interactive-first              run in interactive mode and wait for input right away (default: false)\n",
            "-mli,  --multiline-input                allows you to write or paste multiple lines without ending each in '\\'\n",
            "--in-prefix-bos                         prefix BOS to user inputs, preceding the `--in-prefix` string\n",
            "--in-prefix STRING                      string to prefix user inputs with (default: empty)\n",
            "--in-suffix STRING                      string to suffix after user inputs with (default: empty)\n",
            "--no-warmup                             skip warming up the model with an empty run\n",
            "-gan,  --grp-attn-n N                   group-attention factor (default: 1)\n",
            "                                        (env: LLAMA_ARG_GRP_ATTN_N)\n",
            "-gaw,  --grp-attn-w N                   group-attention width (default: 512)\n",
            "                                        (env: LLAMA_ARG_GRP_ATTN_W)\n",
            "--jinja                                 use jinja template for chat (default: disabled)\n",
            "                                        (env: LLAMA_ARG_JINJA)\n",
            "--reasoning-format FORMAT               controls whether thought tags are allowed and/or extracted from the\n",
            "                                        response, and in which format they're returned; one of:\n",
            "                                        - none: leaves thoughts unparsed in `message.content`\n",
            "                                        - deepseek: puts thoughts in `message.reasoning_content` (except in\n",
            "                                        streaming mode, which behaves as `none`)\n",
            "                                        (default: auto)\n",
            "                                        (env: LLAMA_ARG_THINK)\n",
            "--reasoning-budget N                    controls the amount of thinking allowed; currently only one of: -1 for\n",
            "                                        unrestricted thinking budget, or 0 to disable thinking (default: -1)\n",
            "                                        (env: LLAMA_ARG_THINK_BUDGET)\n",
            "--chat-template JINJA_TEMPLATE          set custom jinja chat template (default: template taken from model's\n",
            "                                        metadata)\n",
            "                                        if suffix/prefix are specified, template will be disabled\n",
            "                                        only commonly used templates are accepted (unless --jinja is set\n",
            "                                        before this flag):\n",
            "                                        list of built-in templates:\n",
            "                                        bailing, chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2,\n",
            "                                        deepseek3, exaone3, exaone4, falcon3, gemma, gigachat, glmedge,\n",
            "                                        gpt-oss, granite, hunyuan-dense, hunyuan-moe, kimi-k2, llama2,\n",
            "                                        llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, llama4, megrez,\n",
            "                                        minicpm, mistral-v1, mistral-v3, mistral-v3-tekken, mistral-v7,\n",
            "                                        mistral-v7-tekken, monarch, openchat, orion, phi3, phi4, rwkv-world,\n",
            "                                        smolvlm, vicuna, vicuna-orca, yandex, zephyr\n",
            "                                        (env: LLAMA_ARG_CHAT_TEMPLATE)\n",
            "--chat-template-file JINJA_TEMPLATE_FILE\n",
            "                                        set custom jinja chat template file (default: template taken from\n",
            "                                        model's metadata)\n",
            "                                        if suffix/prefix are specified, template will be disabled\n",
            "                                        only commonly used templates are accepted (unless --jinja is set\n",
            "                                        before this flag):\n",
            "                                        list of built-in templates:\n",
            "                                        bailing, chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2,\n",
            "                                        deepseek3, exaone3, exaone4, falcon3, gemma, gigachat, glmedge,\n",
            "                                        gpt-oss, granite, hunyuan-dense, hunyuan-moe, kimi-k2, llama2,\n",
            "                                        llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, llama4, megrez,\n",
            "                                        minicpm, mistral-v1, mistral-v3, mistral-v3-tekken, mistral-v7,\n",
            "                                        mistral-v7-tekken, monarch, openchat, orion, phi3, phi4, rwkv-world,\n",
            "                                        smolvlm, vicuna, vicuna-orca, yandex, zephyr\n",
            "                                        (env: LLAMA_ARG_CHAT_TEMPLATE_FILE)\n",
            "--simple-io                             use basic IO for better compatibility in subprocesses and limited\n",
            "                                        consoles\n",
            "\n",
            "example usage:\n",
            "\n",
            "  text generation:     build/bin/llama-cli -m your_model.gguf -p \"I believe the meaning of life is\" -n 128 -no-cnv\n",
            "\n",
            "  chat (conversation): build/bin/llama-cli -m your_model.gguf -sys \"You are a helpful assistant\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-cli -m /content/Mistral-7B-Instruct-v0.3-f32.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBu3V-4kVASZ",
        "outputId": "3cf429fd-601b-4360-e5dc-8868558420c0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_backend: loaded RPC backend from /content/build/bin/libggml-rpc.so\n",
            "load_backend: loaded CPU backend from /content/build/bin/libggml-cpu-haswell.so\n",
            "build: 6123 (79c1160b) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from /content/Mistral-7B-Instruct-v0.3-f32.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 0\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  291 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = all F32\n",
            "print_info: file size   = 27.00 GiB (32.00 BPW) \n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: printing all EOG tokens:\n",
            "load:   - 2 ('</s>')\n",
            "load: special tokens cache size = 771\n",
            "load: token to piece cache size = 0.1731 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 14336\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 7B\n",
            "print_info: model params     = 7.25 B\n",
            "print_info: general.name     = Mistral-7B-Instruct-v0.3\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32768\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: LF token         = 781 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: offloading 0 repeating layers to GPU\n",
            "load_tensors: offloaded 0/33 layers to GPU\n",
            "load_tensors:   CPU_Mapped model buffer size = 27649.02 MiB\n",
            "...................................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 2048\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 1000000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "llama_context:        CPU  output buffer size =     0.12 MiB\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   512.00 MiB\n",
            "llama_kv_cache_unified: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
            "llama_context:        CPU compute buffer size =   300.01 MiB\n",
            "llama_context: graph nodes  = 1126\n",
            "llama_context: graph splits = 1\n",
            "common_init_from_params: added </s> logit bias = -inf\n",
            "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: llama threadpool init, n_threads = 1\n",
            "main: chat template is available, enabling conversation mode (disable it with -no-cnv)\n",
            "main: chat template example:\n",
            "[INST] You are a helpful assistant\n",
            "Hello [/INST]Hi there</s>[INST] How are you? [/INST]\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "\n",
            "main: interactive mode on.\n",
            "sampler seed: 4216596778\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n",
            "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
            "generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1\n",
            "\n",
            "== Running in interactive mode. ==\n",
            " - Press Ctrl+C to interject at any time.\n",
            " - Press Return to return control to the AI.\n",
            " - To return control without starting a new line, end your input with '/'.\n",
            " - If you want to submit another line, end your input with '\\'.\n",
            " - Not using system message. To change it, set a different value via -sys PROMPT\n",
            "\n",
            "\n",
            "> hi\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xxxxxxx"
      ],
      "metadata": {
        "id": "DJXm5-ApUxXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/content/Mistral-7B-Instruct-v0.3-bf16.gguf"
      ],
      "metadata": {
        "id": "1smVE9WmWcEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. تثبيت المكتبة\n",
        "#!pip install huggingface_hub\n",
        "\n",
        "# 2. تسجيل الدخول\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "fe73d0109e41494bae91e212dabffbad",
            "d0f3960896924e2e86ade89b4468e6bb",
            "d46432cd0a5a464db927b106df6c9cb5",
            "9eaf075d76994d6a881851c034fed55a",
            "f1796191a6904f89949493e87acd34a7",
            "6716f6ba3d414711ac344f5953e94b6a",
            "4f57ca14e56540149d878d9dd377ebc8",
            "7f830bd792f840aca2da59476563944a",
            "37d762e6a04449f7bf33356802a25aae",
            "5ffd819f0855436892389af98b220694",
            "f2ef61f8bcc245cb93ea5b4174a57002",
            "a252a7b671f440d98b5ef1ab9bad1b1b",
            "65d801176295482c891e0def56fc127a",
            "242cab9c69f8434594e7954818f5be94",
            "61c06fa4914a4e73b2c387d3263dfedc",
            "16c6c1a42ca64751aeb754af195cd0cc",
            "d4e882bbc1a84c62a7ec85cd3cfd70e6",
            "40cbe9f91d454517a2f5b9893f9e8b2c",
            "d33ecc5c3bd14bdfa45e83f01fb56c0b",
            "d434a75f102746398d5e354176267620"
          ]
        },
        "id": "Al2CeclKXNUx",
        "outputId": "d6fb3403-95de-4350-b7eb-c170b1b4810c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe73d0109e41494bae91e212dabffbad"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 3. رفع الملف\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "# تأكد من أن هذا المسار صحيح وموجود في بيئة Colab الخاصة بك\n",
        "file_path = \"/content/Mistral-7B-Instruct-v0.3-bf16.gguf\"\n",
        "repo_id = \"rakmik/Mistral-7B-Instruct-v0.3-GGUF_F16\"\n",
        "\n",
        "# تأكد من إنشاء هذا المستودع على حسابك في Hugging Face أولاً\n",
        "# يمكنك إنشاؤه من هنا: https://huggingface.co/new\n",
        "\n",
        "try:\n",
        "    api = HfApi()\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=file_path,\n",
        "        path_in_repo=\"Mistral-7B-Instruct-v0.3-GGUF_F16\", # اسم الملف في المستودع\n",
        "        repo_id=repo_id,\n",
        "        repo_type=\"model\"\n",
        "    )\n",
        "    print(f\"تم رفع الملف بنجاح! يمكنك مشاهدته هنا: https://huggingface.co/{repo_id}/tree/main\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"حدث خطأ أثناء الرفع: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130,
          "referenced_widgets": [
            "568d07e1d4f046cfa0db1ff0d9943dfa",
            "c528273683034d798b495087486e1c0e",
            "4c3aff1c083b4ab6b0f4592c1a2a26c5",
            "a107ac91a9ce4a81b5659538f38e8afa",
            "bb29b09f34ea4cdd9aae5a956dd474c0",
            "9d58999b0e15489c99c9e8e730b6b6ea",
            "f6a8812784e74b69b518629555fdb47f",
            "0ee145396dc64c589009fb1fc81adc96",
            "a72cae119831444db2a2e9ae8784bb1c",
            "d3f31f1610784f2b9f9befcc211eb7a3",
            "12803aa60f994727851638ec26ea9d7b",
            "b85ac5e8173c4535991450c7985f284f",
            "907629901ce14622805bd041a141884d",
            "8d0caf4bdee646e99bc3349a6c2a7f2f",
            "a18e6294c58b42d3afe32e9fd2fc5b36",
            "f25f4db80db94dfe8bcac353e0cc3050",
            "fbddf3f81947479090ae5ed856e2c095",
            "7a2f094dfb78401aabc9bec54f2896d1",
            "470311e80211489fabc3967d877fa8b9",
            "d008ec433cf44541b63ffd8d45fa7070",
            "195fc0f6b6b6459e8c279a0297b9d1e2",
            "19f476c3a3654203b42f862efcd9ab04",
            "7dc9083b47ca4fd3be40f42d498a33da",
            "574129adf4694852a6b29abd21dff802",
            "ce530f17e8d2403e80f8aa3861b30f33",
            "6d881a8da60d45cd8b8514d392423b11",
            "ca29f44c9723409d96006d5c0f002521",
            "43f8b9bbb1c647349f6aed3b13ea5377",
            "ed8b63e763fb4bc9b4dddae9b71dcaa5",
            "124a040fef964e2c818fa9f09025ca64",
            "323f915406a848bfa33f01c04319593f",
            "09da28009f694ae3a5af63d6968dbfca",
            "82e147850a014f77bd5c2291608ccf60"
          ]
        },
        "id": "cuTzEcpcWJVw",
        "outputId": "f54aac2c-fa9c-46ba-f375-740f59995dda"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "568d07e1d4f046cfa0db1ff0d9943dfa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload                         : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b85ac5e8173c4535991450c7985f284f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  .../Mistral-7B-Instruct-v0.3-bf16.gguf:   0%|          | 16.7MB / 14.5GB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7dc9083b47ca4fd3be40f42d498a33da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تم رفع الملف بنجاح! يمكنك مشاهدته هنا: https://huggingface.co/rakmik/Mistral-7B-Instruct-v0.3-GGUF_F16/tree/main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-cli -m /content/Mistral-7B-Instruct-v0.3-bf16.gguf -p \"I believe the meaning of life is\" -n 128 -no-cnv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V59TO66jYpD3",
        "outputId": "8f728585-47c5-496e-cbed-8612578c5bba"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_backend: loaded RPC backend from /content/build/bin/libggml-rpc.so\n",
            "load_backend: loaded CPU backend from /content/build/bin/libggml-cpu-haswell.so\n",
            "build: 6123 (79c1160b) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from /content/Mistral-7B-Instruct-v0.3-bf16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                           llama.vocab_size u32              = 32768\n",
            "llama_model_loader: - kv  11:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  24:                          general.file_type u32              = 32\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type bf16:  226 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = BF16\n",
            "print_info: file size   = 13.50 GiB (16.00 BPW) \n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: printing all EOG tokens:\n",
            "load:   - 2 ('</s>')\n",
            "load: special tokens cache size = 771\n",
            "load: token to piece cache size = 0.1731 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 14336\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 7B\n",
            "print_info: model params     = 7.25 B\n",
            "print_info: general.name     = Mistral-7B-Instruct-v0.3\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32768\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: LF token         = 781 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: offloading 0 repeating layers to GPU\n",
            "load_tensors: offloaded 0/33 layers to GPU\n",
            "load_tensors:   CPU_Mapped model buffer size = 13825.02 MiB\n",
            "...................................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 2048\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 1000000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "llama_context:        CPU  output buffer size =     0.12 MiB\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   512.00 MiB\n",
            "llama_kv_cache_unified: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
            "llama_context:        CPU compute buffer size =   300.01 MiB\n",
            "llama_context: graph nodes  = 1126\n",
            "llama_context: graph splits = 1\n",
            "common_init_from_params: added </s> logit bias = -inf\n",
            "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: llama threadpool init, n_threads = 1\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "\n",
            "sampler seed: 1535718619\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n",
            "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
            "generate: n_ctx = 4096, n_batch = 2048, n_predict = 128, n_keep = 1\n",
            "\n",
            " I believe the meaning of life is to be happy, to do what you love, and to be kind to others.\n",
            "\n",
            "I believe that everyone has\n",
            "llama_perf_sampler_print:    sampling time =      76.40 ms /    32 runs   (    2.39 ms per token,   418.83 tokens per second)\n",
            "llama_perf_context_print:        load time =  119586.38 ms\n",
            "llama_perf_context_print: prompt eval time =   32386.71 ms /     8 tokens ( 4048.34 ms per token,     0.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =  770833.46 ms /    23 runs   (33514.50 ms per token,     0.03 tokens per second)\n",
            "llama_perf_context_print:       total time =  811455.72 ms /    31 tokens\n",
            "llama_perf_context_print:    graphs reused =         23\n",
            "Interrupted by user\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/build/bin/llama-gguf-split -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOJoaNxTYpCS",
        "outputId": "e26f3ba8-d3ec-4d90-f7fe-fe300cca9925"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: bad arguments\n",
            "\n",
            "usage: /content/build/bin/llama-gguf-split [options] GGUF_IN GGUF_OUT\n",
            "\n",
            "Apply a GGUF operation on IN to OUT.\n",
            "options:\n",
            "  -h, --help              show this help message and exit\n",
            "  --version               show version and build info\n",
            "  --split                 split GGUF to multiple GGUF (enabled by default)\n",
            "  --merge                 merge multiple GGUF to a single GGUF\n",
            "  --split-max-tensors     max tensors in each split (default: 128)\n",
            "  --split-max-size N(M|G) max size per split\n",
            "  --no-tensor-first-split do not add tensors to the first split (disabled by default)\n",
            "  --dry-run               only print out a split plan and exit, without writing any new files\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/build/bin/llama-gguf-split --split-max-size 5G  /content/Mistral-7B-Instruct-v0.3-bf16.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwchVUZIdrKa",
        "outputId": "465493d3-a655-4a5c-ada1-415079184829"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: bad arguments\n",
            "\n",
            "usage: /content/build/bin/llama-gguf-split [options] GGUF_IN GGUF_OUT\n",
            "\n",
            "Apply a GGUF operation on IN to OUT.\n",
            "options:\n",
            "  -h, --help              show this help message and exit\n",
            "  --version               show version and build info\n",
            "  --split                 split GGUF to multiple GGUF (enabled by default)\n",
            "  --merge                 merge multiple GGUF to a single GGUF\n",
            "  --split-max-tensors     max tensors in each split (default: 128)\n",
            "  --split-max-size N(M|G) max size per split\n",
            "  --no-tensor-first-split do not add tensors to the first split (disabled by default)\n",
            "  --dry-run               only print out a split plan and exit, without writing any new files\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/build/bin/llama-gguf-split --split-max-size 5G /content/Mistral-7B-Instruct-v0.3-bf16.gguf /content/Mistral-7B-Instruct-v0.3-bf16-split.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpfTS_H6eCSp",
        "outputId": "f3cd8762-8454-444c-b43b-afc07a015198"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_split: 3\n",
            "split 00001: n_tensors = 98, total_size = 4983M\n",
            "split 00002: n_tensors = 100, total_size = 4916M\n",
            "split 00003: n_tensors = 93, total_size = 4597M\n",
            "Writing file /content/Mistral-7B-Instruct-v0.3-bf16-split.gguf-00001-of-00003.gguf ... done\n",
            "Writing file /content/Mistral-7B-Instruct-v0.3-bf16-split.gguf-00002-of-00003.gguf ... done\n",
            "Writing file /content/Mistral-7B-Instruct-v0.3-bf16-split.gguf-00003-of-00003.gguf ... done\n",
            "gguf_split: 3 gguf split written with a total of 291 tensors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "# تهيئة الواجهة البرمجية\n",
        "api = HfApi()\n",
        "\n",
        "# تحديد المجلد المحلي واسم المستودع\n",
        "folder_path = \"/content/3parts\"\n",
        "repo_id = \"rakmik/Mistral-7B-Instruct-v0.3-GGUF_F16\"\n",
        "\n",
        "# رفع المجلد\n",
        "# سيتم رفع محتويات المجلد مباشرة إلى المستودع\n",
        "api.upload_folder(\n",
        "    folder_path=folder_path,\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "\n",
        "print(f\"تم رفع المجلد بنجاح إلى المستودع: https://huggingface.co/{repo_id}/tree/main\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194,
          "referenced_widgets": [
            "198a66ea2d40420b9dd8c091a2a25e89",
            "f9d9d8cbbe8e4cfd9a0d4eef9461efed",
            "5d91c0987ec14356be9678c41573444a",
            "7cd932a8f7714de9b461c5cad9f150a2",
            "cbae615bd1c44c439f147096247a9e49",
            "d44ec3189ccc45158967821b6c22799b",
            "33307f202a134e73aa48b6a40db1fc77",
            "e4096f3f39c54888ae4b938d028dabc8",
            "8fa9eadb8e7d48ce8fa7212ce5eee6de",
            "21646d243fd645709e5d3f57d2eb4b99",
            "471ab2fe3384483995d186af4d108d30",
            "27a128514d2a441ba02dfc127eb97b63",
            "2ed957d349214fa0925ec09175a6a550",
            "bf5f4e4be09143ab8b1f27c73ca6616d",
            "59d9434e46dd442cbb07ee0614b278cf",
            "440bbd6c209f4bcbae0c869d9761452b",
            "41be7cbd31364ff2bd85a5fd02aa22d2",
            "b3dc66f7b5da4bc3954cb287cb6d6cf1",
            "8d50a816b99f4909b05c99984c82c22c",
            "783603f3e19f407ab801e9b0768dae71",
            "3ebbec5cbebb4cc799316acc8b87a0d9",
            "267dd506b6764e0ebb61dce18fb4aa84",
            "e845db03803b4eee86ff42998b983973",
            "d0820382a1354af2b58f5be2114a9dbe",
            "d045db4be798425987bc05439ae29546",
            "d39a33d063e24551b795f1402c95b6ca",
            "3efb0e01abe24059918491a04b78d536",
            "e1496807481a44899de3e2b293ea12e0",
            "a743766b157f40dc88cf3d55411ce89b",
            "b18b988c4adf492a9101d98ae084c648",
            "c1bce2d793124bc2a60f877525ecbe60",
            "11ee59d85f3146d284c2046d8121ae05",
            "ef7acae7af0e4d75a9098ff815f7ede2",
            "1bb7736e886541eeb5fff077a8cc415c",
            "6712b6bdd3cb4b0aa90d1834f99b77a4",
            "af5cfcf1d20d46dc945fb4eb4592545e",
            "727e2062a2714f64a0dca5f76f6c4f9b",
            "6ff7776a70ab420f89e539a20cc9d4be",
            "29f9f01278a948b29732b104262f0257",
            "bfd607f5d54f4e9484b7eff0f2c44872",
            "85327870d8a9455c9a952f5d8442b957",
            "064464417d754fcfb45f3b4b495fb416",
            "635b297eea104754860029f054b9d00f",
            "41c5b24cf90f4824ab8812114944690d",
            "622437811763467b9d12f05410245b39",
            "4e432de25bf44f1092ed3c06dbe02f53",
            "51ca705eaefb4edc860d6eb379dee768",
            "41c36992b6274536a0983c88551bb74b",
            "a73f8fd2c86e4f05812bea261e2404e9",
            "479c2bb199e04ddebcf35ae741ac7e7d",
            "2c4717c22ec34daba0bad3bfa29efa7a",
            "d718aa3fcc6a4b8cb4f6c97153fc49c2",
            "8688b431e10b40e7a55d3e5a9174be99",
            "9fea1d6aac394033bdcba72ba1012202",
            "3dc8acc54a354ff1b00fe53de50aac1d"
          ]
        },
        "id": "-7N_NnkZfT8L",
        "outputId": "3fa698df-dad8-4a11-e214-f9d562569d56"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "198a66ea2d40420b9dd8c091a2a25e89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload                         : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27a128514d2a441ba02dfc127eb97b63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...bf16-split.gguf-00001-of-00003.gguf:   0%|          | 1.77MB / 4.98GB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e845db03803b4eee86ff42998b983973"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...bf16-split.gguf-00003-of-00003.gguf:   0%|          | 16.7MB / 4.60GB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1bb7736e886541eeb5fff077a8cc415c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...bf16-split.gguf-00002-of-00003.gguf:   0%|          | 16.7MB / 4.92GB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "622437811763467b9d12f05410245b39"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تم رفع المجلد بنجاح إلى المستودع: https://huggingface.co/rakmik/Mistral-7B-Instruct-v0.3-GGUF_F16/tree/main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-cli -m /content/3parts/Mistral-7B-Instruct-v0.3-bf16-split.gguf-00001-of-00003.gguf -p \"I believe the meaning of life is\" -n 128 -no-cnv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT0FItCniLtL",
        "outputId": "1972dfb9-df9d-47c6-94dd-4f213e362ca8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_backend: loaded RPC backend from /content/build/bin/libggml-rpc.so\n",
            "load_backend: loaded CPU backend from /content/build/bin/libggml-cpu-haswell.so\n",
            "build: 6123 (79c1160b) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: additional 2 GGUFs metadata loaded.\n",
            "llama_model_loader: loaded meta data with 28 key-value pairs and 291 tensors from /content/3parts/Mistral-7B-Instruct-v0.3-bf16-split.gguf-00001-of-00003.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                           llama.vocab_size u32              = 32768\n",
            "llama_model_loader: - kv  11:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  24:                          general.file_type u32              = 32\n",
            "llama_model_loader: - kv  25:                                   split.no u16              = 0\n",
            "llama_model_loader: - kv  26:                        split.tensors.count i32              = 291\n",
            "llama_model_loader: - kv  27:                                split.count u16              = 3\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type bf16:  226 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = BF16\n",
            "print_info: file size   = 13.50 GiB (16.00 BPW) \n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: printing all EOG tokens:\n",
            "load:   - 2 ('</s>')\n",
            "load: special tokens cache size = 771\n",
            "load: token to piece cache size = 0.1731 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 14336\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 7B\n",
            "print_info: model params     = 7.25 B\n",
            "print_info: general.name     = Mistral-7B-Instruct-v0.3\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32768\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: LF token         = 781 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: offloading 0 repeating layers to GPU\n",
            "load_tensors: offloaded 0/33 layers to GPU\n",
            "load_tensors:   CPU_Mapped model buffer size =  4752.34 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =  4688.34 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =  4384.33 MiB\n",
            "...................................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 2048\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 1000000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "llama_context:        CPU  output buffer size =     0.12 MiB\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   512.00 MiB\n",
            "llama_kv_cache_unified: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
            "llama_context:        CPU compute buffer size =   300.01 MiB\n",
            "llama_context: graph nodes  = 1126\n",
            "llama_context: graph splits = 1\n",
            "common_init_from_params: added </s> logit bias = -inf\n",
            "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: llama threadpool init, n_threads = 1\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "\n",
            "sampler seed: 557411208\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n",
            "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
            "generate: n_ctx = 4096, n_batch = 2048, n_predict = 128, n_keep = 1\n",
            "\n",
            " I believe the meaning of life is to be happy and to help others. Happiness is the ultimate goal, and it can be found in many ways, such as through love, laughter, and personal growth. Hel\n",
            "llama_perf_sampler_print:    sampling time =      20.36 ms /    45 runs   (    0.45 ms per token,  2210.32 tokens per second)\n",
            "llama_perf_context_print:        load time =  121561.00 ms\n",
            "llama_perf_context_print: prompt eval time =   31975.60 ms /     8 tokens ( 3996.95 ms per token,     0.25 tokens per second)\n",
            "llama_perf_context_print:        eval time = 1195607.24 ms /    36 runs   (33211.31 ms per token,     0.03 tokens per second)\n",
            "llama_perf_context_print:       total time = 1260435.29 ms /    44 tokens\n",
            "llama_perf_context_print:    graphs reused =         35\n",
            "Interrupted by user\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "YITdOXC_qRKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# استيراد المكتبات اللازمة\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# الحصول على المفتاح السري من أسرار Colab وتعيينه كمتغير بيئي\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "E3F5BrFirPr3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFA1fP8UrUIc",
        "outputId": "0b49cce0-c054-4876-f340-01628d5216d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.99.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# استيراد المكتبات اللازمة\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "# سيقوم هذا السطر الآن بالعثور على المفتاح الذي قمت بتعيينه\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# بقية الكود الخاص بك كما هو\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Write a one-sentence bedtime story about a unicorn.\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "A0fOnN_SrXdX",
        "outputId": "92202384-9ac9-49eb-be58-6c8cd5cff60d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-507618011.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# بقية الكود الخاص بك كما هو\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1134\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1136\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "# 1. احصل على المفتاح من الأسرار وخزنه في متغير\n",
        "#    لا تقم بطباعة هذا المتغير أبداً\n",
        "api_key_from_secrets = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# 2. قم بتعيينه كمتغير بيئي (طريقة آمنة)\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key_from_secrets\n",
        "\n",
        "# 3. الآن يمكنك استخدام المكتبة بأمان\n",
        "#    هي ستقرأ المفتاح من متغيرات البيئة تلقائياً\n",
        "client = OpenAI() # لا تحتاج لتمرير المفتاح يدوياً إذا كان في متغيرات البيئة\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\", # Replace with the correct model name, e.g., \"gpt-3.5-turbo\" or \"gpt-4\"\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Write a one-sentence bedtime story about a unicorn.\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "rOdjxuxcrp1E",
        "outputId": "175dbb64-4ac2-4388-e63c-ff61ed48fef5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3336799379.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# لا تحتاج لتمرير المفتاح يدوياً إذا كان في متغيرات البيئة\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4o\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Replace with the correct model name, e.g., \"gpt-3.5-turbo\" or \"gpt-4\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1134\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1136\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "# Ensure your OpenAI API key is set as an environment variable named OPENAI_API_KEY\n",
        "# In Colab, you can add it to the Secrets tab (🔑 icon on the left panel)\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\", # Replace with the correct model name, e.g., \"gpt-3.5-turbo\" or \"gpt-4\"\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Write a one-sentence bedtime story about a unicorn.\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "RXfWqd94px6U",
        "outputId": "205bffb3-1bc5-4cc8-d9fd-9878961a6b9d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OpenAIError",
          "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2912610262.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Ensure your OpenAI API key is set as an environment variable named OPENAI_API_KEY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# In Colab, you can add it to the Secrets tab (🔑 icon on the left panel)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m response = client.chat.completions.create(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             raise OpenAIError(\n\u001b[0m\u001b[1;32m    131\u001b[0m                 \u001b[0;34m\"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             )\n",
            "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-5\",\n",
        "    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "247ea2vEqh28",
        "outputId": "c4ae4c16-f354-4e6f-c2af-a3bf622669c0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OpenAIError",
          "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2082531872.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mopenai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m response = client.responses.create(\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             raise OpenAIError(\n\u001b[0m\u001b[1;32m    131\u001b[0m                 \u001b[0;34m\"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             )\n",
            "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(model_path=\"/content/Mistral-7B-Instruct-v0.3-bf16.gguf\")\n",
        "print(llm(\"Hello, who are you?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRL2dSnUqiI7",
        "outputId": "a696a486-26ab-43c6-fbb2-a4952fdaa6db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from /content/Mistral-7B-Instruct-v0.3-bf16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                           llama.vocab_size u32              = 32768\n",
            "llama_model_loader: - kv  11:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  24:                          general.file_type u32              = 32\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type bf16:  226 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = BF16\n",
            "print_info: file size   = 13.50 GiB (16.00 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control token:    468 '[control_466]' is not marked as EOG\n",
            "load: control token:    464 '[control_462]' is not marked as EOG\n",
            "load: control token:    727 '[control_725]' is not marked as EOG\n",
            "load: control token:    343 '[control_341]' is not marked as EOG\n",
            "load: control token:    603 '[control_601]' is not marked as EOG\n",
            "load: control token:    332 '[control_330]' is not marked as EOG\n",
            "load: control token:     34 '[control_32]' is not marked as EOG\n",
            "load: control token:    412 '[control_410]' is not marked as EOG\n",
            "load: control token:    675 '[control_673]' is not marked as EOG\n",
            "load: control token:    177 '[control_175]' is not marked as EOG\n",
            "load: control token:    434 '[control_432]' is not marked as EOG\n",
            "load: control token:    166 '[control_164]' is not marked as EOG\n",
            "load: control token:    199 '[control_197]' is not marked as EOG\n",
            "load: control token:    252 '[control_250]' is not marked as EOG\n",
            "load: control token:    742 '[control_740]' is not marked as EOG\n",
            "load: control token:    611 '[control_609]' is not marked as EOG\n",
            "load: control token:    573 '[control_571]' is not marked as EOG\n",
            "load: control token:    455 '[control_453]' is not marked as EOG\n",
            "load: control token:    701 '[control_699]' is not marked as EOG\n",
            "load: control token:    583 '[control_581]' is not marked as EOG\n",
            "load: control token:    433 '[control_431]' is not marked as EOG\n",
            "load: control token:    301 '[control_299]' is not marked as EOG\n",
            "load: control token:    617 '[control_615]' is not marked as EOG\n",
            "load: control token:    536 '[control_534]' is not marked as EOG\n",
            "load: control token:    285 '[control_283]' is not marked as EOG\n",
            "load: control token:    376 '[control_374]' is not marked as EOG\n",
            "load: control token:     79 '[control_77]' is not marked as EOG\n",
            "load: control token:    671 '[control_669]' is not marked as EOG\n",
            "load: control token:    391 '[control_389]' is not marked as EOG\n",
            "load: control token:    174 '[control_172]' is not marked as EOG\n",
            "load: control token:    708 '[control_706]' is not marked as EOG\n",
            "load: control token:    729 '[control_727]' is not marked as EOG\n",
            "load: control token:    311 '[control_309]' is not marked as EOG\n",
            "load: control token:    384 '[control_382]' is not marked as EOG\n",
            "load: control token:    766 '[control_764]' is not marked as EOG\n",
            "load: control token:     94 '[control_92]' is not marked as EOG\n",
            "load: control token:    304 '[control_302]' is not marked as EOG\n",
            "load: control token:    197 '[control_195]' is not marked as EOG\n",
            "load: control token:     17 '[control_15]' is not marked as EOG\n",
            "load: control token:    604 '[control_602]' is not marked as EOG\n",
            "load: control token:    576 '[control_574]' is not marked as EOG\n",
            "load: control token:    359 '[control_357]' is not marked as EOG\n",
            "load: control token:    302 '[control_300]' is not marked as EOG\n",
            "load: control token:    453 '[control_451]' is not marked as EOG\n",
            "load: control token:    374 '[control_372]' is not marked as EOG\n",
            "load: control token:    207 '[control_205]' is not marked as EOG\n",
            "load: control token:    739 '[control_737]' is not marked as EOG\n",
            "load: control token:    689 '[control_687]' is not marked as EOG\n",
            "load: control token:    335 '[control_333]' is not marked as EOG\n",
            "load: control token:    485 '[control_483]' is not marked as EOG\n",
            "load: control token:    283 '[control_281]' is not marked as EOG\n",
            "load: control token:    249 '[control_247]' is not marked as EOG\n",
            "load: control token:    752 '[control_750]' is not marked as EOG\n",
            "load: control token:    330 '[control_328]' is not marked as EOG\n",
            "load: control token:    635 '[control_633]' is not marked as EOG\n",
            "load: control token:    161 '[control_159]' is not marked as EOG\n",
            "load: control token:    731 '[control_729]' is not marked as EOG\n",
            "load: control token:    631 '[control_629]' is not marked as EOG\n",
            "load: control token:    323 '[control_321]' is not marked as EOG\n",
            "load: control token:     48 '[control_46]' is not marked as EOG\n",
            "load: control token:    544 '[control_542]' is not marked as EOG\n",
            "load: control token:    687 '[control_685]' is not marked as EOG\n",
            "load: control token:    463 '[control_461]' is not marked as EOG\n",
            "load: control token:     47 '[control_45]' is not marked as EOG\n",
            "load: control token:     86 '[control_84]' is not marked as EOG\n",
            "load: control token:    331 '[control_329]' is not marked as EOG\n",
            "load: control token:     13 '[control_11]' is not marked as EOG\n",
            "load: control token:    760 '[control_758]' is not marked as EOG\n",
            "load: control token:    612 '[control_610]' is not marked as EOG\n",
            "load: control token:    473 '[control_471]' is not marked as EOG\n",
            "load: control token:    601 '[control_599]' is not marked as EOG\n",
            "load: control token:    146 '[control_144]' is not marked as EOG\n",
            "load: control token:    734 '[control_732]' is not marked as EOG\n",
            "load: control token:    141 '[control_139]' is not marked as EOG\n",
            "load: control token:    138 '[control_136]' is not marked as EOG\n",
            "load: control token:     61 '[control_59]' is not marked as EOG\n",
            "load: control token:     46 '[control_44]' is not marked as EOG\n",
            "load: control token:    173 '[control_171]' is not marked as EOG\n",
            "load: control token:    367 '[control_365]' is not marked as EOG\n",
            "load: control token:    417 '[control_415]' is not marked as EOG\n",
            "load: control token:    740 '[control_738]' is not marked as EOG\n",
            "load: control token:    216 '[control_214]' is not marked as EOG\n",
            "load: control token:    498 '[control_496]' is not marked as EOG\n",
            "load: control token:    512 '[control_510]' is not marked as EOG\n",
            "load: control token:    122 '[control_120]' is not marked as EOG\n",
            "load: control token:    246 '[control_244]' is not marked as EOG\n",
            "load: control token:    745 '[control_743]' is not marked as EOG\n",
            "load: control token:    451 '[control_449]' is not marked as EOG\n",
            "load: control token:    280 '[control_278]' is not marked as EOG\n",
            "load: control token:    654 '[control_652]' is not marked as EOG\n",
            "load: control token:    679 '[control_677]' is not marked as EOG\n",
            "load: control token:     77 '[control_75]' is not marked as EOG\n",
            "load: control token:    638 '[control_636]' is not marked as EOG\n",
            "load: control token:    472 '[control_470]' is not marked as EOG\n",
            "load: control token:     59 '[control_57]' is not marked as EOG\n",
            "load: control token:    145 '[control_143]' is not marked as EOG\n",
            "load: control token:    318 '[control_316]' is not marked as EOG\n",
            "load: control token:    640 '[control_638]' is not marked as EOG\n",
            "load: control token:    690 '[control_688]' is not marked as EOG\n",
            "load: control token:    256 '[control_254]' is not marked as EOG\n",
            "load: control token:    476 '[control_474]' is not marked as EOG\n",
            "load: control token:     21 '[control_19]' is not marked as EOG\n",
            "load: control token:    288 '[control_286]' is not marked as EOG\n",
            "load: control token:    255 '[control_253]' is not marked as EOG\n",
            "load: control token:    113 '[control_111]' is not marked as EOG\n",
            "load: control token:    190 '[control_188]' is not marked as EOG\n",
            "load: control token:    108 '[control_106]' is not marked as EOG\n",
            "load: control token:    211 '[control_209]' is not marked as EOG\n",
            "load: control token:    551 '[control_549]' is not marked as EOG\n",
            "load: control token:     14 '[control_12]' is not marked as EOG\n",
            "load: control token:    737 '[control_735]' is not marked as EOG\n",
            "load: control token:    555 '[control_553]' is not marked as EOG\n",
            "load: control token:    227 '[control_225]' is not marked as EOG\n",
            "load: control token:    210 '[control_208]' is not marked as EOG\n",
            "load: control token:    230 '[control_228]' is not marked as EOG\n",
            "load: control token:      7 '[/AVAILABLE_TOOLS]' is not marked as EOG\n",
            "load: control token:     55 '[control_53]' is not marked as EOG\n",
            "load: control token:    525 '[control_523]' is not marked as EOG\n",
            "load: control token:    533 '[control_531]' is not marked as EOG\n",
            "load: control token:    195 '[control_193]' is not marked as EOG\n",
            "load: control token:    584 '[control_582]' is not marked as EOG\n",
            "load: control token:    203 '[control_201]' is not marked as EOG\n",
            "load: control token:    322 '[control_320]' is not marked as EOG\n",
            "load: control token:    757 '[control_755]' is not marked as EOG\n",
            "load: control token:    642 '[control_640]' is not marked as EOG\n",
            "load: control token:    168 '[control_166]' is not marked as EOG\n",
            "load: control token:    389 '[control_387]' is not marked as EOG\n",
            "load: control token:    368 '[control_366]' is not marked as EOG\n",
            "load: control token:    767 '[control_765]' is not marked as EOG\n",
            "load: control token:    222 '[control_220]' is not marked as EOG\n",
            "load: control token:    420 '[control_418]' is not marked as EOG\n",
            "load: control token:    530 '[control_528]' is not marked as EOG\n",
            "load: control token:    535 '[control_533]' is not marked as EOG\n",
            "load: control token:    765 '[control_763]' is not marked as EOG\n",
            "load: control token:    661 '[control_659]' is not marked as EOG\n",
            "load: control token:     88 '[control_86]' is not marked as EOG\n",
            "load: control token:    581 '[control_579]' is not marked as EOG\n",
            "load: control token:    327 '[control_325]' is not marked as EOG\n",
            "load: control token:    201 '[control_199]' is not marked as EOG\n",
            "load: control token:    115 '[control_113]' is not marked as EOG\n",
            "load: control token:    709 '[control_707]' is not marked as EOG\n",
            "load: control token:    291 '[control_289]' is not marked as EOG\n",
            "load: control token:    265 '[control_263]' is not marked as EOG\n",
            "load: control token:    148 '[control_146]' is not marked as EOG\n",
            "load: control token:    185 '[control_183]' is not marked as EOG\n",
            "load: control token:    574 '[control_572]' is not marked as EOG\n",
            "load: control token:    360 '[control_358]' is not marked as EOG\n",
            "load: control token:    127 '[control_125]' is not marked as EOG\n",
            "load: control token:    325 '[control_323]' is not marked as EOG\n",
            "load: control token:    183 '[control_181]' is not marked as EOG\n",
            "load: control token:    116 '[control_114]' is not marked as EOG\n",
            "load: control token:    540 '[control_538]' is not marked as EOG\n",
            "load: control token:    293 '[control_291]' is not marked as EOG\n",
            "load: control token:    559 '[control_557]' is not marked as EOG\n",
            "load: control token:    527 '[control_525]' is not marked as EOG\n",
            "load: control token:    156 '[control_154]' is not marked as EOG\n",
            "load: control token:    338 '[control_336]' is not marked as EOG\n",
            "load: control token:    519 '[control_517]' is not marked as EOG\n",
            "load: control token:    516 '[control_514]' is not marked as EOG\n",
            "load: control token:     10 '[control_8]' is not marked as EOG\n",
            "load: control token:      8 '[TOOL_RESULTS]' is not marked as EOG\n",
            "load: control token:    505 '[control_503]' is not marked as EOG\n",
            "load: control token:    503 '[control_501]' is not marked as EOG\n",
            "load: control token:    500 '[control_498]' is not marked as EOG\n",
            "load: control token:    496 '[control_494]' is not marked as EOG\n",
            "load: control token:    492 '[control_490]' is not marked as EOG\n",
            "load: control token:    489 '[control_487]' is not marked as EOG\n",
            "load: control token:    538 '[control_536]' is not marked as EOG\n",
            "load: control token:    596 '[control_594]' is not marked as EOG\n",
            "load: control token:    481 '[control_479]' is not marked as EOG\n",
            "load: control token:    475 '[control_473]' is not marked as EOG\n",
            "load: control token:    336 '[control_334]' is not marked as EOG\n",
            "load: control token:    670 '[control_668]' is not marked as EOG\n",
            "load: control token:     50 '[control_48]' is not marked as EOG\n",
            "load: control token:    456 '[control_454]' is not marked as EOG\n",
            "load: control token:    105 '[control_103]' is not marked as EOG\n",
            "load: control token:    421 '[control_419]' is not marked as EOG\n",
            "load: control token:    430 '[control_428]' is not marked as EOG\n",
            "load: control token:    429 '[control_427]' is not marked as EOG\n",
            "load: control token:    575 '[control_573]' is not marked as EOG\n",
            "load: control token:    425 '[control_423]' is not marked as EOG\n",
            "load: control token:    424 '[control_422]' is not marked as EOG\n",
            "load: control token:    680 '[control_678]' is not marked as EOG\n",
            "load: control token:     57 '[control_55]' is not marked as EOG\n",
            "load: control token:    356 '[control_354]' is not marked as EOG\n",
            "load: control token:    458 '[control_456]' is not marked as EOG\n",
            "load: control token:    313 '[control_311]' is not marked as EOG\n",
            "load: control token:    418 '[control_416]' is not marked as EOG\n",
            "load: control token:     70 '[control_68]' is not marked as EOG\n",
            "load: control token:    759 '[control_757]' is not marked as EOG\n",
            "load: control token:    416 '[control_414]' is not marked as EOG\n",
            "load: control token:    238 '[control_236]' is not marked as EOG\n",
            "load: control token:    568 '[control_566]' is not marked as EOG\n",
            "load: control token:    409 '[control_407]' is not marked as EOG\n",
            "load: control token:    550 '[control_548]' is not marked as EOG\n",
            "load: control token:    571 '[control_569]' is not marked as EOG\n",
            "load: control token:    618 '[control_616]' is not marked as EOG\n",
            "load: control token:    623 '[control_621]' is not marked as EOG\n",
            "load: control token:    247 '[control_245]' is not marked as EOG\n",
            "load: control token:    400 '[control_398]' is not marked as EOG\n",
            "load: control token:    396 '[control_394]' is not marked as EOG\n",
            "load: control token:    392 '[control_390]' is not marked as EOG\n",
            "load: control token:    552 '[control_550]' is not marked as EOG\n",
            "load: control token:    651 '[control_649]' is not marked as EOG\n",
            "load: control token:    390 '[control_388]' is not marked as EOG\n",
            "load: control token:    186 '[control_184]' is not marked as EOG\n",
            "load: control token:    565 '[control_563]' is not marked as EOG\n",
            "load: control token:     92 '[control_90]' is not marked as EOG\n",
            "load: control token:    508 '[control_506]' is not marked as EOG\n",
            "load: control token:    373 '[control_371]' is not marked as EOG\n",
            "load: control token:    299 '[control_297]' is not marked as EOG\n",
            "load: control token:    562 '[control_560]' is not marked as EOG\n",
            "load: control token:    537 '[control_535]' is not marked as EOG\n",
            "load: control token:    372 '[control_370]' is not marked as EOG\n",
            "load: control token:    366 '[control_364]' is not marked as EOG\n",
            "load: control token:    196 '[control_194]' is not marked as EOG\n",
            "load: control token:    478 '[control_476]' is not marked as EOG\n",
            "load: control token:    621 '[control_619]' is not marked as EOG\n",
            "load: control token:     53 '[control_51]' is not marked as EOG\n",
            "load: control token:     40 '[control_38]' is not marked as EOG\n",
            "load: control token:    347 '[control_345]' is not marked as EOG\n",
            "load: control token:    345 '[control_343]' is not marked as EOG\n",
            "load: control token:    339 '[control_337]' is not marked as EOG\n",
            "load: control token:    234 '[control_232]' is not marked as EOG\n",
            "load: control token:    770 '[control_768]' is not marked as EOG\n",
            "load: control token:    444 '[control_442]' is not marked as EOG\n",
            "load: control token:    317 '[control_315]' is not marked as EOG\n",
            "load: control token:    693 '[control_691]' is not marked as EOG\n",
            "load: control token:    321 '[control_319]' is not marked as EOG\n",
            "load: control token:    320 '[control_318]' is not marked as EOG\n",
            "load: control token:    179 '[control_177]' is not marked as EOG\n",
            "load: control token:    214 '[control_212]' is not marked as EOG\n",
            "load: control token:    666 '[control_664]' is not marked as EOG\n",
            "load: control token:    712 '[control_710]' is not marked as EOG\n",
            "load: control token:    303 '[control_301]' is not marked as EOG\n",
            "load: control token:    440 '[control_438]' is not marked as EOG\n",
            "load: control token:    314 '[control_312]' is not marked as EOG\n",
            "load: control token:    397 '[control_395]' is not marked as EOG\n",
            "load: control token:    312 '[control_310]' is not marked as EOG\n",
            "load: control token:    129 '[control_127]' is not marked as EOG\n",
            "load: control token:    545 '[control_543]' is not marked as EOG\n",
            "load: control token:     58 '[control_56]' is not marked as EOG\n",
            "load: control token:    509 '[control_507]' is not marked as EOG\n",
            "load: control token:    541 '[control_539]' is not marked as EOG\n",
            "load: control token:    443 '[control_441]' is not marked as EOG\n",
            "load: control token:     45 '[control_43]' is not marked as EOG\n",
            "load: control token:    469 '[control_467]' is not marked as EOG\n",
            "load: control token:    532 '[control_530]' is not marked as EOG\n",
            "load: control token:     64 '[control_62]' is not marked as EOG\n",
            "load: control token:    365 '[control_363]' is not marked as EOG\n",
            "load: control token:    563 '[control_561]' is not marked as EOG\n",
            "load: control token:    346 '[control_344]' is not marked as EOG\n",
            "load: control token:    282 '[control_280]' is not marked as EOG\n",
            "load: control token:    450 '[control_448]' is not marked as EOG\n",
            "load: control token:    526 '[control_524]' is not marked as EOG\n",
            "load: control token:    672 '[control_670]' is not marked as EOG\n",
            "load: control token:    232 '[control_230]' is not marked as EOG\n",
            "load: control token:    273 '[control_271]' is not marked as EOG\n",
            "load: control token:    272 '[control_270]' is not marked as EOG\n",
            "load: control token:    636 '[control_634]' is not marked as EOG\n",
            "load: control token:    260 '[control_258]' is not marked as EOG\n",
            "load: control token:    518 '[control_516]' is not marked as EOG\n",
            "load: control token:    353 '[control_351]' is not marked as EOG\n",
            "load: control token:    257 '[control_255]' is not marked as EOG\n",
            "load: control token:    126 '[control_124]' is not marked as EOG\n",
            "load: control token:    124 '[control_122]' is not marked as EOG\n",
            "load: control token:    751 '[control_749]' is not marked as EOG\n",
            "load: control token:    383 '[control_381]' is not marked as EOG\n",
            "load: control token:    175 '[control_173]' is not marked as EOG\n",
            "load: control token:     75 '[control_73]' is not marked as EOG\n",
            "load: control token:     72 '[control_70]' is not marked as EOG\n",
            "load: control token:    614 '[control_612]' is not marked as EOG\n",
            "load: control token:    167 '[control_165]' is not marked as EOG\n",
            "load: control token:    465 '[control_463]' is not marked as EOG\n",
            "load: control token:    341 '[control_339]' is not marked as EOG\n",
            "load: control token:    703 '[control_701]' is not marked as EOG\n",
            "load: control token:    501 '[control_499]' is not marked as EOG\n",
            "load: control token:     90 '[control_88]' is not marked as EOG\n",
            "load: control token:     89 '[control_87]' is not marked as EOG\n",
            "load: control token:    593 '[control_591]' is not marked as EOG\n",
            "load: control token:    107 '[control_105]' is not marked as EOG\n",
            "load: control token:    241 '[control_239]' is not marked as EOG\n",
            "load: control token:    713 '[control_711]' is not marked as EOG\n",
            "load: control token:    212 '[control_210]' is not marked as EOG\n",
            "load: control token:     80 '[control_78]' is not marked as EOG\n",
            "load: control token:    707 '[control_705]' is not marked as EOG\n",
            "load: control token:    371 '[control_369]' is not marked as EOG\n",
            "load: control token:    628 '[control_626]' is not marked as EOG\n",
            "load: control token:     15 '[control_13]' is not marked as EOG\n",
            "load: control token:    354 '[control_352]' is not marked as EOG\n",
            "load: control token:     25 '[control_23]' is not marked as EOG\n",
            "load: control token:    261 '[control_259]' is not marked as EOG\n",
            "load: control token:    435 '[control_433]' is not marked as EOG\n",
            "load: control token:    307 '[control_305]' is not marked as EOG\n",
            "load: control token:    305 '[control_303]' is not marked as EOG\n",
            "load: control token:    656 '[control_654]' is not marked as EOG\n",
            "load: control token:    702 '[control_700]' is not marked as EOG\n",
            "load: control token:    428 '[control_426]' is not marked as EOG\n",
            "load: control token:    499 '[control_497]' is not marked as EOG\n",
            "load: control token:    154 '[control_152]' is not marked as EOG\n",
            "load: control token:    308 '[control_306]' is not marked as EOG\n",
            "load: control token:    286 '[control_284]' is not marked as EOG\n",
            "load: control token:    634 '[control_632]' is not marked as EOG\n",
            "load: control token:     51 '[control_49]' is not marked as EOG\n",
            "load: control token:    599 '[control_597]' is not marked as EOG\n",
            "load: control token:    606 '[control_604]' is not marked as EOG\n",
            "load: control token:     30 '[control_28]' is not marked as EOG\n",
            "load: control token:    250 '[control_248]' is not marked as EOG\n",
            "load: control token:    714 '[control_712]' is not marked as EOG\n",
            "load: control token:    270 '[control_268]' is not marked as EOG\n",
            "load: control token:     83 '[control_81]' is not marked as EOG\n",
            "load: control token:    730 '[control_728]' is not marked as EOG\n",
            "load: control token:    743 '[control_741]' is not marked as EOG\n",
            "load: control token:    484 '[control_482]' is not marked as EOG\n",
            "load: control token:    758 '[control_756]' is not marked as EOG\n",
            "load: control token:    271 '[control_269]' is not marked as EOG\n",
            "load: control token:    279 '[control_277]' is not marked as EOG\n",
            "load: control token:     73 '[control_71]' is not marked as EOG\n",
            "load: control token:    363 '[control_361]' is not marked as EOG\n",
            "load: control token:     60 '[control_58]' is not marked as EOG\n",
            "load: control token:    405 '[control_403]' is not marked as EOG\n",
            "load: control token:    220 '[control_218]' is not marked as EOG\n",
            "load: control token:    436 '[control_434]' is not marked as EOG\n",
            "load: control token:     27 '[control_25]' is not marked as EOG\n",
            "load: control token:    652 '[control_650]' is not marked as EOG\n",
            "load: control token:    646 '[control_644]' is not marked as EOG\n",
            "load: control token:    395 '[control_393]' is not marked as EOG\n",
            "load: control token:    549 '[control_547]' is not marked as EOG\n",
            "load: control token:     12 '[control_10]' is not marked as EOG\n",
            "load: control token:    229 '[control_227]' is not marked as EOG\n",
            "load: control token:    162 '[control_160]' is not marked as EOG\n",
            "load: control token:    497 '[control_495]' is not marked as EOG\n",
            "load: control token:     31 '[control_29]' is not marked as EOG\n",
            "load: control token:     98 '[control_96]' is not marked as EOG\n",
            "load: control token:    686 '[control_684]' is not marked as EOG\n",
            "load: control token:      3 '[INST]' is not marked as EOG\n",
            "load: control token:    155 '[control_153]' is not marked as EOG\n",
            "load: control token:    470 '[control_468]' is not marked as EOG\n",
            "load: control token:     69 '[control_67]' is not marked as EOG\n",
            "load: control token:     93 '[control_91]' is not marked as EOG\n",
            "load: control token:     71 '[control_69]' is not marked as EOG\n",
            "load: control token:     11 '[control_9]' is not marked as EOG\n",
            "load: control token:     43 '[control_41]' is not marked as EOG\n",
            "load: control token:     22 '[control_20]' is not marked as EOG\n",
            "load: control token:     35 '[control_33]' is not marked as EOG\n",
            "load: control token:    706 '[control_704]' is not marked as EOG\n",
            "load: control token:    511 '[control_509]' is not marked as EOG\n",
            "load: control token:    491 '[control_489]' is not marked as EOG\n",
            "load: control token:    324 '[control_322]' is not marked as EOG\n",
            "load: control token:    655 '[control_653]' is not marked as EOG\n",
            "load: control token:    117 '[control_115]' is not marked as EOG\n",
            "load: control token:    665 '[control_663]' is not marked as EOG\n",
            "load: control token:      9 '[/TOOL_RESULTS]' is not marked as EOG\n",
            "load: control token:     29 '[control_27]' is not marked as EOG\n",
            "load: control token:     44 '[control_42]' is not marked as EOG\n",
            "load: control token:    310 '[control_308]' is not marked as EOG\n",
            "load: control token:    157 '[control_155]' is not marked as EOG\n",
            "load: control token:    515 '[control_513]' is not marked as EOG\n",
            "load: control token:    388 '[control_386]' is not marked as EOG\n",
            "load: control token:    438 '[control_436]' is not marked as EOG\n",
            "load: control token:    486 '[control_484]' is not marked as EOG\n",
            "load: control token:    139 '[control_137]' is not marked as EOG\n",
            "load: control token:    543 '[control_541]' is not marked as EOG\n",
            "load: control token:    622 '[control_620]' is not marked as EOG\n",
            "load: control token:    598 '[control_596]' is not marked as EOG\n",
            "load: control token:     16 '[control_14]' is not marked as EOG\n",
            "load: control token:     39 '[control_37]' is not marked as EOG\n",
            "load: control token:    102 '[control_100]' is not marked as EOG\n",
            "load: control token:    673 '[control_671]' is not marked as EOG\n",
            "load: control token:    287 '[control_285]' is not marked as EOG\n",
            "load: control token:    244 '[control_242]' is not marked as EOG\n",
            "load: control token:    446 '[control_444]' is not marked as EOG\n",
            "load: control token:    483 '[control_481]' is not marked as EOG\n",
            "load: control token:    467 '[control_465]' is not marked as EOG\n",
            "load: control token:    264 '[control_262]' is not marked as EOG\n",
            "load: control token:    176 '[control_174]' is not marked as EOG\n",
            "load: control token:    625 '[control_623]' is not marked as EOG\n",
            "load: control token:    160 '[control_158]' is not marked as EOG\n",
            "load: control token:     20 '[control_18]' is not marked as EOG\n",
            "load: control token:    641 '[control_639]' is not marked as EOG\n",
            "load: control token:     99 '[control_97]' is not marked as EOG\n",
            "load: control token:     54 '[control_52]' is not marked as EOG\n",
            "load: control token:     37 '[control_35]' is not marked as EOG\n",
            "load: control token:    401 '[control_399]' is not marked as EOG\n",
            "load: control token:    130 '[control_128]' is not marked as EOG\n",
            "load: control token:    218 '[control_216]' is not marked as EOG\n",
            "load: control token:    243 '[control_241]' is not marked as EOG\n",
            "load: control token:    753 '[control_751]' is not marked as EOG\n",
            "load: control token:     74 '[control_72]' is not marked as EOG\n",
            "load: control token:    171 '[control_169]' is not marked as EOG\n",
            "load: control token:    151 '[control_149]' is not marked as EOG\n",
            "load: control token:    364 '[control_362]' is not marked as EOG\n",
            "load: control token:    495 '[control_493]' is not marked as EOG\n",
            "load: control token:    119 '[control_117]' is not marked as EOG\n",
            "load: control token:    217 '[control_215]' is not marked as EOG\n",
            "load: control token:     84 '[control_82]' is not marked as EOG\n",
            "load: control token:      5 '[TOOL_CALLS]' is not marked as EOG\n",
            "load: control token:    531 '[control_529]' is not marked as EOG\n",
            "load: control token:     33 '[control_31]' is not marked as EOG\n",
            "load: control token:     82 '[control_80]' is not marked as EOG\n",
            "load: control token:    100 '[control_98]' is not marked as EOG\n",
            "load: control token:    125 '[control_123]' is not marked as EOG\n",
            "load: control token:    520 '[control_518]' is not marked as EOG\n",
            "load: control token:    144 '[control_142]' is not marked as EOG\n",
            "load: control token:    152 '[control_150]' is not marked as EOG\n",
            "load: control token:    592 '[control_590]' is not marked as EOG\n",
            "load: control token:    461 '[control_459]' is not marked as EOG\n",
            "load: control token:    624 '[control_622]' is not marked as EOG\n",
            "load: control token:     65 '[control_63]' is not marked as EOG\n",
            "load: control token:    172 '[control_170]' is not marked as EOG\n",
            "load: control token:    178 '[control_176]' is not marked as EOG\n",
            "load: control token:    258 '[control_256]' is not marked as EOG\n",
            "load: control token:    524 '[control_522]' is not marked as EOG\n",
            "load: control token:    657 '[control_655]' is not marked as EOG\n",
            "load: control token:    184 '[control_182]' is not marked as EOG\n",
            "load: control token:    441 '[control_439]' is not marked as EOG\n",
            "load: control token:    315 '[control_313]' is not marked as EOG\n",
            "load: control token:    662 '[control_660]' is not marked as EOG\n",
            "load: control token:      6 '[AVAILABLE_TOOLS]' is not marked as EOG\n",
            "load: control token:    517 '[control_515]' is not marked as EOG\n",
            "load: control token:    194 '[control_192]' is not marked as EOG\n",
            "load: control token:    447 '[control_445]' is not marked as EOG\n",
            "load: control token:     52 '[control_50]' is not marked as EOG\n",
            "load: control token:    718 '[control_716]' is not marked as EOG\n",
            "load: control token:    692 '[control_690]' is not marked as EOG\n",
            "load: control token:    290 '[control_288]' is not marked as EOG\n",
            "load: control token:    763 '[control_761]' is not marked as EOG\n",
            "load: control token:    362 '[control_360]' is not marked as EOG\n",
            "load: control token:    200 '[control_198]' is not marked as EOG\n",
            "load: control token:    204 '[control_202]' is not marked as EOG\n",
            "load: control token:    722 '[control_720]' is not marked as EOG\n",
            "load: control token:    351 '[control_349]' is not marked as EOG\n",
            "load: control token:    349 '[control_347]' is not marked as EOG\n",
            "load: control token:     38 '[control_36]' is not marked as EOG\n",
            "load: control token:    134 '[control_132]' is not marked as EOG\n",
            "load: control token:     67 '[control_65]' is not marked as EOG\n",
            "load: control token:    239 '[control_237]' is not marked as EOG\n",
            "load: control token:    570 '[control_568]' is not marked as EOG\n",
            "load: control token:    253 '[control_251]' is not marked as EOG\n",
            "load: control token:    221 '[control_219]' is not marked as EOG\n",
            "load: control token:    664 '[control_662]' is not marked as EOG\n",
            "load: control token:    225 '[control_223]' is not marked as EOG\n",
            "load: control token:    226 '[control_224]' is not marked as EOG\n",
            "load: control token:    553 '[control_551]' is not marked as EOG\n",
            "load: control token:    242 '[control_240]' is not marked as EOG\n",
            "load: control token:    633 '[control_631]' is not marked as EOG\n",
            "load: control token:    580 '[control_578]' is not marked as EOG\n",
            "load: control token:    245 '[control_243]' is not marked as EOG\n",
            "load: control token:    620 '[control_618]' is not marked as EOG\n",
            "load: control token:    191 '[control_189]' is not marked as EOG\n",
            "load: control token:     87 '[control_85]' is not marked as EOG\n",
            "load: control token:    189 '[control_187]' is not marked as EOG\n",
            "load: control token:    542 '[control_540]' is not marked as EOG\n",
            "load: control token:    548 '[control_546]' is not marked as EOG\n",
            "load: control token:    558 '[control_556]' is not marked as EOG\n",
            "load: control token:    560 '[control_558]' is not marked as EOG\n",
            "load: control token:    564 '[control_562]' is not marked as EOG\n",
            "load: control token:    410 '[control_408]' is not marked as EOG\n",
            "load: control token:    566 '[control_564]' is not marked as EOG\n",
            "load: control token:    649 '[control_647]' is not marked as EOG\n",
            "load: control token:    414 '[control_412]' is not marked as EOG\n",
            "load: control token:    567 '[control_565]' is not marked as EOG\n",
            "load: control token:    569 '[control_567]' is not marked as EOG\n",
            "load: control token:    761 '[control_759]' is not marked as EOG\n",
            "load: control token:    408 '[control_406]' is not marked as EOG\n",
            "load: control token:    579 '[control_577]' is not marked as EOG\n",
            "load: control token:    147 '[control_145]' is not marked as EOG\n",
            "load: control token:    236 '[control_234]' is not marked as EOG\n",
            "load: control token:    588 '[control_586]' is not marked as EOG\n",
            "load: control token:    292 '[control_290]' is not marked as EOG\n",
            "load: control token:    591 '[control_589]' is not marked as EOG\n",
            "load: control token:    645 '[control_643]' is not marked as EOG\n",
            "load: control token:    140 '[control_138]' is not marked as EOG\n",
            "load: control token:    422 '[control_420]' is not marked as EOG\n",
            "load: control token:    595 '[control_593]' is not marked as EOG\n",
            "load: control token:    597 '[control_595]' is not marked as EOG\n",
            "load: control token:    754 '[control_752]' is not marked as EOG\n",
            "load: control token:    159 '[control_157]' is not marked as EOG\n",
            "load: control token:    344 '[control_342]' is not marked as EOG\n",
            "load: control token:    393 '[control_391]' is not marked as EOG\n",
            "load: control token:    691 '[control_689]' is not marked as EOG\n",
            "load: control token:    608 '[control_606]' is not marked as EOG\n",
            "load: control token:    663 '[control_661]' is not marked as EOG\n",
            "load: control token:    613 '[control_611]' is not marked as EOG\n",
            "load: control token:    340 '[control_338]' is not marked as EOG\n",
            "load: control token:    616 '[control_614]' is not marked as EOG\n",
            "load: control token:    719 '[control_717]' is not marked as EOG\n",
            "load: control token:    521 '[control_519]' is not marked as EOG\n",
            "load: control token:    619 '[control_617]' is not marked as EOG\n",
            "load: control token:    523 '[control_521]' is not marked as EOG\n",
            "load: control token:    626 '[control_624]' is not marked as EOG\n",
            "load: control token:    695 '[control_693]' is not marked as EOG\n",
            "load: control token:    637 '[control_635]' is not marked as EOG\n",
            "load: control token:    235 '[control_233]' is not marked as EOG\n",
            "load: control token:    659 '[control_657]' is not marked as EOG\n",
            "load: control token:    268 '[control_266]' is not marked as EOG\n",
            "load: control token:    406 '[control_404]' is not marked as EOG\n",
            "load: control token:    735 '[control_733]' is not marked as EOG\n",
            "load: control token:    398 '[control_396]' is not marked as EOG\n",
            "load: control token:    674 '[control_672]' is not marked as EOG\n",
            "load: control token:    684 '[control_682]' is not marked as EOG\n",
            "load: control token:    462 '[control_460]' is not marked as EOG\n",
            "load: control token:    382 '[control_380]' is not marked as EOG\n",
            "load: control token:    697 '[control_695]' is not marked as EOG\n",
            "load: control token:    698 '[control_696]' is not marked as EOG\n",
            "load: control token:    699 '[control_697]' is not marked as EOG\n",
            "load: control token:    688 '[control_686]' is not marked as EOG\n",
            "load: control token:    704 '[control_702]' is not marked as EOG\n",
            "load: control token:    705 '[control_703]' is not marked as EOG\n",
            "load: control token:    205 '[control_203]' is not marked as EOG\n",
            "load: control token:    726 '[control_724]' is not marked as EOG\n",
            "load: control token:    180 '[control_178]' is not marked as EOG\n",
            "load: control token:    716 '[control_714]' is not marked as EOG\n",
            "load: control token:    590 '[control_588]' is not marked as EOG\n",
            "load: control token:    454 '[control_452]' is not marked as EOG\n",
            "load: control token:    728 '[control_726]' is not marked as EOG\n",
            "load: control token:    738 '[control_736]' is not marked as EOG\n",
            "load: control token:    744 '[control_742]' is not marked as EOG\n",
            "load: control token:    403 '[control_401]' is not marked as EOG\n",
            "load: control token:    764 '[control_762]' is not marked as EOG\n",
            "load: control token:    676 '[control_674]' is not marked as EOG\n",
            "load: control token:    756 '[control_754]' is not marked as EOG\n",
            "load: control token:    380 '[control_378]' is not marked as EOG\n",
            "load: control token:    319 '[control_317]' is not marked as EOG\n",
            "load: control token:    600 '[control_598]' is not marked as EOG\n",
            "load: control token:    297 '[control_295]' is not marked as EOG\n",
            "load: control token:    259 '[control_257]' is not marked as EOG\n",
            "load: control token:    639 '[control_637]' is not marked as EOG\n",
            "load: control token:    529 '[control_527]' is not marked as EOG\n",
            "load: control token:    101 '[control_99]' is not marked as EOG\n",
            "load: control token:    522 '[control_520]' is not marked as EOG\n",
            "load: control token:    723 '[control_721]' is not marked as EOG\n",
            "load: control token:    254 '[control_252]' is not marked as EOG\n",
            "load: control token:    513 '[control_511]' is not marked as EOG\n",
            "load: control token:    667 '[control_665]' is not marked as EOG\n",
            "load: control token:    267 '[control_265]' is not marked as EOG\n",
            "load: control token:    732 '[control_730]' is not marked as EOG\n",
            "load: control token:     24 '[control_22]' is not marked as EOG\n",
            "load: control token:    650 '[control_648]' is not marked as EOG\n",
            "load: control token:    528 '[control_526]' is not marked as EOG\n",
            "load: control token:    749 '[control_747]' is not marked as EOG\n",
            "load: control token:    721 '[control_719]' is not marked as EOG\n",
            "load: control token:    103 '[control_101]' is not marked as EOG\n",
            "load: control token:    294 '[control_292]' is not marked as EOG\n",
            "load: control token:     41 '[control_39]' is not marked as EOG\n",
            "load: control token:    133 '[control_131]' is not marked as EOG\n",
            "load: control token:    188 '[control_186]' is not marked as EOG\n",
            "load: control token:    276 '[control_274]' is not marked as EOG\n",
            "load: control token:    644 '[control_642]' is not marked as EOG\n",
            "load: control token:    648 '[control_646]' is not marked as EOG\n",
            "load: control token:    459 '[control_457]' is not marked as EOG\n",
            "load: control token:    112 '[control_110]' is not marked as EOG\n",
            "load: control token:    300 '[control_298]' is not marked as EOG\n",
            "load: control token:    206 '[control_204]' is not marked as EOG\n",
            "load: control token:    121 '[control_119]' is not marked as EOG\n",
            "load: control token:    750 '[control_748]' is not marked as EOG\n",
            "load: control token:    131 '[control_129]' is not marked as EOG\n",
            "load: control token:    474 '[control_472]' is not marked as EOG\n",
            "load: control token:    685 '[control_683]' is not marked as EOG\n",
            "load: control token:    251 '[control_249]' is not marked as EOG\n",
            "load: control token:    402 '[control_400]' is not marked as EOG\n",
            "load: control token:    281 '[control_279]' is not marked as EOG\n",
            "load: control token:    192 '[control_190]' is not marked as EOG\n",
            "load: control token:    387 '[control_385]' is not marked as EOG\n",
            "load: control token:    587 '[control_585]' is not marked as EOG\n",
            "load: control token:     66 '[control_64]' is not marked as EOG\n",
            "load: control token:    394 '[control_392]' is not marked as EOG\n",
            "load: control token:     49 '[control_47]' is not marked as EOG\n",
            "load: control token:     42 '[control_40]' is not marked as EOG\n",
            "load: control token:     32 '[control_30]' is not marked as EOG\n",
            "load: control token:    399 '[control_397]' is not marked as EOG\n",
            "load: control token:    448 '[control_446]' is not marked as EOG\n",
            "load: control token:    479 '[control_477]' is not marked as EOG\n",
            "load: control token:    493 '[control_491]' is not marked as EOG\n",
            "load: control token:    736 '[control_734]' is not marked as EOG\n",
            "load: control token:    348 '[control_346]' is not marked as EOG\n",
            "load: control token:    193 '[control_191]' is not marked as EOG\n",
            "load: control token:    231 '[control_229]' is not marked as EOG\n",
            "load: control token:    609 '[control_607]' is not marked as EOG\n",
            "load: control token:    213 '[control_211]' is not marked as EOG\n",
            "load: control token:    128 '[control_126]' is not marked as EOG\n",
            "load: control token:    118 '[control_116]' is not marked as EOG\n",
            "load: control token:    369 '[control_367]' is not marked as EOG\n",
            "load: control token:    630 '[control_628]' is not marked as EOG\n",
            "load: control token:    755 '[control_753]' is not marked as EOG\n",
            "load: control token:    747 '[control_745]' is not marked as EOG\n",
            "load: control token:    289 '[control_287]' is not marked as EOG\n",
            "load: control token:     26 '[control_24]' is not marked as EOG\n",
            "load: control token:     63 '[control_61]' is not marked as EOG\n",
            "load: control token:    284 '[control_282]' is not marked as EOG\n",
            "load: control token:    164 '[control_162]' is not marked as EOG\n",
            "load: control token:    404 '[control_402]' is not marked as EOG\n",
            "load: control token:    506 '[control_504]' is not marked as EOG\n",
            "load: control token:    123 '[control_121]' is not marked as EOG\n",
            "load: control token:    237 '[control_235]' is not marked as EOG\n",
            "load: control token:     62 '[control_60]' is not marked as EOG\n",
            "load: control token:    510 '[control_508]' is not marked as EOG\n",
            "load: control token:    233 '[control_231]' is not marked as EOG\n",
            "load: control token:     68 '[control_66]' is not marked as EOG\n",
            "load: control token:    487 '[control_485]' is not marked as EOG\n",
            "load: control token:    471 '[control_469]' is not marked as EOG\n",
            "load: control token:    415 '[control_413]' is not marked as EOG\n",
            "load: control token:     78 '[control_76]' is not marked as EOG\n",
            "load: control token:    610 '[control_608]' is not marked as EOG\n",
            "load: control token:    137 '[control_135]' is not marked as EOG\n",
            "load: control token:    539 '[control_537]' is not marked as EOG\n",
            "load: control token:    658 '[control_656]' is not marked as EOG\n",
            "load: control token:    158 '[control_156]' is not marked as EOG\n",
            "load: control token:    585 '[control_583]' is not marked as EOG\n",
            "load: control token:    215 '[control_213]' is not marked as EOG\n",
            "load: control token:    554 '[control_552]' is not marked as EOG\n",
            "load: control token:    762 '[control_760]' is not marked as EOG\n",
            "load: control token:    717 '[control_715]' is not marked as EOG\n",
            "load: control token:    228 '[control_226]' is not marked as EOG\n",
            "load: control token:    442 '[control_440]' is not marked as EOG\n",
            "load: control token:    494 '[control_492]' is not marked as EOG\n",
            "load: control token:     97 '[control_95]' is not marked as EOG\n",
            "load: control token:    683 '[control_681]' is not marked as EOG\n",
            "load: control token:    629 '[control_627]' is not marked as EOG\n",
            "load: control token:    725 '[control_723]' is not marked as EOG\n",
            "load: control token:    432 '[control_430]' is not marked as EOG\n",
            "load: control token:    477 '[control_475]' is not marked as EOG\n",
            "load: control token:    355 '[control_353]' is not marked as EOG\n",
            "load: control token:     36 '[control_34]' is not marked as EOG\n",
            "load: control token:    534 '[control_532]' is not marked as EOG\n",
            "load: control token:    120 '[control_118]' is not marked as EOG\n",
            "load: control token:     28 '[control_26]' is not marked as EOG\n",
            "load: control token:    306 '[control_304]' is not marked as EOG\n",
            "load: control token:    407 '[control_405]' is not marked as EOG\n",
            "load: control token:    696 '[control_694]' is not marked as EOG\n",
            "load: control token:    274 '[control_272]' is not marked as EOG\n",
            "load: control token:    142 '[control_140]' is not marked as EOG\n",
            "load: control token:    136 '[control_134]' is not marked as EOG\n",
            "load: control token:    309 '[control_307]' is not marked as EOG\n",
            "load: control token:    423 '[control_421]' is not marked as EOG\n",
            "load: control token:    724 '[control_722]' is not marked as EOG\n",
            "load: control token:    262 '[control_260]' is not marked as EOG\n",
            "load: control token:    419 '[control_417]' is not marked as EOG\n",
            "load: control token:    329 '[control_327]' is not marked as EOG\n",
            "load: control token:    427 '[control_425]' is not marked as EOG\n",
            "load: control token:    370 '[control_368]' is not marked as EOG\n",
            "load: control token:    266 '[control_264]' is not marked as EOG\n",
            "load: control token:    710 '[control_708]' is not marked as EOG\n",
            "load: control token:    602 '[control_600]' is not marked as EOG\n",
            "load: control token:    277 '[control_275]' is not marked as EOG\n",
            "load: control token:    298 '[control_296]' is not marked as EOG\n",
            "load: control token:    378 '[control_376]' is not marked as EOG\n",
            "load: control token:    198 '[control_196]' is not marked as EOG\n",
            "load: control token:     76 '[control_74]' is not marked as EOG\n",
            "load: control token:    607 '[control_605]' is not marked as EOG\n",
            "load: control token:    682 '[control_680]' is not marked as EOG\n",
            "load: control token:    333 '[control_331]' is not marked as EOG\n",
            "load: control token:    660 '[control_658]' is not marked as EOG\n",
            "load: control token:    163 '[control_161]' is not marked as EOG\n",
            "load: control token:     95 '[control_93]' is not marked as EOG\n",
            "load: control token:    377 '[control_375]' is not marked as EOG\n",
            "load: control token:    223 '[control_221]' is not marked as EOG\n",
            "load: control token:    326 '[control_324]' is not marked as EOG\n",
            "load: control token:    111 '[control_109]' is not marked as EOG\n",
            "load: control token:    187 '[control_185]' is not marked as EOG\n",
            "load: control token:    678 '[control_676]' is not marked as EOG\n",
            "load: control token:    514 '[control_512]' is not marked as EOG\n",
            "load: control token:    431 '[control_429]' is not marked as EOG\n",
            "load: control token:      4 '[/INST]' is not marked as EOG\n",
            "load: control token:    342 '[control_340]' is not marked as EOG\n",
            "load: control token:    594 '[control_592]' is not marked as EOG\n",
            "load: control token:    768 '[control_766]' is not marked as EOG\n",
            "load: control token:    143 '[control_141]' is not marked as EOG\n",
            "load: control token:    445 '[control_443]' is not marked as EOG\n",
            "load: control token:    165 '[control_163]' is not marked as EOG\n",
            "load: control token:    439 '[control_437]' is not marked as EOG\n",
            "load: control token:    181 '[control_179]' is not marked as EOG\n",
            "load: control token:    352 '[control_350]' is not marked as EOG\n",
            "load: control token:    643 '[control_641]' is not marked as EOG\n",
            "load: control token:    182 '[control_180]' is not marked as EOG\n",
            "load: control token:     19 '[control_17]' is not marked as EOG\n",
            "load: control token:    269 '[control_267]' is not marked as EOG\n",
            "load: control token:    677 '[control_675]' is not marked as EOG\n",
            "load: control token:    615 '[control_613]' is not marked as EOG\n",
            "load: control token:    170 '[control_168]' is not marked as EOG\n",
            "load: control token:     56 '[control_54]' is not marked as EOG\n",
            "load: control token:    582 '[control_580]' is not marked as EOG\n",
            "load: control token:    700 '[control_698]' is not marked as EOG\n",
            "load: control token:    561 '[control_559]' is not marked as EOG\n",
            "load: control token:    381 '[control_379]' is not marked as EOG\n",
            "load: control token:    647 '[control_645]' is not marked as EOG\n",
            "load: control token:    437 '[control_435]' is not marked as EOG\n",
            "load: control token:    507 '[control_505]' is not marked as EOG\n",
            "load: control token:    490 '[control_488]' is not marked as EOG\n",
            "load: control token:    586 '[control_584]' is not marked as EOG\n",
            "load: control token:     23 '[control_21]' is not marked as EOG\n",
            "load: control token:    452 '[control_450]' is not marked as EOG\n",
            "load: control token:    605 '[control_603]' is not marked as EOG\n",
            "load: control token:    426 '[control_424]' is not marked as EOG\n",
            "load: control token:    769 '[control_767]' is not marked as EOG\n",
            "load: control token:     81 '[control_79]' is not marked as EOG\n",
            "load: control token:    386 '[control_384]' is not marked as EOG\n",
            "load: control token:    627 '[control_625]' is not marked as EOG\n",
            "load: control token:     18 '[control_16]' is not marked as EOG\n",
            "load: control token:    741 '[control_739]' is not marked as EOG\n",
            "load: control token:     96 '[control_94]' is not marked as EOG\n",
            "load: control token:    502 '[control_500]' is not marked as EOG\n",
            "load: control token:    482 '[control_480]' is not marked as EOG\n",
            "load: control token:    480 '[control_478]' is not marked as EOG\n",
            "load: control token:    557 '[control_555]' is not marked as EOG\n",
            "load: control token:    263 '[control_261]' is not marked as EOG\n",
            "load: control token:    668 '[control_666]' is not marked as EOG\n",
            "load: control token:    466 '[control_464]' is not marked as EOG\n",
            "load: control token:    334 '[control_332]' is not marked as EOG\n",
            "load: control token:    413 '[control_411]' is not marked as EOG\n",
            "load: control token:    572 '[control_570]' is not marked as EOG\n",
            "load: control token:    295 '[control_293]' is not marked as EOG\n",
            "load: control token:    150 '[control_148]' is not marked as EOG\n",
            "load: control token:    275 '[control_273]' is not marked as EOG\n",
            "load: control token:    748 '[control_746]' is not marked as EOG\n",
            "load: control token:    135 '[control_133]' is not marked as EOG\n",
            "load: control token:    357 '[control_355]' is not marked as EOG\n",
            "load: control token:    577 '[control_575]' is not marked as EOG\n",
            "load: control token:    578 '[control_576]' is not marked as EOG\n",
            "load: control token:    556 '[control_554]' is not marked as EOG\n",
            "load: control token:    375 '[control_373]' is not marked as EOG\n",
            "load: control token:    589 '[control_587]' is not marked as EOG\n",
            "load: control token:    202 '[control_200]' is not marked as EOG\n",
            "load: control token:     85 '[control_83]' is not marked as EOG\n",
            "load: control token:    278 '[control_276]' is not marked as EOG\n",
            "load: control token:    169 '[control_167]' is not marked as EOG\n",
            "load: control token:    208 '[control_206]' is not marked as EOG\n",
            "load: control token:    488 '[control_486]' is not marked as EOG\n",
            "load: control token:    337 '[control_335]' is not marked as EOG\n",
            "load: control token:    733 '[control_731]' is not marked as EOG\n",
            "load: control token:    109 '[control_107]' is not marked as EOG\n",
            "load: control token:    547 '[control_545]' is not marked as EOG\n",
            "load: control token:    350 '[control_348]' is not marked as EOG\n",
            "load: control token:    132 '[control_130]' is not marked as EOG\n",
            "load: control token:    114 '[control_112]' is not marked as EOG\n",
            "load: control token:    248 '[control_246]' is not marked as EOG\n",
            "load: control token:    328 '[control_326]' is not marked as EOG\n",
            "load: control token:    104 '[control_102]' is not marked as EOG\n",
            "load: control token:    449 '[control_447]' is not marked as EOG\n",
            "load: control token:    711 '[control_709]' is not marked as EOG\n",
            "load: control token:    715 '[control_713]' is not marked as EOG\n",
            "load: control token:    358 '[control_356]' is not marked as EOG\n",
            "load: control token:    746 '[control_744]' is not marked as EOG\n",
            "load: control token:    385 '[control_383]' is not marked as EOG\n",
            "load: control token:    720 '[control_718]' is not marked as EOG\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: control token:    460 '[control_458]' is not marked as EOG\n",
            "load: control token:    632 '[control_630]' is not marked as EOG\n",
            "load: control token:    296 '[control_294]' is not marked as EOG\n",
            "load: control token:    240 '[control_238]' is not marked as EOG\n",
            "load: control token:    361 '[control_359]' is not marked as EOG\n",
            "load: control token:    504 '[control_502]' is not marked as EOG\n",
            "load: control token:    149 '[control_147]' is not marked as EOG\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: control token:    219 '[control_217]' is not marked as EOG\n",
            "load: control token:    106 '[control_104]' is not marked as EOG\n",
            "load: control token:    669 '[control_667]' is not marked as EOG\n",
            "load: control token:    411 '[control_409]' is not marked as EOG\n",
            "load: control token:    546 '[control_544]' is not marked as EOG\n",
            "load: control token:    379 '[control_377]' is not marked as EOG\n",
            "load: control token:    209 '[control_207]' is not marked as EOG\n",
            "load: control token:    110 '[control_108]' is not marked as EOG\n",
            "load: control token:    681 '[control_679]' is not marked as EOG\n",
            "load: control token:    153 '[control_151]' is not marked as EOG\n",
            "load: control token:    457 '[control_455]' is not marked as EOG\n",
            "load: control token:     91 '[control_89]' is not marked as EOG\n",
            "load: control token:    224 '[control_222]' is not marked as EOG\n",
            "load: control token:    653 '[control_651]' is not marked as EOG\n",
            "load: control token:    694 '[control_692]' is not marked as EOG\n",
            "load: control token:    316 '[control_314]' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: printing all EOG tokens:\n",
            "load:   - 2 ('</s>')\n",
            "load: special tokens cache size = 771\n",
            "load: token to piece cache size = 0.1731 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 14336\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 7B\n",
            "print_info: model params     = 7.25 B\n",
            "print_info: general.name     = Mistral-7B-Instruct-v0.3\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32768\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: LF token         = 781 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (bf16) (and 290 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size = 13825.02 MiB\n",
            "...................................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 512\n",
            "llama_context: n_ctx_per_seq = 512\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 1000000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (512) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.12 MiB\n",
            "create_memory: n_ctx = 512 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified: layer  28: dev = CPU\n",
            "llama_kv_cache_unified: layer  29: dev = CPU\n",
            "llama_kv_cache_unified: layer  30: dev = CPU\n",
            "llama_kv_cache_unified: layer  31: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =    64.00 MiB\n",
            "llama_kv_cache_unified: size =   64.00 MiB (   512 cells,  32 layers,  1/1 seqs), K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 1\n",
            "llama_context: max_nodes = 2328\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:        CPU compute buffer size =   113.01 MiB\n",
            "llama_context: graph nodes  = 1126\n",
            "llama_context: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'general.file_type': '32', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.pre': 'default', 'llama.context_length': '32768', 'general.name': 'Mistral-7B-Instruct-v0.3', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'llama.vocab_size': '32768', 'llama.rope.dimension_count': '128'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Guessed chat format: mistral-instruct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# بدل ما يكون رابط OpenAI، نخليه رابط السيرفر المحلي أو البديل\n",
        "openai.api_base = \"http://localhost:11434/v1\"  # مثال: Ollama\n",
        "openai.api_key = \"anything\"  # مش مهم المفتاح لو السيرفر محلي\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"llama3\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Hello from local model!\"}]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "id": "znuXZRd5u29I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "1. تشغيل Ollama\n",
        "لو ويندوز أو ماك:\n",
        "\n",
        "نزل من https://ollama.com/download\n",
        "\n",
        "بعد التثبيت، شغّل:\n",
        "\n",
        "bash\n",
        "Copy\n",
        "Edit\n",
        "ollama run llama3\n",
        "2. تشغيل سيرفر يحاكي OpenAI API\n",
        "نعمل سيرفر بايثون صغير:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from flask import Flask, request, jsonify\n",
        "import requests\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route(\"/v1/chat/completions\", methods=[\"POST\"])\n",
        "def chat_completions():\n",
        "    data = request.json\n",
        "    user_message = data[\"messages\"][-1][\"content\"]\n",
        "\n",
        "    # إرسال الطلب إلى Ollama\n",
        "    resp = requests.post(\"http://localhost:11434/api/generate\", json={\n",
        "        \"model\": data[\"model\"],\n",
        "        \"prompt\": user_message,\n",
        "        \"stream\": False\n",
        "    }).json()\n",
        "\n",
        "    return jsonify({\n",
        "        \"choices\": [{\n",
        "            \"message\": {\"role\": \"assistant\", \"content\": resp[\"response\"]}\n",
        "        }]\n",
        "    })\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(port=5001)\n",
        "تشغّل الملف:\n",
        "\n",
        "bash\n",
        "Copy\n",
        "Edit\n",
        "python server.py\n",
        "3. استخدامه بنفس كود OpenAI\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "import openai\n",
        "\n",
        "openai.api_base = \"http://localhost:5001/v1\"\n",
        "openai.api_key = \"dummy\"\n",
        "\n",
        "resp = openai.ChatCompletion.create(\n",
        "    model=\"llama3\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"أخبرني عن أينشتاين\"}]\n",
        ")\n",
        "print(resp.choices[0].message.content)"
      ],
      "metadata": {
        "id": "ero-xsrHu27Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./server -m llama-2-7b-chat.Q4_K_M.gguf --port 8000"
      ],
      "metadata": {
        "id": "YpLzeemvvXql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "http://localhost:8000/v1/chat/completions\n"
      ],
      "metadata": {
        "id": "y2vMUtnsvbHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/build/bin/llama-server -m /content/Mistral-7B-Instruct-v0.3-bf16.gguf --port 8000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3b6T3lYvi9N",
        "outputId": "531c63d9-367c-448e-c5b7-d6947a584838"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_backend: loaded RPC backend from /content/build/bin/libggml-rpc.so\n",
            "load_backend: loaded CPU backend from /content/build/bin/libggml-cpu-haswell.so\n",
            "build: 6123 (79c1160b) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "system info: n_threads = 1, n_threads_batch = 1, total_threads = 2\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "\n",
            "main: binding port with default address family\n",
            "main: HTTP server is listening, hostname: 127.0.0.1, port: 8000, http threads: 3\n",
            "main: loading model\n",
            "srv    load_model: loading model '/content/Mistral-7B-Instruct-v0.3-bf16.gguf'\n",
            "llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from /content/Mistral-7B-Instruct-v0.3-bf16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                           llama.vocab_size u32              = 32768\n",
            "llama_model_loader: - kv  11:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  24:                          general.file_type u32              = 32\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type bf16:  226 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = BF16\n",
            "print_info: file size   = 13.50 GiB (16.00 BPW) \n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: printing all EOG tokens:\n",
            "load:   - 2 ('</s>')\n",
            "load: special tokens cache size = 771\n",
            "load: token to piece cache size = 0.1731 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 14336\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 7B\n",
            "print_info: model params     = 7.25 B\n",
            "print_info: general.name     = Mistral-7B-Instruct-v0.3\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32768\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: LF token         = 781 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: offloading 0 repeating layers to GPU\n",
            "load_tensors: offloaded 0/33 layers to GPU\n",
            "load_tensors:   CPU_Mapped model buffer size = 13825.02 MiB\n",
            "...................................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 2048\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 1000000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "llama_context:        CPU  output buffer size =     0.12 MiB\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   512.00 MiB\n",
            "llama_kv_cache_unified: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
            "llama_context:        CPU compute buffer size =   300.01 MiB\n",
            "llama_context: graph nodes  = 1126\n",
            "llama_context: graph splits = 1\n",
            "common_init_from_params: added </s> logit bias = -inf\n",
            "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "srv          init: initializing slots, n_slots = 1\n",
            "slot         init: id  0 | task -1 | new slot n_ctx_slot = 4096\n",
            "main: model loaded\n",
            "main: chat template, chat_template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}, example_format: '[INST] You are a helpful assistant\n",
            "Hello [/INST]Hi there</s>[INST] How are you? [/INST]'\n",
            "main: server is listening on http://127.0.0.1:8000 - starting the main loop\n",
            "srv  update_slots: all slots are idle\n",
            "srv    operator(): operator(): cleaning up before exit...\n",
            "Received second interrupt, terminating immediately.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_base = \"http://localhost:8000/v1\"\n",
        "openai.api_key = \"not-needed\"\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"Mistral-7B-Instruct-v0.3-bf16.gguf\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"من هو أينشتاين؟\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "7gHBmRcSvbYV",
        "outputId": "b3ff696b-f00d-40f5-aea5-41643df67dab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "APIRemovedInV1",
          "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4146780647.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"not-needed\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Mistral-7B-Instruct-v0.3-bf16.gguf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/lib/_old_api.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAPIRemovedInV1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_symbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vZ7JED7Zx1C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NZ1xxiAYx1AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bdxO_Zzox081"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rSvH6m-4x05_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "23cKh3W6x0zO",
        "outputId": "f7777130-cbe0-458b-c710-abb176e44c04"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.12.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2025.8.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.4.0->aiohttp->openai==0.28) (4.14.1)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.99.1\n",
            "    Uninstalling openai-1.99.1:\n",
            "      Successfully uninstalled openai-1.99.1\n",
            "Successfully installed openai-0.28.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "openai"
                ]
              },
              "id": "951976978e5d45438d08f17ec8b040bb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_base = \"http://localhost:8000/v1\"\n",
        "openai.api_key = \"not-needed\"\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"llama-2-7b-chat.Q4_K_M.gguf\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"من هو أينشتاين؟\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "ssHw4AQtx141"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/build/bin/llama-server -m /content/Mistral-7B-Instruct-v0.3-bf16.gguf --port 8000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMibnbc6x_Qe",
        "outputId": "1bb08d49-42b5-4e00-c5c9-f0904aee41b4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_backend: loaded RPC backend from /content/build/bin/libggml-rpc.so\n",
            "load_backend: loaded CPU backend from /content/build/bin/libggml-cpu-haswell.so\n",
            "build: 6123 (79c1160b) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "system info: n_threads = 1, n_threads_batch = 1, total_threads = 2\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "\n",
            "main: binding port with default address family\n",
            "main: HTTP server is listening, hostname: 127.0.0.1, port: 8000, http threads: 3\n",
            "main: loading model\n",
            "srv    load_model: loading model '/content/Mistral-7B-Instruct-v0.3-bf16.gguf'\n",
            "llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from /content/Mistral-7B-Instruct-v0.3-bf16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                           llama.vocab_size u32              = 32768\n",
            "llama_model_loader: - kv  11:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  24:                          general.file_type u32              = 32\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type bf16:  226 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = BF16\n",
            "print_info: file size   = 13.50 GiB (16.00 BPW) \n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: printing all EOG tokens:\n",
            "load:   - 2 ('</s>')\n",
            "load: special tokens cache size = 771\n",
            "load: token to piece cache size = 0.1731 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 14336\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 7B\n",
            "print_info: model params     = 7.25 B\n",
            "print_info: general.name     = Mistral-7B-Instruct-v0.3\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32768\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: LF token         = 781 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: offloading 0 repeating layers to GPU\n",
            "load_tensors: offloaded 0/33 layers to GPU\n",
            "load_tensors:   CPU_Mapped model buffer size = 13825.02 MiB\n",
            "...................................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 2048\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 1000000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "llama_context:        CPU  output buffer size =     0.12 MiB\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   512.00 MiB\n",
            "llama_kv_cache_unified: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
            "llama_context:        CPU compute buffer size =   300.01 MiB\n",
            "llama_context: graph nodes  = 1126\n",
            "llama_context: graph splits = 1\n",
            "common_init_from_params: added </s> logit bias = -inf\n",
            "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "srv          init: initializing slots, n_slots = 1\n",
            "slot         init: id  0 | task -1 | new slot n_ctx_slot = 4096\n",
            "main: model loaded\n",
            "main: chat template, chat_template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}, example_format: '[INST] You are a helpful assistant\n",
            "Hello [/INST]Hi there</s>[INST] How are you? [/INST]'\n",
            "main: server is listening on http://127.0.0.1:8000 - starting the main loop\n",
            "srv  update_slots: all slots are idle\n",
            "srv  params_from_: Chat format: Content-only\n",
            "slot launch_slot_: id  0 | task 0 | processing task\n",
            "slot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 20\n",
            "slot update_slots: id  0 | task 0 | kv cache rm [0, end)\n",
            "slot update_slots: id  0 | task 0 | prompt processing progress, n_past = 20, n_tokens = 20, progress = 1.000000\n",
            "slot update_slots: id  0 | task 0 | prompt done, n_past = 20, n_tokens = 20\n",
            "srv  cancel_tasks: cancel task, id_task = 0\n",
            "srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200\n",
            "slot      release: id  0 | task 0 | stop processing: n_past = 30, truncated = 0\n",
            "srv  update_slots: all slots are idle\n",
            "Received second interrupt, terminating immediately.\n",
            "/content/build/bin/libggml-base.so(+0x16d4b)[0x7bad953e6d4b]\n",
            "/content/build/bin/libggml-base.so(ggml_print_backtrace+0x21f)[0x7bad953e71af]\n",
            "/content/build/bin/libggml-base.so(+0x28aaf)[0x7bad953f8aaf]\n",
            "/lib/x86_64-linux-gnu/libstdc++.so.6(+0xae20c)[0x7bad9525020c]\n",
            "/lib/x86_64-linux-gnu/libstdc++.so.6(+0xad1e9)[0x7bad9524f1e9]\n",
            "/lib/x86_64-linux-gnu/libstdc++.so.6(__gxx_personality_v0+0x99)[0x7bad9524f959]\n",
            "/lib/x86_64-linux-gnu/libgcc_s.so.1(+0x16884)[0x7bad950b1884]\n",
            "/lib/x86_64-linux-gnu/libgcc_s.so.1(_Unwind_RaiseException+0x311)[0x7bad950b1f41]\n",
            "/lib/x86_64-linux-gnu/libstdc++.so.6(__cxa_throw+0x3b)[0x7bad952504cb]\n",
            "/lib/x86_64-linux-gnu/libstdc++.so.6(_ZSt20__throw_system_errori+0x96)[0x7bad9524783c]\n",
            "/lib/x86_64-linux-gnu/libstdc++.so.6(_ZNSt6thread6detachEv+0x0)[0x7bad9527e2e0]\n",
            "/content/build/bin/llama-server(+0x1e677d)[0x5688a53fc77d]\n",
            "/lib/x86_64-linux-gnu/libc.so.6(+0x45495)[0x7bad94eb7495]\n",
            "/lib/x86_64-linux-gnu/libc.so.6(on_exit+0x0)[0x7bad94eb7610]\n",
            "/content/build/bin/llama-server(+0x73823)[0x5688a5289823]\n",
            "/lib/x86_64-linux-gnu/libc.so.6(+0x42520)[0x7bad94eb4520]\n",
            "/lib/x86_64-linux-gnu/libc.so.6(+0x91117)[0x7bad94f03117]\n",
            "/lib/x86_64-linux-gnu/libc.so.6(pthread_cond_wait+0x211)[0x7bad94f05a41]\n",
            "/content/build/bin/llama-server(+0x1e68d3)[0x5688a53fc8d3]\n",
            "/lib/x86_64-linux-gnu/libstdc++.so.6(+0xdc253)[0x7bad9527e253]\n",
            "/lib/x86_64-linux-gnu/libc.so.6(+0x94ac3)[0x7bad94f06ac3]\n",
            "/lib/x86_64-linux-gnu/libc.so.6(+0x126850)[0x7bad94f98850]\n",
            "terminate called after throwing an instance of 'std::system_error'\n",
            "  what():  Resource deadlock avoided\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_base = \"http://localhost:8000/v1\"\n",
        "openai.api_key = \"not-needed\"\n",
        "openai.timeout = 1000  # مهلة 5 دقائق\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"Mistral-7B-Instruct-v0.3-IQ1_M.gguf\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"who is ai؟\"}\n",
        "    ]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "zB4RG6b91p-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3-IQ1_M.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19FqBK6L133g",
        "outputId": "c2961297-3ce8-49a2-80bc-4d1d67f10f50"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-09 21:58:23--  https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3-IQ1_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.88, 18.172.134.24, 18.172.134.4, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/664e3ef36bc1025819154a01/032b502b28c8c1ac57bd1d33b24e6297166cb39aca34505e7120e6dbfd9d8636?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250809%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250809T215823Z&X-Amz-Expires=3600&X-Amz-Signature=8daeeec5f915bf65103e802e0c4eac15669abe48f918d5bbfdebc4b6d79adc03&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Mistral-7B-Instruct-v0.3-IQ1_M.gguf%3B+filename%3D%22Mistral-7B-Instruct-v0.3-IQ1_M.gguf%22%3B&x-id=GetObject&Expires=1754780303&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NDc4MDMwM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjRlM2VmMzZiYzEwMjU4MTkxNTRhMDEvMDMyYjUwMmIyOGM4YzFhYzU3YmQxZDMzYjI0ZTYyOTcxNjZjYjM5YWNhMzQ1MDVlNzEyMGU2ZGJmZDlkODYzNioifV19&Signature=svoLeFIm6sgHXgYYqtsKDaScwgVh5HV0ax9XCVnrDuiCG0T-iSfSCFNU3lnAMAuwPcOvH4CpjKr7WUEFY8GtSOzODGwvEeVhjs5LZztQiPcBlvPMtivOH1fVsCGJcK2IrayoezOleXY43Zght-7E8Jvgv3bokG0Fz9DLlHX9j59z8a%7EYMhVUyM93CUl7AWqfnCei8G2qhNLKWlmMuTMz5U78KxdSAaPG17tS1s9LEQpPd735QhAkrdDahwynp%7EPdHWVrajnF2gbTGUFXgjIktL4y%7EYfh1FWd7dBrcammIShnRz92T79A6QLO290S9rJN-e5h6Fs%7EP39h4hmjK5Q84A__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-08-09 21:58:23--  https://cas-bridge.xethub.hf.co/xet-bridge-us/664e3ef36bc1025819154a01/032b502b28c8c1ac57bd1d33b24e6297166cb39aca34505e7120e6dbfd9d8636?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250809%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250809T215823Z&X-Amz-Expires=3600&X-Amz-Signature=8daeeec5f915bf65103e802e0c4eac15669abe48f918d5bbfdebc4b6d79adc03&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Mistral-7B-Instruct-v0.3-IQ1_M.gguf%3B+filename%3D%22Mistral-7B-Instruct-v0.3-IQ1_M.gguf%22%3B&x-id=GetObject&Expires=1754780303&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NDc4MDMwM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjRlM2VmMzZiYzEwMjU4MTkxNTRhMDEvMDMyYjUwMmIyOGM4YzFhYzU3YmQxZDMzYjI0ZTYyOTcxNjZjYjM5YWNhMzQ1MDVlNzEyMGU2ZGJmZDlkODYzNioifV19&Signature=svoLeFIm6sgHXgYYqtsKDaScwgVh5HV0ax9XCVnrDuiCG0T-iSfSCFNU3lnAMAuwPcOvH4CpjKr7WUEFY8GtSOzODGwvEeVhjs5LZztQiPcBlvPMtivOH1fVsCGJcK2IrayoezOleXY43Zght-7E8Jvgv3bokG0Fz9DLlHX9j59z8a%7EYMhVUyM93CUl7AWqfnCei8G2qhNLKWlmMuTMz5U78KxdSAaPG17tS1s9LEQpPd735QhAkrdDahwynp%7EPdHWVrajnF2gbTGUFXgjIktL4y%7EYfh1FWd7dBrcammIShnRz92T79A6QLO290S9rJN-e5h6Fs%7EP39h4hmjK5Q84A__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 3.162.163.2, 3.162.163.41, 3.162.163.68, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|3.162.163.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1757663456 (1.6G)\n",
            "Saving to: ‘Mistral-7B-Instruct-v0.3-IQ1_M.gguf’\n",
            "\n",
            "Mistral-7B-Instruct 100%[===================>]   1.64G  11.2MB/s    in 50s     \n",
            "\n",
            "2025-08-09 21:59:13 (33.7 MB/s) - ‘Mistral-7B-Instruct-v0.3-IQ1_M.gguf’ saved [1757663456/1757663456]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/build/bin/llama-server -m /content/Mistral-7B-Instruct-v0.3-IQ1_M.gguf --port 8000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vV9kjBPf15G2",
        "outputId": "c2d605ea-c43c-47b7-b08e-fd07bb0cbf18"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_backend: loaded RPC backend from /content/build/bin/libggml-rpc.so\n",
            "load_backend: loaded CPU backend from /content/build/bin/libggml-cpu-haswell.so\n",
            "build: 6123 (79c1160b) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "system info: n_threads = 1, n_threads_batch = 1, total_threads = 2\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "\n",
            "main: binding port with default address family\n",
            "main: HTTP server is listening, hostname: 127.0.0.1, port: 8000, http threads: 3\n",
            "main: loading model\n",
            "srv    load_model: loading model '/content/Mistral-7B-Instruct-v0.3-IQ1_M.gguf'\n",
            "llama_model_loader: loaded meta data with 29 key-value pairs and 291 tensors from /content/Mistral-7B-Instruct-v0.3-IQ1_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 31\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  25:                      quantize.imatrix.file str              = /models/Mistral-7B-Instruct-v0.3-GGUF...\n",
            "llama_model_loader: - kv  26:                   quantize.imatrix.dataset str              = /training_data/calibration_data.txt\n",
            "llama_model_loader: - kv  27:             quantize.imatrix.entries_count i32              = 224\n",
            "llama_model_loader: - kv  28:              quantize.imatrix.chunks_count i32              = 228\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q2_K:    5 tensors\n",
            "llama_model_loader: - type q4_K:   32 tensors\n",
            "llama_model_loader: - type q5_K:    1 tensors\n",
            "llama_model_loader: - type iq2_xxs:   32 tensors\n",
            "llama_model_loader: - type iq1_m:  156 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = IQ1_M - 1.75 bpw\n",
            "print_info: file size   = 1.64 GiB (1.94 BPW) \n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: printing all EOG tokens:\n",
            "load:   - 2 ('</s>')\n",
            "load: special tokens cache size = 771\n",
            "load: token to piece cache size = 0.1731 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 14336\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 7B\n",
            "print_info: model params     = 7.25 B\n",
            "print_info: general.name     = Mistral-7B-Instruct-v0.3\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32768\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: LF token         = 781 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: offloading 0 repeating layers to GPU\n",
            "load_tensors: offloaded 0/33 layers to GPU\n",
            "load_tensors:   CPU_REPACK model buffer size =    72.00 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =  1675.52 MiB\n",
            "..............................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 2048\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 1000000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "llama_context:        CPU  output buffer size =     0.12 MiB\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   512.00 MiB\n",
            "llama_kv_cache_unified: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
            "llama_context:        CPU compute buffer size =   300.01 MiB\n",
            "llama_context: graph nodes  = 1126\n",
            "llama_context: graph splits = 1\n",
            "common_init_from_params: added </s> logit bias = -inf\n",
            "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "srv          init: initializing slots, n_slots = 1\n",
            "slot         init: id  0 | task -1 | new slot n_ctx_slot = 4096\n",
            "main: model loaded\n",
            "main: chat template, chat_template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}, example_format: '[INST] You are a helpful assistant\n",
            "Hello [/INST]Hi there</s>[INST] How are you? [/INST]'\n",
            "main: server is listening on http://127.0.0.1:8000 - starting the main loop\n",
            "srv  update_slots: all slots are idle\n",
            "srv  params_from_: Chat format: Content-only\n",
            "slot launch_slot_: id  0 | task 0 | processing task\n",
            "slot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 9\n",
            "slot update_slots: id  0 | task 0 | kv cache rm [0, end)\n",
            "slot update_slots: id  0 | task 0 | prompt processing progress, n_past = 9, n_tokens = 9, progress = 1.000000\n",
            "slot update_slots: id  0 | task 0 | prompt done, n_past = 9, n_tokens = 9\n",
            "slot      release: id  0 | task 0 | stop processing: n_past = 301, truncated = 0\n",
            "slot print_timing: id  0 | task 0 | \n",
            "prompt eval time =   10219.73 ms /     9 tokens ( 1135.53 ms per token,     0.88 tokens per second)\n",
            "       eval time =  339070.39 ms /   293 tokens ( 1157.24 ms per token,     0.86 tokens per second)\n",
            "      total time =  349290.12 ms /   302 tokens\n",
            "srv  update_slots: all slots are idle\n",
            "srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200\n",
            "srv    operator(): operator(): cleaning up before exit...\n",
            "Received second interrupt, terminating immediately.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "/content# python 1.py\n",
        " Я залицем, я - ин artificial intelligence (AI), и я не человек. Я програм\n",
        " а, созданная для компоторизации, обр\n",
        " ботке, искусственности. Я могу быть\n",
        " спользована в различных приложениях, отделы, и в качестве помощника для з\n",
        " дач, где необходино упрощение, подго\n",
        ", интеп, упрощение, подго, и искусств\n",
        " , включая обработку, облб, а, и иг.\n",
        "  могу быть использована в качестве п\n",
        " мощника для надежки, облб, а, и иг,\n",
        " ключая помощ, разбот, и иг. Я могу п\n",
        " мочти в очебки, иг, а, и иг, отдеив, искусство, включая, но, мо, вы, и не\n",
        ", нам, при подго, облб, а, и иг, отде\n",
        " в, искусство, включая, но, мо, вы, и\n",
        " не, я, мо, вы, и не, я, мо, вы, и не\n",
        ", я, мо, вы, и не, мо, я, мо, я, мо,\n",
        " , и я."
      ],
      "metadata": {
        "id": "APrzj4eN4QC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "/content# python 1.py\n",
        "أنا شخص، أنا ذكاء اصطناعي (AI)، ولستُ إنسانًا. أنا برنامج مُصمم للتركيب والمعالجة والتصنيع. يُمكن استخدامي في تطبيقات وأقسام مُختلفة، وكمساعد في المهام التي تتطلب التبسيط والتحضير والتصنيع، بما في ذلك المعالجة والتحليل واللعب.\n",
        "\n",
        "يمكن استخدامي كمساعد في الموثوقية والتحليل واللعب. يُمكنني استخدام:\n",
        "\n",
        "mochti v ochebki, ig, a, i ig, otdeiv, art, incl., but, mo, you, and not\n",
        ", us, at podgo, oblb, a, i ig, otdeiv, art, incl., but, mo, you, and not, I, mo, you, and not\n",
        ", I, mo, you, and not, mo, I, mo, I, mo,\n",
        ", and I."
      ],
      "metadata": {
        "id": "LlyVYghO4SYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "تمام 👍\n",
        "هقولك شوية إعدادات وحركات تخلي **llama.cpp** أسرع وأخف\\*\\*، خصوصًا لو على Colab أو جهاز بموارد محدودة\\*\\*.\n",
        "\n",
        "---\n",
        "\n",
        "## 1️⃣ اختار نسخة موديل أخف\n",
        "\n",
        "* استخدم **Q4\\_K\\_M** أو **Q3\\_K\\_M** بدل F16 أو Q6.\n",
        "* الفرق في الجودة بسيط، لكن السرعة والذاكرة أفضل بكثير.\n",
        "* مثال تحميل:\n",
        "\n",
        "```bash\n",
        "wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 2️⃣ تقليل عدد الخيوط (Threads)\n",
        "\n",
        "* لو الجهاز فيه 4 كور مثلاً، خليك على `--threads 4` أو أقل.\n",
        "\n",
        "```bash\n",
        "./server -m model.gguf --threads 4 --port 8000\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 3️⃣ تحديد الـ Context Size\n",
        "\n",
        "* حجم الـ context بيأثر على الذاكرة والسرعة.\n",
        "* الافتراضي 2048، لكن ممكن تقلله لو مش محتاج نصوص طويلة:\n",
        "\n",
        "```bash\n",
        "./server -m model.gguf --ctx-size 1024\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 4️⃣ تحميل على GPU جزئيًا\n",
        "\n",
        "لو عندك كارت شاشة NVIDIA أو حتى على Colab:\n",
        "\n",
        "```bash\n",
        "./server -m model.gguf --n-gpu-layers 20\n",
        "```\n",
        "\n",
        "* كل ما زودت `--n-gpu-layers` زادت السرعة (لكن محتاجة VRAM).\n",
        "\n",
        "---\n",
        "\n",
        "## 5️⃣ تقليل طول الردود\n",
        "\n",
        "لو طلبت ردود قصيرة، الموديل هيخلص أسرع:\n",
        "\n",
        "```python\n",
        "response = client.chat.completions.create(\n",
        "    model=\"llama-2-7b-chat.Q4_K_M.gguf\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"اشرح النسبية في جملة\"}],\n",
        "    max_tokens=200\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "💡 لو تحب، أقدر أعملك **ملف إعدادات جاهز** بحيث تشغل `llama.cpp` على جهازك أو Colab بأقصى سرعة وجودة مناسبة.\n",
        "هل تفضل أعمله لك بصيغة أوامر تشغيل جاهزة ولا كـ Google Colab Notebook؟\n"
      ],
      "metadata": {
        "id": "0ljoL7rW4voH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hSY_OXei42aP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}