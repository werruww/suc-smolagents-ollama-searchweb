# -*- coding: utf-8 -*-
"""Qwen3-235B.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//huggingface.co/rakmik/Qwen3-235B.ipynb
"""

!pip install -U llama-cpp-python

"""## Local Inference on GPU
Model page: https://huggingface.co/rakmik/Qwen3-235B

‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/rakmik/Qwen3-235B)
			and/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè
"""

# !pip install llama-cpp-python

from llama_cpp import Llama

llm = Llama.from_pretrained(
        repo_id="rakmik/Qwen3-235B",
        filename="qwen_split.gguf-00001-of-00009.gguf",
)

llm.create_chat_completion(
	messages = "No input example has been defined for this model task."
)

# !pip install llama-cpp-python

from llama_cpp import Llama

llm = Llama.from_pretrained(
	repo_id="rakmik/Qwen3-235B",
	filename="qwen_split.gguf-00001-of-00009.gguf",
)

llm.create_chat_completion(
	messages = "No input example has been defined for this model task."
)







!huggingface-cli download rakmik/Qwen3-235B --resume-download --local-dir qwen3 --local-dir-use-symlinks False

# !pip install llama-cpp-python

from llama_cpp import Llama

llm = Llama.from_pretrained(
        repo_id="rakmik/Qwen3-235B",
        filename="qwen_split.gguf-00001-of-00009.gguf",
)

llm.create_chat_completion(
	messages = "No input example has been defined for this model task."
)

from llama_cpp import Llama

model_path = "/content/qwen3/qwen_split.gguf-00001-of-00009.gguf"

# Initialize the Llama model with the local path
llm = Llama(model_path=model_path)

# Example inference
prompt = "Write a short story about a cat."
output = llm(prompt, max_tokens=100)
print(output["choices"][0]["text"])