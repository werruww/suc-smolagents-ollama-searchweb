# -*- coding: utf-8 -*-
"""suc_mistral7bF32toBF16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14kx-45MConyoiIuee5vzZvLsqncXIaCH
"""



!pip install llama-cpp-python

# !pip install llama-cpp-python

from llama_cpp import Llama

llm = Llama.from_pretrained(
	repo_id="rakmik/Mistral-7B-Instruct-v0.3-Q8_0-GGUF",
	filename="mistral-7b-instruct-v0.3-q8_0-00001-of-00002.gguf",
)

llm.create_chat_completion(
	messages = "No input example has been defined for this model task."
)

!wget https://huggingface.co/rakmik/Mistral-7B-Instruct-v0.3-Q8_0-GGUF/resolve/main/mistral-7b-instruct-v0.3-q8_0-00001-of-00002.gguf

!wget https://huggingface.co/rakmik/Mistral-7B-Instruct-v0.3-Q8_0-GGUF/resolve/main/mistral-7b-instruct-v0.3-q8_0-00002-of-00002.gguf

from llama_cpp import Llama

llm = Llama(model_path="/content/mistral-7b-instruct-v0.3-q8_0-00001-of-00002.gguf")
response = llm.create_chat_completion(
  messages=[
    {
        "role": "user",
        "content": "how big is the sky"
    }
])
print(response)



from llama_cpp import Llama

llm = Llama(
      model_path="./models/7B/llama-model.gguf",
      # n_gpu_layers=-1, # Uncomment to use GPU acceleration
      # seed=1337, # Uncomment to set a specific seed
      # n_ctx=2048, # Uncomment to increase the context window
)
output = llm(
      "Q: Name the planets in the solar system? A: ", # Prompt
      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window
      stop=["Q:", "\n"], # Stop generating just before the model would generate a new question
      echo=True # Echo the prompt back in the output
) # Generate a completion, can also call create_completion
print(output)

!wget https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3-f32.gguf

!wget https://github.com/ggml-org/llama.cpp/releases/download/b6123/llama-b6123-bin-ubuntu-x64.zip

!unzip llama-b6123-bin-ubuntu-x64.zip

!./llama-quantize ./models/mymodel/ggml-model-f16.gguf ./models/mymodel/ggml-model-Q4_K_M.gguf Q4_K_M

./llama-quantize ./models/mymodel/ggml-model-f16.gguf ./models/mymodel/ggml-model-Q4_K_M.gguf Q4_K_M

!build/bin/llama-quantize -h

!build/bin/llama-quantize /content/Mistral-7B-Instruct-v0.3-f32.gguf /content/Mistral-7B-Instruct-v0.3-bf16.gguf BF16

!/content/build/bin/llama-gguf-split -h

!build/bin/llama-cli -h

!build/bin/llama-cli -m /content/Mistral-7B-Instruct-v0.3-f32.gguf

xxxxxxx

/content/Mistral-7B-Instruct-v0.3-bf16.gguf

# 1. ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø©
#!pip install huggingface_hub

# 2. ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„
from huggingface_hub import notebook_login
notebook_login()

# 3. Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù
from huggingface_hub import HfApi

# ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ù‡Ø°Ø§ Ø§Ù„Ù…Ø³Ø§Ø± ØµØ­ÙŠØ­ ÙˆÙ…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø¨ÙŠØ¦Ø© Colab Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ
file_path = "/content/Mistral-7B-Instruct-v0.3-bf16.gguf"
repo_id = "rakmik/Mistral-7B-Instruct-v0.3-GGUF_F16"

# ØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ù†Ø´Ø§Ø¡ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ø¹Ù„Ù‰ Ø­Ø³Ø§Ø¨Ùƒ ÙÙŠ Hugging Face Ø£ÙˆÙ„Ø§Ù‹
# ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ù†Ø´Ø§Ø¤Ù‡ Ù…Ù† Ù‡Ù†Ø§: https://huggingface.co/new

try:
    api = HfApi()
    api.upload_file(
        path_or_fileobj=file_path,
        path_in_repo="Mistral-7B-Instruct-v0.3-GGUF_F16", # Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù ÙÙŠ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹
        repo_id=repo_id,
        repo_type="model"
    )
    print(f"ØªÙ… Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø¬Ø§Ø­! ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø´Ø§Ù‡Ø¯ØªÙ‡ Ù‡Ù†Ø§: https://huggingface.co/{repo_id}/tree/main")

except Exception as e:
    print(f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø±ÙØ¹: {e}")

!build/bin/llama-cli -m /content/Mistral-7B-Instruct-v0.3-bf16.gguf -p "I believe the meaning of life is" -n 128 -no-cnv

!/content/build/bin/llama-gguf-split -h

!/content/build/bin/llama-gguf-split --split-max-size 5G  /content/Mistral-7B-Instruct-v0.3-bf16.gguf

!/content/build/bin/llama-gguf-split --split-max-size 5G /content/Mistral-7B-Instruct-v0.3-bf16.gguf /content/Mistral-7B-Instruct-v0.3-bf16-split.gguf

from huggingface_hub import HfApi

# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©
api = HfApi()

# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø­Ù„ÙŠ ÙˆØ§Ø³Ù… Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹
folder_path = "/content/3parts"
repo_id = "rakmik/Mistral-7B-Instruct-v0.3-GGUF_F16"

# Ø±ÙØ¹ Ø§Ù„Ù…Ø¬Ù„Ø¯
# Ø³ÙŠØªÙ… Ø±ÙØ¹ Ù…Ø­ØªÙˆÙŠØ§Øª Ø§Ù„Ù…Ø¬Ù„Ø¯ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹
api.upload_folder(
    folder_path=folder_path,
    repo_id=repo_id,
    repo_type="model"
)

print(f"ØªÙ… Ø±ÙØ¹ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø¨Ù†Ø¬Ø§Ø­ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹: https://huggingface.co/{repo_id}/tree/main")

!build/bin/llama-cli -m /content/3parts/Mistral-7B-Instruct-v0.3-bf16-split.gguf-00001-of-00003.gguf -p "I believe the meaning of life is" -n 128 -no-cnv

from google.colab import userdata
userdata.get('OPENAI_API_KEY')

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø©
from google.colab import userdata
import os

# Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙØªØ§Ø­ Ø§Ù„Ø³Ø±ÙŠ Ù…Ù† Ø£Ø³Ø±Ø§Ø± Colab ÙˆØªØ¹ÙŠÙŠÙ†Ù‡ ÙƒÙ…ØªØºÙŠØ± Ø¨ÙŠØ¦ÙŠ
os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

!pip install openai

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø©
from openai import OpenAI
import os

# Ø³ÙŠÙ‚ÙˆÙ… Ù‡Ø°Ø§ Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ø¢Ù† Ø¨Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙØªØ§Ø­ Ø§Ù„Ø°ÙŠ Ù‚Ù…Øª Ø¨ØªØ¹ÙŠÙŠÙ†Ù‡
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

# Ø¨Ù‚ÙŠØ© Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ ÙƒÙ…Ø§ Ù‡Ùˆ
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "user", "content": "Write a one-sentence bedtime story about a unicorn."}
    ]
)

print(response.choices[0].message.content)

from google.colab import userdata
from openai import OpenAI
import os

# 1. Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙØªØ§Ø­ Ù…Ù† Ø§Ù„Ø£Ø³Ø±Ø§Ø± ÙˆØ®Ø²Ù†Ù‡ ÙÙŠ Ù…ØªØºÙŠØ±
#    Ù„Ø§ ØªÙ‚Ù… Ø¨Ø·Ø¨Ø§Ø¹Ø© Ù‡Ø°Ø§ Ø§Ù„Ù…ØªØºÙŠØ± Ø£Ø¨Ø¯Ø§Ù‹
api_key_from_secrets = userdata.get('OPENAI_API_KEY')

# 2. Ù‚Ù… Ø¨ØªØ¹ÙŠÙŠÙ†Ù‡ ÙƒÙ…ØªØºÙŠØ± Ø¨ÙŠØ¦ÙŠ (Ø·Ø±ÙŠÙ‚Ø© Ø¢Ù…Ù†Ø©)
os.environ["OPENAI_API_KEY"] = api_key_from_secrets

# 3. Ø§Ù„Ø¢Ù† ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø¨Ø£Ù…Ø§Ù†
#    Ù‡ÙŠ Ø³ØªÙ‚Ø±Ø£ Ø§Ù„Ù…ÙØªØ§Ø­ Ù…Ù† Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
client = OpenAI() # Ù„Ø§ ØªØ­ØªØ§Ø¬ Ù„ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù…ÙØªØ§Ø­ ÙŠØ¯ÙˆÙŠØ§Ù‹ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙÙŠ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©

response = client.chat.completions.create(
    model="gpt-4o", # Replace with the correct model name, e.g., "gpt-3.5-turbo" or "gpt-4"
    messages=[
        {"role": "user", "content": "Write a one-sentence bedtime story about a unicorn."}
    ]
)

print(response.choices[0].message.content)

from openai import OpenAI
import os

# Ensure your OpenAI API key is set as an environment variable named OPENAI_API_KEY
# In Colab, you can add it to the Secrets tab (ğŸ”‘ icon on the left panel)
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

response = client.chat.completions.create(
    model="gpt-3.5-turbo", # Replace with the correct model name, e.g., "gpt-3.5-turbo" or "gpt-4"
    messages=[
        {"role": "user", "content": "Write a one-sentence bedtime story about a unicorn."}
    ]
)

print(response.choices[0].message.content)

from openai import OpenAI
client = OpenAI()

response = client.responses.create(
    model="gpt-5",
    input="Write a one-sentence bedtime story about a unicorn."
)

print(response.output_text)

from llama_cpp import Llama

llm = Llama(model_path="/content/Mistral-7B-Instruct-v0.3-bf16.gguf")
print(llm("Hello, who are you?"))

import openai

# Ø¨Ø¯Ù„ Ù…Ø§ ÙŠÙƒÙˆÙ† Ø±Ø§Ø¨Ø· OpenAIØŒ Ù†Ø®Ù„ÙŠÙ‡ Ø±Ø§Ø¨Ø· Ø§Ù„Ø³ÙŠØ±ÙØ± Ø§Ù„Ù…Ø­Ù„ÙŠ Ø£Ùˆ Ø§Ù„Ø¨Ø¯ÙŠÙ„
openai.api_base = "http://localhost:11434/v1"  # Ù…Ø«Ø§Ù„: Ollama
openai.api_key = "anything"  # Ù…Ø´ Ù…Ù‡Ù… Ø§Ù„Ù…ÙØªØ§Ø­ Ù„Ùˆ Ø§Ù„Ø³ÙŠØ±ÙØ± Ù…Ø­Ù„ÙŠ

response = openai.ChatCompletion.create(
    model="llama3",
    messages=[{"role": "user", "content": "Hello from local model!"}]
)

print(response.choices[0].message.content)

1. ØªØ´ØºÙŠÙ„ Ollama
Ù„Ùˆ ÙˆÙŠÙ†Ø¯ÙˆØ² Ø£Ùˆ Ù…Ø§Ùƒ:

Ù†Ø²Ù„ Ù…Ù† https://ollama.com/download

Ø¨Ø¹Ø¯ Ø§Ù„ØªØ«Ø¨ÙŠØªØŒ Ø´ØºÙ‘Ù„:

bash
Copy
Edit
ollama run llama3
2. ØªØ´ØºÙŠÙ„ Ø³ÙŠØ±ÙØ± ÙŠØ­Ø§ÙƒÙŠ OpenAI API
Ù†Ø¹Ù…Ù„ Ø³ÙŠØ±ÙØ± Ø¨Ø§ÙŠØ«ÙˆÙ† ØµØºÙŠØ±:

python
Copy
Edit
from flask import Flask, request, jsonify
import requests

app = Flask(__name__)

@app.route("/v1/chat/completions", methods=["POST"])
def chat_completions():
    data = request.json
    user_message = data["messages"][-1]["content"]

    # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø·Ù„Ø¨ Ø¥Ù„Ù‰ Ollama
    resp = requests.post("http://localhost:11434/api/generate", json={
        "model": data["model"],
        "prompt": user_message,
        "stream": False
    }).json()

    return jsonify({
        "choices": [{
            "message": {"role": "assistant", "content": resp["response"]}
        }]
    })

if __name__ == "__main__":
    app.run(port=5001)
ØªØ´ØºÙ‘Ù„ Ø§Ù„Ù…Ù„Ù:

bash
Copy
Edit
python server.py
3. Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ø¨Ù†ÙØ³ ÙƒÙˆØ¯ OpenAI
python
Copy
Edit
import openai

openai.api_base = "http://localhost:5001/v1"
openai.api_key = "dummy"

resp = openai.ChatCompletion.create(
    model="llama3",
    messages=[{"role": "user", "content": "Ø£Ø®Ø¨Ø±Ù†ÙŠ Ø¹Ù† Ø£ÙŠÙ†Ø´ØªØ§ÙŠÙ†"}]
)
print(resp.choices[0].message.content)

!./server -m llama-2-7b-chat.Q4_K_M.gguf --port 8000

http://localhost:8000/v1/chat/completions

!/content/build/bin/llama-server -m /content/Mistral-7B-Instruct-v0.3-bf16.gguf --port 8000

import openai

openai.api_base = "http://localhost:8000/v1"
openai.api_key = "not-needed"

response = openai.ChatCompletion.create(
    model="Mistral-7B-Instruct-v0.3-bf16.gguf",
    messages=[
        {"role": "user", "content": "Ù…Ù† Ù‡Ùˆ Ø£ÙŠÙ†Ø´ØªØ§ÙŠÙ†ØŸ"}
    ]
)

print(response.choices[0].message.content)









!pip install openai==0.28

import openai

openai.api_base = "http://localhost:8000/v1"
openai.api_key = "not-needed"

response = openai.ChatCompletion.create(
    model="llama-2-7b-chat.Q4_K_M.gguf",
    messages=[
        {"role": "user", "content": "Ù…Ù† Ù‡Ùˆ Ø£ÙŠÙ†Ø´ØªØ§ÙŠÙ†ØŸ"}
    ]
)

print(response.choices[0].message.content)

!/content/build/bin/llama-server -m /content/Mistral-7B-Instruct-v0.3-bf16.gguf --port 8000

import openai

openai.api_base = "http://localhost:8000/v1"
openai.api_key = "not-needed"
openai.timeout = 1000  # Ù…Ù‡Ù„Ø© 5 Ø¯Ù‚Ø§Ø¦Ù‚

response = openai.ChatCompletion.create(
    model="Mistral-7B-Instruct-v0.3-IQ1_M.gguf",
    messages=[
        {"role": "user", "content": "who is aiØŸ"}
    ]
)
print(response.choices[0].message.content)

!wget https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3-IQ1_M.gguf

!/content/build/bin/llama-server -m /content/Mistral-7B-Instruct-v0.3-IQ1_M.gguf --port 8000

"""/content# python 1.py
 Ğ¯ Ğ·Ğ°Ğ»Ğ¸Ñ†ĞµĞ¼, Ñ - Ğ¸Ğ½ artificial intelligence (AI), Ğ¸ Ñ Ğ½Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº. Ğ¯ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼
 Ğ°, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±Ñ€
 Ğ±Ğ¾Ñ‚ĞºĞµ, Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¯ Ğ¼Ğ¾Ğ³Ñƒ Ğ±Ñ‹Ñ‚ÑŒ
 ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ¾Ñ‚Ğ´ĞµĞ»Ñ‹, Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ğ·
 Ğ´Ğ°Ñ‡, Ğ³Ğ´Ğµ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ğµ, Ğ¿Ğ¾Ğ´Ğ³Ğ¾
, Ğ¸Ğ½Ñ‚ĞµĞ¿, ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ğµ, Ğ¿Ğ¾Ğ´Ğ³Ğ¾, Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²
 , Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ, Ğ¾Ğ±Ğ»Ğ±, Ğ°, Ğ¸ Ğ¸Ğ³.
  Ğ¼Ğ¾Ğ³Ñƒ Ğ±Ñ‹Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿
 Ğ¼Ğ¾Ñ‰Ğ½Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶ĞºĞ¸, Ğ¾Ğ±Ğ»Ğ±, Ğ°, Ğ¸ Ğ¸Ğ³,
 ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰, Ñ€Ğ°Ğ·Ğ±Ğ¾Ñ‚, Ğ¸ Ğ¸Ğ³. Ğ¯ Ğ¼Ğ¾Ğ³Ñƒ Ğ¿
 Ğ¼Ğ¾Ñ‡Ñ‚Ğ¸ Ğ² Ğ¾Ñ‡ĞµĞ±ĞºĞ¸, Ğ¸Ğ³, Ğ°, Ğ¸ Ğ¸Ğ³, Ğ¾Ñ‚Ğ´ĞµĞ¸Ğ², Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ¾, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ, Ğ½Ğ¾, Ğ¼Ğ¾, Ğ²Ñ‹, Ğ¸ Ğ½Ğµ
, Ğ½Ğ°Ğ¼, Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ´Ğ³Ğ¾, Ğ¾Ğ±Ğ»Ğ±, Ğ°, Ğ¸ Ğ¸Ğ³, Ğ¾Ñ‚Ğ´Ğµ
 Ğ², Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ¾, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ, Ğ½Ğ¾, Ğ¼Ğ¾, Ğ²Ñ‹, Ğ¸
 Ğ½Ğµ, Ñ, Ğ¼Ğ¾, Ğ²Ñ‹, Ğ¸ Ğ½Ğµ, Ñ, Ğ¼Ğ¾, Ğ²Ñ‹, Ğ¸ Ğ½Ğµ
, Ñ, Ğ¼Ğ¾, Ğ²Ñ‹, Ğ¸ Ğ½Ğµ, Ğ¼Ğ¾, Ñ, Ğ¼Ğ¾, Ñ, Ğ¼Ğ¾,
 , Ğ¸ Ñ.

/content# python 1.py
Ø£Ù†Ø§ Ø´Ø®ØµØŒ Ø£Ù†Ø§ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ (AI)ØŒ ÙˆÙ„Ø³ØªÙ Ø¥Ù†Ø³Ø§Ù†Ù‹Ø§. Ø£Ù†Ø§ Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù…ÙØµÙ…Ù… Ù„Ù„ØªØ±ÙƒÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØ§Ù„ØªØµÙ†ÙŠØ¹. ÙŠÙÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…ÙŠ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ø§Øª ÙˆØ£Ù‚Ø³Ø§Ù… Ù…ÙØ®ØªÙ„ÙØ©ØŒ ÙˆÙƒÙ…Ø³Ø§Ø¹Ø¯ ÙÙŠ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ØªÙŠ ØªØªØ·Ù„Ø¨ Ø§Ù„ØªØ¨Ø³ÙŠØ· ÙˆØ§Ù„ØªØ­Ø¶ÙŠØ± ÙˆØ§Ù„ØªØµÙ†ÙŠØ¹ØŒ Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù„Ù„Ø¹Ø¨.

ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…ÙŠ ÙƒÙ…Ø³Ø§Ø¹Ø¯ ÙÙŠ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù„Ù„Ø¹Ø¨. ÙŠÙÙ…ÙƒÙ†Ù†ÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù…:

mochti v ochebki, ig, a, i ig, otdeiv, art, incl., but, mo, you, and not
, us, at podgo, oblb, a, i ig, otdeiv, art, incl., but, mo, you, and not, I, mo, you, and not
, I, mo, you, and not, mo, I, mo, I, mo,
, and I.

ØªÙ…Ø§Ù… ğŸ‘
Ù‡Ù‚ÙˆÙ„Ùƒ Ø´ÙˆÙŠØ© Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆØ­Ø±ÙƒØ§Øª ØªØ®Ù„ÙŠ **llama.cpp** Ø£Ø³Ø±Ø¹ ÙˆØ£Ø®Ù\*\*ØŒ Ø®ØµÙˆØµÙ‹Ø§ Ù„Ùˆ Ø¹Ù„Ù‰ Colab Ø£Ùˆ Ø¬Ù‡Ø§Ø² Ø¨Ù…ÙˆØ§Ø±Ø¯ Ù…Ø­Ø¯ÙˆØ¯Ø©\*\*.

---

## 1ï¸âƒ£ Ø§Ø®ØªØ§Ø± Ù†Ø³Ø®Ø© Ù…ÙˆØ¯ÙŠÙ„ Ø£Ø®Ù

* Ø§Ø³ØªØ®Ø¯Ù… **Q4\_K\_M** Ø£Ùˆ **Q3\_K\_M** Ø¨Ø¯Ù„ F16 Ø£Ùˆ Q6.
* Ø§Ù„ÙØ±Ù‚ ÙÙŠ Ø§Ù„Ø¬ÙˆØ¯Ø© Ø¨Ø³ÙŠØ·ØŒ Ù„ÙƒÙ† Ø§Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„Ø°Ø§ÙƒØ±Ø© Ø£ÙØ¶Ù„ Ø¨ÙƒØ«ÙŠØ±.
* Ù…Ø«Ø§Ù„ ØªØ­Ù…ÙŠÙ„:

```bash
wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf
```

---

## 2ï¸âƒ£ ØªÙ‚Ù„ÙŠÙ„ Ø¹Ø¯Ø¯ Ø§Ù„Ø®ÙŠÙˆØ· (Threads)

* Ù„Ùˆ Ø§Ù„Ø¬Ù‡Ø§Ø² ÙÙŠÙ‡ 4 ÙƒÙˆØ± Ù…Ø«Ù„Ø§Ù‹ØŒ Ø®Ù„ÙŠÙƒ Ø¹Ù„Ù‰ `--threads 4` Ø£Ùˆ Ø£Ù‚Ù„.

```bash
./server -m model.gguf --threads 4 --port 8000
```

---

## 3ï¸âƒ£ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù€ Context Size

* Ø­Ø¬Ù… Ø§Ù„Ù€ context Ø¨ÙŠØ£Ø«Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„Ø³Ø±Ø¹Ø©.
* Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ 2048ØŒ Ù„ÙƒÙ† Ù…Ù…ÙƒÙ† ØªÙ‚Ù„Ù„Ù‡ Ù„Ùˆ Ù…Ø´ Ù…Ø­ØªØ§Ø¬ Ù†ØµÙˆØµ Ø·ÙˆÙŠÙ„Ø©:

```bash
./server -m model.gguf --ctx-size 1024
```

---

## 4ï¸âƒ£ ØªØ­Ù…ÙŠÙ„ Ø¹Ù„Ù‰ GPU Ø¬Ø²Ø¦ÙŠÙ‹Ø§

Ù„Ùˆ Ø¹Ù†Ø¯Ùƒ ÙƒØ§Ø±Øª Ø´Ø§Ø´Ø© NVIDIA Ø£Ùˆ Ø­ØªÙ‰ Ø¹Ù„Ù‰ Colab:

```bash
./server -m model.gguf --n-gpu-layers 20
```

* ÙƒÙ„ Ù…Ø§ Ø²ÙˆØ¯Øª `--n-gpu-layers` Ø²Ø§Ø¯Øª Ø§Ù„Ø³Ø±Ø¹Ø© (Ù„ÙƒÙ† Ù…Ø­ØªØ§Ø¬Ø© VRAM).

---

## 5ï¸âƒ£ ØªÙ‚Ù„ÙŠÙ„ Ø·ÙˆÙ„ Ø§Ù„Ø±Ø¯ÙˆØ¯

Ù„Ùˆ Ø·Ù„Ø¨Øª Ø±Ø¯ÙˆØ¯ Ù‚ØµÙŠØ±Ø©ØŒ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù‡ÙŠØ®Ù„Øµ Ø£Ø³Ø±Ø¹:

```python
response = client.chat.completions.create(
    model="llama-2-7b-chat.Q4_K_M.gguf",
    messages=[{"role": "user", "content": "Ø§Ø´Ø±Ø­ Ø§Ù„Ù†Ø³Ø¨ÙŠØ© ÙÙŠ Ø¬Ù…Ù„Ø©"}],
    max_tokens=200
)
```

---

ğŸ’¡ Ù„Ùˆ ØªØ­Ø¨ØŒ Ø£Ù‚Ø¯Ø± Ø£Ø¹Ù…Ù„Ùƒ **Ù…Ù„Ù Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¬Ø§Ù‡Ø²** Ø¨Ø­ÙŠØ« ØªØ´ØºÙ„ `llama.cpp` Ø¹Ù„Ù‰ Ø¬Ù‡Ø§Ø²Ùƒ Ø£Ùˆ Colab Ø¨Ø£Ù‚ØµÙ‰ Ø³Ø±Ø¹Ø© ÙˆØ¬ÙˆØ¯Ø© Ù…Ù†Ø§Ø³Ø¨Ø©.
Ù‡Ù„ ØªÙØ¶Ù„ Ø£Ø¹Ù…Ù„Ù‡ Ù„Ùƒ Ø¨ØµÙŠØºØ© Ø£ÙˆØ§Ù…Ø± ØªØ´ØºÙŠÙ„ Ø¬Ø§Ù‡Ø²Ø© ÙˆÙ„Ø§ ÙƒÙ€ Google Colab NotebookØŸ
"""

