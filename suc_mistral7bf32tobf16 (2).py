# -*- coding: utf-8 -*-
"""suc_mistral7bF32toBF16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14kx-45MConyoiIuee5vzZvLsqncXIaCH
"""



!pip install llama-cpp-python

# !pip install llama-cpp-python

from llama_cpp import Llama

llm = Llama.from_pretrained(
	repo_id="rakmik/Mistral-7B-Instruct-v0.3-Q8_0-GGUF",
	filename="mistral-7b-instruct-v0.3-q8_0-00001-of-00002.gguf",
)

llm.create_chat_completion(
	messages = "No input example has been defined for this model task."
)

!wget https://huggingface.co/rakmik/Mistral-7B-Instruct-v0.3-Q8_0-GGUF/resolve/main/mistral-7b-instruct-v0.3-q8_0-00001-of-00002.gguf

!wget https://huggingface.co/rakmik/Mistral-7B-Instruct-v0.3-Q8_0-GGUF/resolve/main/mistral-7b-instruct-v0.3-q8_0-00002-of-00002.gguf

from llama_cpp import Llama

llm = Llama(model_path="/content/mistral-7b-instruct-v0.3-q8_0-00001-of-00002.gguf")
response = llm.create_chat_completion(
  messages=[
    {
        "role": "user",
        "content": "how big is the sky"
    }
])
print(response)



from llama_cpp import Llama

llm = Llama(
      model_path="./models/7B/llama-model.gguf",
      # n_gpu_layers=-1, # Uncomment to use GPU acceleration
      # seed=1337, # Uncomment to set a specific seed
      # n_ctx=2048, # Uncomment to increase the context window
)
output = llm(
      "Q: Name the planets in the solar system? A: ", # Prompt
      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window
      stop=["Q:", "\n"], # Stop generating just before the model would generate a new question
      echo=True # Echo the prompt back in the output
) # Generate a completion, can also call create_completion
print(output)

!wget https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3-f32.gguf

!wget https://github.com/ggml-org/llama.cpp/releases/download/b6123/llama-b6123-bin-ubuntu-x64.zip

!unzip llama-b6123-bin-ubuntu-x64.zip

!./llama-quantize ./models/mymodel/ggml-model-f16.gguf ./models/mymodel/ggml-model-Q4_K_M.gguf Q4_K_M

./llama-quantize ./models/mymodel/ggml-model-f16.gguf ./models/mymodel/ggml-model-Q4_K_M.gguf Q4_K_M

!build/bin/llama-quantize -h

!build/bin/llama-quantize /content/Mistral-7B-Instruct-v0.3-f32.gguf /content/Mistral-7B-Instruct-v0.3-bf16.gguf BF16

!/content/build/bin/llama-gguf-split -h

!build/bin/llama-cli -h

!build/bin/llama-cli -m /content/Mistral-7B-Instruct-v0.3-f32.gguf

xxxxxxx

/content/Mistral-7B-Instruct-v0.3-bf16.gguf

# 1. تثبيت المكتبة
#!pip install huggingface_hub

# 2. تسجيل الدخول
from huggingface_hub import notebook_login
notebook_login()

# 3. رفع الملف
from huggingface_hub import HfApi

# تأكد من أن هذا المسار صحيح وموجود في بيئة Colab الخاصة بك
file_path = "/content/Mistral-7B-Instruct-v0.3-bf16.gguf"
repo_id = "rakmik/Mistral-7B-Instruct-v0.3-GGUF_F16"

# تأكد من إنشاء هذا المستودع على حسابك في Hugging Face أولاً
# يمكنك إنشاؤه من هنا: https://huggingface.co/new

try:
    api = HfApi()
    api.upload_file(
        path_or_fileobj=file_path,
        path_in_repo="Mistral-7B-Instruct-v0.3-GGUF_F16", # اسم الملف في المستودع
        repo_id=repo_id,
        repo_type="model"
    )
    print(f"تم رفع الملف بنجاح! يمكنك مشاهدته هنا: https://huggingface.co/{repo_id}/tree/main")

except Exception as e:
    print(f"حدث خطأ أثناء الرفع: {e}")

!build/bin/llama-cli -m /content/Mistral-7B-Instruct-v0.3-bf16.gguf -p "I believe the meaning of life is" -n 128 -no-cnv

!/content/build/bin/llama-gguf-split -h

!/content/build/bin/llama-gguf-split --split-max-size 5G  /content/Mistral-7B-Instruct-v0.3-bf16.gguf

!/content/build/bin/llama-gguf-split --split-max-size 5G /content/Mistral-7B-Instruct-v0.3-bf16.gguf /content/Mistral-7B-Instruct-v0.3-bf16-split.gguf

from huggingface_hub import HfApi

# تهيئة الواجهة البرمجية
api = HfApi()

# تحديد المجلد المحلي واسم المستودع
folder_path = "/content/3parts"
repo_id = "rakmik/Mistral-7B-Instruct-v0.3-GGUF_F16"

# رفع المجلد
# سيتم رفع محتويات المجلد مباشرة إلى المستودع
api.upload_folder(
    folder_path=folder_path,
    repo_id=repo_id,
    repo_type="model"
)

print(f"تم رفع المجلد بنجاح إلى المستودع: https://huggingface.co/{repo_id}/tree/main")

!build/bin/llama-cli -m /content/3parts/Mistral-7B-Instruct-v0.3-bf16-split.gguf-00001-of-00003.gguf -p "I believe the meaning of life is" -n 128 -no-cnv

from google.colab import userdata
userdata.get('OPENAI_API_KEY')

# استيراد المكتبات اللازمة
from google.colab import userdata
import os

# الحصول على المفتاح السري من أسرار Colab وتعيينه كمتغير بيئي
os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

!pip install openai

# استيراد المكتبات اللازمة
from openai import OpenAI
import os

# سيقوم هذا السطر الآن بالعثور على المفتاح الذي قمت بتعيينه
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

# بقية الكود الخاص بك كما هو
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "user", "content": "Write a one-sentence bedtime story about a unicorn."}
    ]
)

print(response.choices[0].message.content)

from google.colab import userdata
from openai import OpenAI
import os

# 1. احصل على المفتاح من الأسرار وخزنه في متغير
#    لا تقم بطباعة هذا المتغير أبداً
api_key_from_secrets = userdata.get('OPENAI_API_KEY')

# 2. قم بتعيينه كمتغير بيئي (طريقة آمنة)
os.environ["OPENAI_API_KEY"] = api_key_from_secrets

# 3. الآن يمكنك استخدام المكتبة بأمان
#    هي ستقرأ المفتاح من متغيرات البيئة تلقائياً
client = OpenAI() # لا تحتاج لتمرير المفتاح يدوياً إذا كان في متغيرات البيئة

response = client.chat.completions.create(
    model="gpt-4o", # Replace with the correct model name, e.g., "gpt-3.5-turbo" or "gpt-4"
    messages=[
        {"role": "user", "content": "Write a one-sentence bedtime story about a unicorn."}
    ]
)

print(response.choices[0].message.content)

from openai import OpenAI
import os

# Ensure your OpenAI API key is set as an environment variable named OPENAI_API_KEY
# In Colab, you can add it to the Secrets tab (🔑 icon on the left panel)
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

response = client.chat.completions.create(
    model="gpt-3.5-turbo", # Replace with the correct model name, e.g., "gpt-3.5-turbo" or "gpt-4"
    messages=[
        {"role": "user", "content": "Write a one-sentence bedtime story about a unicorn."}
    ]
)

print(response.choices[0].message.content)

from openai import OpenAI
client = OpenAI()

response = client.responses.create(
    model="gpt-5",
    input="Write a one-sentence bedtime story about a unicorn."
)

print(response.output_text)

from llama_cpp import Llama

llm = Llama(model_path="/content/Mistral-7B-Instruct-v0.3-bf16.gguf")
print(llm("Hello, who are you?"))

import openai

# بدل ما يكون رابط OpenAI، نخليه رابط السيرفر المحلي أو البديل
openai.api_base = "http://localhost:11434/v1"  # مثال: Ollama
openai.api_key = "anything"  # مش مهم المفتاح لو السيرفر محلي

response = openai.ChatCompletion.create(
    model="llama3",
    messages=[{"role": "user", "content": "Hello from local model!"}]
)

print(response.choices[0].message.content)

1. تشغيل Ollama
لو ويندوز أو ماك:

نزل من https://ollama.com/download

بعد التثبيت، شغّل:

bash
Copy
Edit
ollama run llama3
2. تشغيل سيرفر يحاكي OpenAI API
نعمل سيرفر بايثون صغير:

python
Copy
Edit
from flask import Flask, request, jsonify
import requests

app = Flask(__name__)

@app.route("/v1/chat/completions", methods=["POST"])
def chat_completions():
    data = request.json
    user_message = data["messages"][-1]["content"]

    # إرسال الطلب إلى Ollama
    resp = requests.post("http://localhost:11434/api/generate", json={
        "model": data["model"],
        "prompt": user_message,
        "stream": False
    }).json()

    return jsonify({
        "choices": [{
            "message": {"role": "assistant", "content": resp["response"]}
        }]
    })

if __name__ == "__main__":
    app.run(port=5001)
تشغّل الملف:

bash
Copy
Edit
python server.py
3. استخدامه بنفس كود OpenAI
python
Copy
Edit
import openai

openai.api_base = "http://localhost:5001/v1"
openai.api_key = "dummy"

resp = openai.ChatCompletion.create(
    model="llama3",
    messages=[{"role": "user", "content": "أخبرني عن أينشتاين"}]
)
print(resp.choices[0].message.content)

!./server -m llama-2-7b-chat.Q4_K_M.gguf --port 8000

http://localhost:8000/v1/chat/completions

!/content/build/bin/llama-server -m /content/Mistral-7B-Instruct-v0.3-bf16.gguf --port 8000

import openai

openai.api_base = "http://localhost:8000/v1"
openai.api_key = "not-needed"

response = openai.ChatCompletion.create(
    model="Mistral-7B-Instruct-v0.3-bf16.gguf",
    messages=[
        {"role": "user", "content": "من هو أينشتاين؟"}
    ]
)

print(response.choices[0].message.content)









!pip install openai==0.28

import openai

openai.api_base = "http://localhost:8000/v1"
openai.api_key = "not-needed"

response = openai.ChatCompletion.create(
    model="llama-2-7b-chat.Q4_K_M.gguf",
    messages=[
        {"role": "user", "content": "من هو أينشتاين؟"}
    ]
)

print(response.choices[0].message.content)

!/content/build/bin/llama-server -m /content/Mistral-7B-Instruct-v0.3-bf16.gguf --port 8000

import openai

openai.api_base = "http://localhost:8000/v1"
openai.api_key = "not-needed"
openai.timeout = 1000  # مهلة 5 دقائق

response = openai.ChatCompletion.create(
    model="Mistral-7B-Instruct-v0.3-IQ1_M.gguf",
    messages=[
        {"role": "user", "content": "who is ai؟"}
    ]
)
print(response.choices[0].message.content)

!wget https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3-IQ1_M.gguf

!/content/build/bin/llama-server -m /content/Mistral-7B-Instruct-v0.3-IQ1_M.gguf --port 8000

"""/content# python 1.py
 Я залицем, я - ин artificial intelligence (AI), и я не человек. Я програм
 а, созданная для компоторизации, обр
 ботке, искусственности. Я могу быть
 спользована в различных приложениях, отделы, и в качестве помощника для з
 дач, где необходино упрощение, подго
, интеп, упрощение, подго, и искусств
 , включая обработку, облб, а, и иг.
  могу быть использована в качестве п
 мощника для надежки, облб, а, и иг,
 ключая помощ, разбот, и иг. Я могу п
 мочти в очебки, иг, а, и иг, отдеив, искусство, включая, но, мо, вы, и не
, нам, при подго, облб, а, и иг, отде
 в, искусство, включая, но, мо, вы, и
 не, я, мо, вы, и не, я, мо, вы, и не
, я, мо, вы, и не, мо, я, мо, я, мо,
 , и я.

/content# python 1.py
أنا شخص، أنا ذكاء اصطناعي (AI)، ولستُ إنسانًا. أنا برنامج مُصمم للتركيب والمعالجة والتصنيع. يُمكن استخدامي في تطبيقات وأقسام مُختلفة، وكمساعد في المهام التي تتطلب التبسيط والتحضير والتصنيع، بما في ذلك المعالجة والتحليل واللعب.

يمكن استخدامي كمساعد في الموثوقية والتحليل واللعب. يُمكنني استخدام:

mochti v ochebki, ig, a, i ig, otdeiv, art, incl., but, mo, you, and not
, us, at podgo, oblb, a, i ig, otdeiv, art, incl., but, mo, you, and not, I, mo, you, and not
, I, mo, you, and not, mo, I, mo, I, mo,
, and I.

تمام 👍
هقولك شوية إعدادات وحركات تخلي **llama.cpp** أسرع وأخف\*\*، خصوصًا لو على Colab أو جهاز بموارد محدودة\*\*.

---

## 1️⃣ اختار نسخة موديل أخف

* استخدم **Q4\_K\_M** أو **Q3\_K\_M** بدل F16 أو Q6.
* الفرق في الجودة بسيط، لكن السرعة والذاكرة أفضل بكثير.
* مثال تحميل:

```bash
wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf
```

---

## 2️⃣ تقليل عدد الخيوط (Threads)

* لو الجهاز فيه 4 كور مثلاً، خليك على `--threads 4` أو أقل.

```bash
./server -m model.gguf --threads 4 --port 8000
```

---

## 3️⃣ تحديد الـ Context Size

* حجم الـ context بيأثر على الذاكرة والسرعة.
* الافتراضي 2048، لكن ممكن تقلله لو مش محتاج نصوص طويلة:

```bash
./server -m model.gguf --ctx-size 1024
```

---

## 4️⃣ تحميل على GPU جزئيًا

لو عندك كارت شاشة NVIDIA أو حتى على Colab:

```bash
./server -m model.gguf --n-gpu-layers 20
```

* كل ما زودت `--n-gpu-layers` زادت السرعة (لكن محتاجة VRAM).

---

## 5️⃣ تقليل طول الردود

لو طلبت ردود قصيرة، الموديل هيخلص أسرع:

```python
response = client.chat.completions.create(
    model="llama-2-7b-chat.Q4_K_M.gguf",
    messages=[{"role": "user", "content": "اشرح النسبية في جملة"}],
    max_tokens=200
)
```

---

💡 لو تحب، أقدر أعملك **ملف إعدادات جاهز** بحيث تشغل `llama.cpp` على جهازك أو Colab بأقصى سرعة وجودة مناسبة.
هل تفضل أعمله لك بصيغة أوامر تشغيل جاهزة ولا كـ Google Colab Notebook؟
"""

