# -*- coding: utf-8 -*-
"""suc_llama-cpp-python.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HsgeeBz4Yqoz21OxkeABD8556v9Vbc48
"""

!pip install llama-cpp-python

from llama_cpp import Llama

# Download and load a GGUF model directly from Hugging Face
llm = Llama.from_pretrained(
   repo_id="TheBloke/Mistral-7B-Instruct-v0.2-GGUF",
   filename="mistral-7b-instruct-v0.2.Q2_K.gguf"
)
response = llm.create_chat_completion(
  messages=[
    {"role": "user", "content": "How does a black hole work?"}
  ]
)
print(response)

!llama-server -m mistral-7b-instruct-v0.2.Q2_K.gguf

!wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q2_K.gguf

from llama_cpp import Llama

llm = Llama(model_path="mistral-7b-instruct-v0.2.Q2_K.gguf")
response = llm.create_chat_completion(
  messages=[
    {
        "role": "user",
        "content": "how big is the sky"
    }
])
print(response)