{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-FvAbvjBAKQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-fgqFeyCBhH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kvcache-ai/ktransformers.git"
      ],
      "metadata": {
        "id": "G2zYJatfBhKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install libtbb-dev libssl-dev libcurl4-openssl-dev libaio1 libaio-dev libgflags-dev zlib1g-dev libfmt-dev\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWDwTElnCYS_",
        "outputId": "6e4cf4f2-5033-40b6-b246-8d3f071b803a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libaio1 is already the newest version (0.3.112-13build1).\n",
            "libaio1 set to manually installed.\n",
            "libtbb-dev is already the newest version (2021.5.0-7ubuntu2).\n",
            "libtbb-dev set to manually installed.\n",
            "libcurl4-openssl-dev is already the newest version (7.81.0-1ubuntu1.20).\n",
            "libssl-dev is already the newest version (3.0.2-0ubuntu1.19).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-2ubuntu9.2).\n",
            "zlib1g-dev set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  libfmt8 libgflags2.2\n",
            "Suggested packages:\n",
            "  libfmt-doc\n",
            "The following NEW packages will be installed:\n",
            "  libaio-dev libfmt-dev libfmt8 libgflags-dev libgflags2.2\n",
            "0 upgraded, 5 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 371 kB of archives.\n",
            "After this operation, 1,820 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libaio-dev amd64 0.3.112-13build1 [21.2 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfmt8 amd64 8.1.1+ds1-2 [60.2 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgflags2.2 amd64 2.2.2-2 [78.1 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgflags-dev amd64 2.2.2-2 [93.7 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfmt-dev amd64 8.1.1+ds1-2 [118 kB]\n",
            "Fetched 371 kB in 2s (234 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 5.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libaio-dev:amd64.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack .../libaio-dev_0.3.112-13build1_amd64.deb ...\n",
            "Unpacking libaio-dev:amd64 (0.3.112-13build1) ...\n",
            "Selecting previously unselected package libfmt8:amd64.\n",
            "Preparing to unpack .../libfmt8_8.1.1+ds1-2_amd64.deb ...\n",
            "Unpacking libfmt8:amd64 (8.1.1+ds1-2) ...\n",
            "Selecting previously unselected package libgflags2.2.\n",
            "Preparing to unpack .../libgflags2.2_2.2.2-2_amd64.deb ...\n",
            "Unpacking libgflags2.2 (2.2.2-2) ...\n",
            "Selecting previously unselected package libgflags-dev.\n",
            "Preparing to unpack .../libgflags-dev_2.2.2-2_amd64.deb ...\n",
            "Unpacking libgflags-dev (2.2.2-2) ...\n",
            "Selecting previously unselected package libfmt-dev:amd64.\n",
            "Preparing to unpack .../libfmt-dev_8.1.1+ds1-2_amd64.deb ...\n",
            "Unpacking libfmt-dev:amd64 (8.1.1+ds1-2) ...\n",
            "Setting up libaio-dev:amd64 (0.3.112-13build1) ...\n",
            "Setting up libfmt8:amd64 (8.1.1+ds1-2) ...\n",
            "Setting up libgflags2.2 (2.2.2-2) ...\n",
            "Setting up libgflags-dev (2.2.2-2) ...\n",
            "Setting up libfmt-dev:amd64 (8.1.1+ds1-2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kvcache-ai/ktransformers.git\n",
        "%cd ktransformers\n",
        "!git submodule update --init --recursive\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ2LTXeGCaea",
        "outputId": "d7702e9e-723e-4693-d879-07c6937208cd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ktransformers'...\n",
            "remote: Enumerating objects: 6055, done.\u001b[K\n",
            "remote: Counting objects: 100% (275/275), done.\u001b[K\n",
            "remote: Compressing objects: 100% (170/170), done.\u001b[K\n",
            "remote: Total 6055 (delta 145), reused 164 (delta 103), pack-reused 5780 (from 5)\u001b[K\n",
            "Receiving objects: 100% (6055/6055), 16.79 MiB | 16.65 MiB/s, done.\n",
            "Resolving deltas: 100% (3515/3515), done.\n",
            "/content/ktransformers\n",
            "Submodule 'third_party/PhotonLibOS' (https://github.com/alibaba/PhotonLibOS.git) registered for path 'third_party/PhotonLibOS'\n",
            "Submodule 'third_party/custom_flashinfer' (https://github.com/kvcache-ai/custom_flashinfer.git) registered for path 'third_party/custom_flashinfer'\n",
            "Submodule 'third_party/llama.cpp' (https://github.com/ggerganov/llama.cpp.git) registered for path 'third_party/llama.cpp'\n",
            "Submodule 'third_party/prometheus-cpp' (https://github.com/jupp0r/prometheus-cpp) registered for path 'third_party/prometheus-cpp'\n",
            "Submodule 'third_party/pybind11' (https://github.com/pybind/pybind11.git) registered for path 'third_party/pybind11'\n",
            "Submodule 'third_party/spdlog' (https://github.com/gabime/spdlog.git) registered for path 'third_party/spdlog'\n",
            "Submodule 'third_party/xxHash' (https://github.com/Cyan4973/xxHash.git) registered for path 'third_party/xxHash'\n",
            "Cloning into '/content/ktransformers/third_party/PhotonLibOS'...\n",
            "Cloning into '/content/ktransformers/third_party/custom_flashinfer'...\n",
            "Cloning into '/content/ktransformers/third_party/llama.cpp'...\n",
            "Cloning into '/content/ktransformers/third_party/prometheus-cpp'...\n",
            "Cloning into '/content/ktransformers/third_party/pybind11'...\n",
            "Cloning into '/content/ktransformers/third_party/spdlog'...\n",
            "Cloning into '/content/ktransformers/third_party/xxHash'...\n",
            "remote: Enumerating objects: 78, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 78 (delta 37), reused 37 (delta 37), pack-reused 33 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (78/78), 15.87 KiB | 706.00 KiB/s, done.\n",
            "From https://github.com/alibaba/PhotonLibOS\n",
            " * branch            92f56d4527c24aafcee75d87fd72fce25680266f -> FETCH_HEAD\n",
            "Submodule path 'third_party/PhotonLibOS': checked out '92f56d4527c24aafcee75d87fd72fce25680266f'\n",
            "Submodule path 'third_party/custom_flashinfer': checked out 'a3ebdf890fd15550283e692c1cd3d673be78a93c'\n",
            "Submodule '3rdparty/composable_kernels' (https://github.com/ROCm/composable_kernel.git) registered for path 'third_party/custom_flashinfer/3rdparty/composable_kernels'\n",
            "Submodule '3rdparty/cutlass' (https://github.com/NVIDIA/cutlass.git) registered for path 'third_party/custom_flashinfer/3rdparty/cutlass'\n",
            "Submodule '3rdparty/googletest' (https://github.com/google/googletest.git) registered for path 'third_party/custom_flashinfer/3rdparty/googletest'\n",
            "Submodule '3rdparty/mscclpp' (https://github.com/microsoft/mscclpp.git) registered for path 'third_party/custom_flashinfer/3rdparty/mscclpp'\n",
            "Submodule '3rdparty/nvbench' (https://github.com/NVIDIA/nvbench.git) registered for path 'third_party/custom_flashinfer/3rdparty/nvbench'\n",
            "Submodule '3rdparty/spdlog' (https://github.com/gabime/spdlog.git) registered for path 'third_party/custom_flashinfer/3rdparty/spdlog'\n",
            "Cloning into '/content/ktransformers/third_party/custom_flashinfer/3rdparty/composable_kernels'...\n",
            "Cloning into '/content/ktransformers/third_party/custom_flashinfer/3rdparty/cutlass'...\n",
            "Cloning into '/content/ktransformers/third_party/custom_flashinfer/3rdparty/googletest'...\n",
            "Cloning into '/content/ktransformers/third_party/custom_flashinfer/3rdparty/mscclpp'...\n",
            "Cloning into '/content/ktransformers/third_party/custom_flashinfer/3rdparty/nvbench'...\n",
            "Cloning into '/content/ktransformers/third_party/custom_flashinfer/3rdparty/spdlog'...\n",
            "Submodule path 'third_party/custom_flashinfer/3rdparty/composable_kernels': checked out '5055b3bdcb5a7f0b8f359b606d3c5b75efd6df54'\n",
            "Submodule path 'third_party/custom_flashinfer/3rdparty/cutlass': checked out 'cc3c29a81a140f7b97045718fb88eb0664c37bd7'\n",
            "Submodule path 'third_party/custom_flashinfer/3rdparty/googletest': checked out '5a37b517ad4ab6738556f0284c256cae1466c5b4'\n",
            "Submodule path 'third_party/custom_flashinfer/3rdparty/mscclpp': checked out 'cddffbc8b6dfa6facf7c64c1b7d73acf30e600b3'\n",
            "Submodule path 'third_party/custom_flashinfer/3rdparty/nvbench': checked out '555d628e9b250868c9da003e4407087ff1982e8e'\n",
            "Submodule path 'third_party/custom_flashinfer/3rdparty/spdlog': checked out 'c3aed4b68373955e1cc94307683d44dca1515d2b'\n",
            "Submodule path 'third_party/llama.cpp': checked out 'a94e6ff8774b7c9f950d9545baf0ce35e8d1ed2f'\n",
            "Submodule 'kompute' (https://github.com/nomic-ai/kompute.git) registered for path 'third_party/llama.cpp/kompute'\n",
            "Cloning into '/content/ktransformers/third_party/llama.cpp/kompute'...\n",
            "Submodule path 'third_party/llama.cpp/kompute': checked out '4565194ed7c32d1d2efa32ceab4d3c6cae006306'\n",
            "Submodule path 'third_party/prometheus-cpp': checked out 'f13cdd052eeae5e89decc11bf03697d0f78b15bc'\n",
            "Submodule 'civetweb' (https://github.com/civetweb/civetweb.git) registered for path 'third_party/prometheus-cpp/3rdparty/civetweb'\n",
            "Submodule 'googletest' (https://github.com/google/googletest.git) registered for path 'third_party/prometheus-cpp/3rdparty/googletest'\n",
            "Cloning into '/content/ktransformers/third_party/prometheus-cpp/3rdparty/civetweb'...\n",
            "Cloning into '/content/ktransformers/third_party/prometheus-cpp/3rdparty/googletest'...\n",
            "Submodule path 'third_party/prometheus-cpp/3rdparty/civetweb': checked out 'd7ba35bbb649209c66e582d5a0244ba988a15159'\n",
            "Submodule path 'third_party/prometheus-cpp/3rdparty/googletest': checked out 'e2239ee6043f73722e7aa812a459f54a28552929'\n",
            "Submodule path 'third_party/pybind11': checked out 'bb05e0810b87e74709d9f4c4545f1f57a1b386f5'\n",
            "Submodule path 'third_party/spdlog': checked out '48bcf39a661a13be22666ac64db8a7f886f2637e'\n",
            "Submodule path 'third_party/xxHash': checked out '953a09abc39096da9e216b6eb0002c681cdc1199'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ktransformers\n",
        "!bash install.sh\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFMnmqlsCgAA",
        "outputId": "b300d86f-375f-4fc0-dcef-14029e72ff1e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ktransformers\n",
            "Selected backend: cuda\n",
            "Installing python dependencies from requirements.txt\n",
            "Ignoring cpufeature: markers 'sys_platform == \"win32\" or sys_platform == \"Windows\"' don't match your environment\n",
            "Collecting fire (from -r requirements-local_chat.txt (line 1))\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers==4.51.3 (from -r requirements-local_chat.txt (line 2))\n",
            "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 3)) (2.0.2)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 4)) (2.6.0+cu124)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 5)) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 7)) (5.29.5)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 8)) (0.9.0)\n",
            "Requirement already satisfied: blobfile in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 9)) (3.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (0.33.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->-r requirements-local_chat.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (2025.7.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.3.0->-r requirements-local_chat.txt (line 4))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.3.0->-r requirements-local_chat.txt (line 4))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.3.0->-r requirements-local_chat.txt (line 4))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.3.0->-r requirements-local_chat.txt (line 4))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.3.0->-r requirements-local_chat.txt (line 4))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.3.0->-r requirements-local_chat.txt (line 4))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.3.0->-r requirements-local_chat.txt (line 4))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.3.0->-r requirements-local_chat.txt (line 4))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.3.0->-r requirements-local_chat.txt (line 4))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.3.0->-r requirements-local_chat.txt (line 4))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: pycryptodomex>=3.8 in /usr/local/lib/python3.11/dist-packages (from blobfile->-r requirements-local_chat.txt (line 9)) (3.23.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.11/dist-packages (from blobfile->-r requirements-local_chat.txt (line 9)) (2.5.0)\n",
            "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.11/dist-packages (from blobfile->-r requirements-local_chat.txt (line 9)) (5.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (3.0.2)\n",
            "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m123.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m123.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=3b7c38aee0e0a37434b602991b0d526c3054920fd802a74a363837d021e98dd6\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built fire\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fire, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, transformers\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.53.2\n",
            "    Uninstalling transformers-4.53.2:\n",
            "      Successfully uninstalled transformers-4.53.2\n",
            "Successfully installed fire-0.7.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 transformers-4.51.3\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 1)) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers==4.51.3 in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 2)) (4.51.3)\n",
            "Requirement already satisfied: fastapi>=0.111.0 in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 3)) (0.116.1)\n",
            "Requirement already satisfied: langchain>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 4)) (0.3.26)\n",
            "Collecting blessed>=1.20.0 (from -r ktransformers/server/requirements.txt (line 5))\n",
            "  Downloading blessed-1.21.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: accelerate>=0.31.0 in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 6)) (1.9.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.97 in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 8)) (1.97.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 9)) (75.2.0)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 10)) (1.2.2.post1)\n",
            "Collecting ninja (from -r ktransformers/server/requirements.txt (line 11))\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 12)) (0.45.1)\n",
            "Collecting colorlog (from -r ktransformers/server/requirements.txt (line 13))\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 14)) (0.7.0)\n",
            "Collecting zmq (from -r ktransformers/server/requirements.txt (line 15))\n",
            "  Downloading zmq-0.0.0.zip (2.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 16)) (5.9.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (2025.7.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->-r ktransformers/server/requirements.txt (line 3)) (0.47.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->-r ktransformers/server/requirements.txt (line 3)) (2.11.7)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (0.3.70)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (0.4.8)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (2.0.41)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from blessed>=1.20.0->-r ktransformers/server/requirements.txt (line 5)) (0.2.13)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r ktransformers/server/requirements.txt (line 8)) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r ktransformers/server/requirements.txt (line 8)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r ktransformers/server/requirements.txt (line 8)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r ktransformers/server/requirements.txt (line 8)) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai->-r ktransformers/server/requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build->-r ktransformers/server/requirements.txt (line 10)) (1.2.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->-r ktransformers/server/requirements.txt (line 14)) (3.1.0)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (from zmq->-r ktransformers/server/requirements.txt (line 15)) (24.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai->-r ktransformers/server/requirements.txt (line 8)) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->-r ktransformers/server/requirements.txt (line 8)) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->-r ktransformers/server/requirements.txt (line 8)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r ktransformers/server/requirements.txt (line 8)) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (1.1.5)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->-r ktransformers/server/requirements.txt (line 3)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->-r ktransformers/server/requirements.txt (line 3)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->-r ktransformers/server/requirements.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (3.2.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (3.0.0)\n",
            "Downloading blessed-1.21.0-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: zmq\n",
            "  Building wheel for zmq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zmq: filename=zmq-0.0.0-py3-none-any.whl size=1265 sha256=ea87b284597405b4fc20d6011d4567ddb77bd93599e9dd825940ce4f867b3fe4\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/9e/c7/d497825227491fa00cca08b88ae37f9bcc14809233db76342c\n",
            "Successfully built zmq\n",
            "Installing collected packages: zmq, ninja, colorlog, blessed\n",
            "Successfully installed blessed-1.21.0 colorlog-6.9.0 ninja-1.11.1.4 zmq-0.0.0\n",
            "Installing ktransformers\n",
            "Using pip 24.1.2 from /usr/local/lib/python3.11/dist-packages/pip (python 3.11)\n",
            "Processing /content/ktransformers\n",
            "  Running command Preparing metadata (pyproject.toml)\n",
            "  /usr/local/lib/python3.11/dist-packages/setuptools/config/_apply_pyprojecttoml.py:78: SetuptoolsWarning: `install_requires` overwritten in `pyproject.toml` (dependencies)\n",
            "    corresp(dist, value, root_dir)\n",
            "  cpuinfer_ext /content/ktransformers/csrc/ktransformers_ext\n",
            "  Using native cpu instruct\n",
            "  running dist_info\n",
            "  creating /tmp/pip-modern-metadata-ixfpouxd/ktransformers.egg-info\n",
            "  writing /tmp/pip-modern-metadata-ixfpouxd/ktransformers.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-modern-metadata-ixfpouxd/ktransformers.egg-info/dependency_links.txt\n",
            "  writing entry points to /tmp/pip-modern-metadata-ixfpouxd/ktransformers.egg-info/entry_points.txt\n",
            "  writing requirements to /tmp/pip-modern-metadata-ixfpouxd/ktransformers.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-modern-metadata-ixfpouxd/ktransformers.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-modern-metadata-ixfpouxd/ktransformers.egg-info/SOURCES.txt'\n",
            "  reading manifest file '/tmp/pip-modern-metadata-ixfpouxd/ktransformers.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  warning: no directories found matching 'local_chat.py'\n",
            "  no previously-included directories found matching 'ktransformers/logs'\n",
            "  no previously-included directories found matching 'ktransformers.egg-info'\n",
            "  warning: no directories found matching 'ktransformers/website/dist'\n",
            "  warning: no previously-included files matching '__pycache__' found anywhere in distribution\n",
            "  warning: no files found matching 'KTransformersOps.*.so'\n",
            "  warning: no files found matching 'cpuinfer_ext.*.so'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file '/tmp/pip-modern-metadata-ixfpouxd/ktransformers.egg-info/SOURCES.txt'\n",
            "  creating '/tmp/pip-modern-metadata-ixfpouxd/ktransformers-0.3.2+cu125torch26fancy.dist-info'\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.2+cu125torch26fancy) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers==4.51.3 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.2+cu125torch26fancy) (4.51.3)\n",
            "Requirement already satisfied: fastapi>=0.111.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.2+cu125torch26fancy) (0.116.1)\n",
            "Requirement already satisfied: uvicorn>=0.30.1 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.2+cu125torch26fancy) (0.35.0)\n",
            "Requirement already satisfied: langchain>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.2+cu125torch26fancy) (0.3.26)\n",
            "Requirement already satisfied: blessed>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.2+cu125torch26fancy) (1.21.0)\n",
            "Requirement already satisfied: accelerate>=0.31.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.2+cu125torch26fancy) (1.9.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.97 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.2+cu125torch26fancy) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.2+cu125torch26fancy) (75.2.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.2+cu125torch26fancy) (1.11.1.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.2+cu125torch26fancy) (0.45.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.2+cu125torch26fancy) (6.9.0)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.2+cu125torch26fancy) (1.2.2.post1)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.2+cu125torch26fancy) (0.7.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.2+cu125torch26fancy) (5.29.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.2+cu125torch26fancy) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.2+cu125torch26fancy) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.2+cu125torch26fancy) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.2+cu125torch26fancy) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.2+cu125torch26fancy) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.2+cu125torch26fancy) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.2+cu125torch26fancy) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.2+cu125torch26fancy) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.2+cu125torch26fancy) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.2+cu125torch26fancy) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.31.0->ktransformers==0.3.2+cu125torch26fancy) (5.9.5)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from blessed>=1.20.0->ktransformers==0.3.2+cu125torch26fancy) (0.2.13)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->ktransformers==0.3.2+cu125torch26fancy) (0.47.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->ktransformers==0.3.2+cu125torch26fancy) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->ktransformers==0.3.2+cu125torch26fancy) (4.14.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers==0.3.2+cu125torch26fancy) (0.3.70)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers==0.3.2+cu125torch26fancy) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers==0.3.2+cu125torch26fancy) (0.4.8)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers==0.3.2+cu125torch26fancy) (2.0.41)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.2+cu125torch26fancy) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.2+cu125torch26fancy) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.2+cu125torch26fancy) (2025.7.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.2+cu125torch26fancy) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.2+cu125torch26fancy) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.2+cu125torch26fancy) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.2+cu125torch26fancy) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.2+cu125torch26fancy) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.2+cu125torch26fancy) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.2+cu125torch26fancy) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.2+cu125torch26fancy) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.2+cu125torch26fancy) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.2+cu125torch26fancy) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.2+cu125torch26fancy) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.2+cu125torch26fancy) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.2+cu125torch26fancy) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.2+cu125torch26fancy) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.2+cu125torch26fancy) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->ktransformers==0.3.2+cu125torch26fancy) (1.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.30.1->ktransformers==0.3.2+cu125torch26fancy) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.30.1->ktransformers==0.3.2+cu125torch26fancy) (0.16.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build->ktransformers==0.3.2+cu125torch26fancy) (1.2.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->ktransformers==0.3.2+cu125torch26fancy) (3.1.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3->ktransformers==0.3.2+cu125torch26fancy) (1.1.5)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain>=0.2.0->ktransformers==0.3.2+cu125torch26fancy) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain>=0.2.0->ktransformers==0.3.2+cu125torch26fancy) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain>=0.2.0->ktransformers==0.3.2+cu125torch26fancy) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain>=0.2.0->ktransformers==0.3.2+cu125torch26fancy) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain>=0.2.0->ktransformers==0.3.2+cu125torch26fancy) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain>=0.2.0->ktransformers==0.3.2+cu125torch26fancy) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->ktransformers==0.3.2+cu125torch26fancy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->ktransformers==0.3.2+cu125torch26fancy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->ktransformers==0.3.2+cu125torch26fancy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->ktransformers==0.3.2+cu125torch26fancy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->ktransformers==0.3.2+cu125torch26fancy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->ktransformers==0.3.2+cu125torch26fancy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->ktransformers==0.3.2+cu125torch26fancy) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.2.0->ktransformers==0.3.2+cu125torch26fancy) (3.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi>=0.111.0->ktransformers==0.3.2+cu125torch26fancy) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->ktransformers==0.3.2+cu125torch26fancy) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi>=0.111.0->ktransformers==0.3.2+cu125torch26fancy) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain>=0.2.0->ktransformers==0.3.2+cu125torch26fancy) (1.0.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain>=0.2.0->ktransformers==0.3.2+cu125torch26fancy) (3.0.0)\n",
            "Building wheels for collected packages: ktransformers\n",
            "  Running command Building wheel for ktransformers (pyproject.toml)\n",
            "  /usr/local/lib/python3.11/dist-packages/setuptools/config/_apply_pyprojecttoml.py:78: SetuptoolsWarning: `install_requires` overwritten in `pyproject.toml` (dependencies)\n",
            "    corresp(dist, value, root_dir)\n",
            "  cpuinfer_ext /content/ktransformers/csrc/ktransformers_ext\n",
            "  Using native cpu instruct\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers\n",
            "  copying ktransformers/local_chat_test.py -> build/lib.linux-x86_64-cpython-311/ktransformers\n",
            "  copying ktransformers/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers\n",
            "  copying ktransformers/local_chat.py -> build/lib.linux-x86_64-cpython-311/ktransformers\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/textstream.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/modeling_rope_utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/weight_loader.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/vendors.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/custom_gguf.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/cuda_graph_runner.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/custom_loader.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/optimize\n",
            "  copying ktransformers/optimize/optimize.py -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/mmlu_test.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/dequant_gpu.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/test_client.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/test_prefix.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/test_pytorch_q8.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/triton_fp8gemm_test.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/dequant_gpu_t.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/score.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/mmlu_test_multi.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/test_speed.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/mmlu_pro_test.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/function_call_test.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/custom_cache.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/modeling_deepseek.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/modeling_deepseek_v3.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/custom_modeling_deepseek_v3.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/configuration_qwen3_moe.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/configuration_deepseek_v3.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/modeling_llama.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/configuration_llama.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/modeling_mixtral.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/configuration_deepseek.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/modeling_qwen3_moe.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/custom_modeling_qwen3_moe.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/modeling_qwen2_moe.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/custom_modeling_qwen2_moe.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/custom_modeling_deepseek_v2.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/configuration_qwen2_moe.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/triton_attention.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/layernorm.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/models.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/attention.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/linear.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/gate.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/RoPE.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/flashinfer_batch_prefill_wrapper.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/balance_serve_attention.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/triton_attention_prefill.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/cpuinfer.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/experts.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/base_operator.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/dynamic_attention.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/flashinfer_wrapper.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/mlp.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server\n",
            "  copying ktransformers/server/main.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server\n",
            "  copying ktransformers/server/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server\n",
            "  copying ktransformers/server/exceptions.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server\n",
            "  copying ktransformers/server/args.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/tests/AIME_2024\n",
            "  copying ktransformers/tests/AIME_2024/evaluation.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests/AIME_2024\n",
            "  copying ktransformers/tests/AIME_2024/eval_api.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests/AIME_2024\n",
            "  copying ktransformers/tests/AIME_2024/prompts.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests/AIME_2024\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/tests/humaneval\n",
            "  copying ktransformers/tests/humaneval/evaluation.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests/humaneval\n",
            "  copying ktransformers/tests/humaneval/eval_api.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests/humaneval\n",
            "  copying ktransformers/tests/humaneval/prompts.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests/humaneval\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/triton\n",
            "  copying ktransformers/ktransformers_ext/triton/fp8gemm.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/triton\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/quant_utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/marlin_24_perms.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/marlin_perms.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/marlin_utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/format_24.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas\n",
            "  copying ktransformers/server/schemas/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas\n",
            "  copying ktransformers/server/schemas/base.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas\n",
            "  copying ktransformers/server/schemas/conversation.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/backend\n",
            "  copying ktransformers/server/backend/context_manager.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend\n",
            "  copying ktransformers/server/backend/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend\n",
            "  copying ktransformers/server/backend/base.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend\n",
            "  copying ktransformers/server/backend/args.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/config\n",
            "  copying ktransformers/server/config/config.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/config\n",
            "  copying ktransformers/server/config/singleton.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/config\n",
            "  copying ktransformers/server/config/log.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/config\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/crud\n",
            "  copying ktransformers/server/crud/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/crud\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api\n",
            "  copying ktransformers/server/api/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve\n",
            "  copying ktransformers/server/balance_serve/sched_rpc.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve\n",
            "  copying ktransformers/server/balance_serve/settings.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/utils\n",
            "  copying ktransformers/server/utils/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/utils\n",
            "  copying ktransformers/server/utils/sql_utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/utils\n",
            "  copying ktransformers/server/utils/multi_timer.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/utils\n",
            "  copying ktransformers/server/utils/create_interface.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/utils\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/models\n",
            "  copying ktransformers/server/models/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/assistants.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/runs.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/messages.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/tool.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/threads.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/streaming.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/legacy\n",
            "  copying ktransformers/server/schemas/legacy/completions.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/legacy\n",
            "  copying ktransformers/server/schemas/legacy/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/legacy\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/endpoints\n",
            "  copying ktransformers/server/schemas/endpoints/chat.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/endpoints\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces\n",
            "  copying ktransformers/server/backend/interfaces/transformers.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces\n",
            "  copying ktransformers/server/backend/interfaces/exllamav2.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces\n",
            "  copying ktransformers/server/backend/interfaces/balance_serve.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces\n",
            "  copying ktransformers/server/backend/interfaces/ktransformers.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces\n",
            "  copying ktransformers/server/backend/interfaces/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants\n",
            "  copying ktransformers/server/crud/assistants/assistants.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants\n",
            "  copying ktransformers/server/crud/assistants/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants\n",
            "  copying ktransformers/server/crud/assistants/runs.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants\n",
            "  copying ktransformers/server/crud/assistants/messages.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants\n",
            "  copying ktransformers/server/crud/assistants/threads.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai\n",
            "  copying ktransformers/server/api/openai/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api/ollama\n",
            "  copying ktransformers/server/api/ollama/completions.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/ollama\n",
            "  copying ktransformers/server/api/ollama/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/ollama\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api/web\n",
            "  copying ktransformers/server/api/web/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/web\n",
            "  copying ktransformers/server/api/web/system.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/web\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants\n",
            "  copying ktransformers/server/api/openai/assistants/assistants.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants\n",
            "  copying ktransformers/server/api/openai/assistants/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants\n",
            "  copying ktransformers/server/api/openai/assistants/runs.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants\n",
            "  copying ktransformers/server/api/openai/assistants/messages.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants\n",
            "  copying ktransformers/server/api/openai/assistants/threads.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/legacy\n",
            "  copying ktransformers/server/api/openai/legacy/completions.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/legacy\n",
            "  copying ktransformers/server/api/openai/legacy/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/legacy\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/endpoints\n",
            "  copying ktransformers/server/api/openai/endpoints/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/endpoints\n",
            "  copying ktransformers/server/api/openai/endpoints/chat.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/endpoints\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference\n",
            "  copying ktransformers/server/balance_serve/inference/config.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference\n",
            "  copying ktransformers/server/balance_serve/inference/query_manager.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference\n",
            "  copying ktransformers/server/balance_serve/inference/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference\n",
            "  copying ktransformers/server/balance_serve/inference/model_runner.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference\n",
            "  copying ktransformers/server/balance_serve/inference/forward_batch.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/sampler.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/communication_op.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/custom_all_reduce.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/pynccl_wrapper.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/parallel_state.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/pynccl.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/custom_all_reduce_utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/cuda_wrapper.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/penaltylib/orchestrator.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/penaltylib/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/presence_penalty.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/frequency_penalty.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/repetition_penalty.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/min_new_tokens.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  copying ktransformers/server/models/assistants/assistants.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  copying ktransformers/server/models/assistants/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  copying ktransformers/server/models/assistants/runs.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  copying ktransformers/server/models/assistants/messages.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  copying ktransformers/server/models/assistants/threads.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  copying ktransformers/server/models/assistants/run_steps.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  running egg_info\n",
            "  creating ktransformers.egg-info\n",
            "  writing ktransformers.egg-info/PKG-INFO\n",
            "  writing dependency_links to ktransformers.egg-info/dependency_links.txt\n",
            "  writing entry points to ktransformers.egg-info/entry_points.txt\n",
            "  writing requirements to ktransformers.egg-info/requires.txt\n",
            "  writing top-level names to ktransformers.egg-info/top_level.txt\n",
            "  writing manifest file 'ktransformers.egg-info/SOURCES.txt'\n",
            "  reading manifest file 'ktransformers.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  warning: no directories found matching 'local_chat.py'\n",
            "  no previously-included directories found matching 'ktransformers/logs'\n",
            "  warning: no directories found matching 'ktransformers/website/dist'\n",
            "  warning: no previously-included files matching '__pycache__' found anywhere in distribution\n",
            "  warning: no files found matching 'KTransformersOps.*.so'\n",
            "  warning: no files found matching 'cpuinfer_ext.*.so'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'ktransformers.egg-info/SOURCES.txt'\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/configs\n",
            "  copying ktransformers/configs/config.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/configs\n",
            "  copying ktransformers/configs/log_config.ini -> build/lib.linux-x86_64-cpython-311/ktransformers/configs\n",
            "  copying ktransformers/tests/.gitignore -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/server/requirements.txt -> build/lib.linux-x86_64-cpython-311/ktransformers/server\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat-multi-gpu-4.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat-multi-gpu.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat-gpu-cpu.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat-multi-gpu.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-amx.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-fp8-linear-ggml-experts-serve-amx.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-fp8-linear-ggml-experts-serve.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-fp8-linear-ggml-experts.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-4.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-8.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-fp8-linear-ggml-experts.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-marlin.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-serve.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Internlm2_5-7b-Chat-1m.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Mixtral.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Moonlight-16B-A3B-serve.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Moonlight-16B-A3B.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Qwen2-57B-A14B-Instruct-multi-gpu.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Qwen2-57B-A14B-Instruct.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Qwen2-serve-amx.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Qwen2-serve.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Qwen3Moe-serve-amx.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Qwen3Moe-serve.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/xpu\n",
            "  copying ktransformers/optimize/optimize_rules/xpu/DeepSeek-V2-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/xpu\n",
            "  copying ktransformers/optimize/optimize_rules/xpu/DeepSeek-V3-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/xpu\n",
            "  copying ktransformers/optimize/optimize_rules/xpu/Qwen3Moe-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/xpu\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/rocm\n",
            "  copying ktransformers/optimize/optimize_rules/rocm/DeepSeek-V3-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/rocm\n",
            "  running build_ext\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:448: UserWarning: The detected CUDA version (12.5) has a minor version mismatch with the version that was used to compile PyTorch (12.4). Most likely this shouldn't be a problem.\n",
            "    warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:458: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.5\n",
            "    warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "  -- The C compiler identification is GNU 11.4.0\n",
            "  -- The CXX compiler identification is GNU 11.4.0\n",
            "  -- Detecting C compiler ABI info\n",
            "  -- Detecting C compiler ABI info - done\n",
            "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
            "  -- Detecting C compile features\n",
            "  -- Detecting C compile features - done\n",
            "  -- Detecting CXX compiler ABI info\n",
            "  -- Detecting CXX compiler ABI info - done\n",
            "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "  -- Detecting CXX compile features\n",
            "  -- Detecting CXX compile features - done\n",
            "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "  -- x86 detected\n",
            "  -- Compiler and CPU support AVX512F (tested by compiling a program)\n",
            "  -- Compiler does NOT support AMX\n",
            "  CMake Deprecation Warning at /content/ktransformers/third_party/pybind11/CMakeLists.txt:13 (cmake_minimum_required):\n",
            "    Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "    CMake.\n",
            "\n",
            "    Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "    to tell CMake that the project requires at least <min> but has been updated\n",
            "    to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\n",
            "  -- pybind11 v2.14.0 dev1\n",
            "  -- Found PythonInterp: /usr/bin/python3 (found suitable version \"3.11.13\", minimum required is \"3.7\")\n",
            "  -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.11.so\n",
            "  -- Performing Test HAS_FLTO\n",
            "  -- Performing Test HAS_FLTO - Success\n",
            "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "  -- Found Threads: TRUE\n",
            "  -- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "  -- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "  -- Found OpenMP: TRUE (found version \"4.5\")\n",
            "  -- OpenMP found\n",
            "  -- Warning: ccache not found - consider installing it for faster compilation or disable this warning with LLAMA_CCACHE=OFF\n",
            "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "  -- x86 detected\n",
            "  CMake Warning (dev) at CMakeLists.txt:310 (find_package):\n",
            "    Policy CMP0146 is not set: The FindCUDA module is removed.  Run \"cmake\n",
            "    --help-policy CMP0146\" for policy details.  Use the cmake_policy command to\n",
            "    set the policy and suppress this warning.\n",
            "\n",
            "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\n",
            "  -- Found CUDA: /usr/local/cuda (found version \"12.5\")\n",
            "  -- Looking for a CUDA compiler\n",
            "  -- Looking for a CUDA compiler - /usr/local/cuda/bin/nvcc\n",
            "  -- CUDA detected\n",
            "  -- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.5.82\")\n",
            "  -- enabling CUDA\n",
            "  -- The CUDA compiler identification is NVIDIA 12.5.82 with host compiler GNU 11.4.0\n",
            "  -- Detecting CUDA compiler ABI info\n",
            "  -- Detecting CUDA compiler ABI info - done\n",
            "  -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "  -- Detecting CUDA compile features\n",
            "  -- Detecting CUDA compile features - done\n",
            "  -- CMAKE_CXX_FLAGS:  -O3 -ffast-math -fopenmp\n",
            "  -- ARCH_FLAGS: -mfma;-mavx;-mavx2;-march=native\n",
            "  -- NUMA support is disabled\n",
            "  -- NUMA library not found or user not set USE_NUMA - disabling NUMA support\n",
            "  -- Configuring done (7.4s)\n",
            "  -- Generating done (0.0s)\n",
            "  CMake Warning:\n",
            "    Manually-specified variables were not used by the project:\n",
            "\n",
            "      EXAMPLE_VERSION_INFO\n",
            "\n",
            "\n",
            "  -- Build files have been written to: /content/ktransformers/csrc/ktransformers_ext/build\n",
            "  Change Dir: '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "\n",
            "  Run Build Command(s): /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E env VERBOSE=1 /usr/bin/gmake -f Makefile -j2\n",
            "  /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -S/content/ktransformers/csrc/ktransformers_ext -B/content/ktransformers/csrc/ktransformers_ext/build --check-build-system CMakeFiles/Makefile.cmake 0\n",
            "  /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_progress_start /content/ktransformers/csrc/ktransformers_ext/build/CMakeFiles /content/ktransformers/csrc/ktransformers_ext/build//CMakeFiles/progress.marks\n",
            "  /usr/bin/gmake  -f CMakeFiles/Makefile2 all\n",
            "  gmake[1]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  /usr/bin/gmake  -f CMakeFiles/llamafile.dir/build.make CMakeFiles/llamafile.dir/depend\n",
            "  /usr/bin/gmake  -f third_party/llama.cpp/CMakeFiles/ggml.dir/build.make third_party/llama.cpp/CMakeFiles/ggml.dir/depend\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /content/ktransformers/csrc/ktransformers_ext /content/ktransformers/csrc/ktransformers_ext /content/ktransformers/csrc/ktransformers_ext/build /content/ktransformers/csrc/ktransformers_ext/build /content/ktransformers/csrc/ktransformers_ext/build/CMakeFiles/llamafile.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /content/ktransformers/csrc/ktransformers_ext /content/ktransformers/third_party/llama.cpp /content/ktransformers/csrc/ktransformers_ext/build /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/CMakeFiles/ggml.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  /usr/bin/gmake  -f CMakeFiles/llamafile.dir/build.make CMakeFiles/llamafile.dir/build\n",
            "  /usr/bin/gmake  -f third_party/llama.cpp/CMakeFiles/ggml.dir/build.make third_party/llama.cpp/CMakeFiles/ggml.dir/build\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  [  1%] Building C object third_party/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o\n",
            "  [  2%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/flags.cpp.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp && /usr/bin/cc -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/. -O3 -DNDEBUG -std=gnu11 -fPIC -mfma -mavx -mavx2 -march=native -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -fopenmp -MD -MT third_party/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -MF CMakeFiles/ggml.dir/ggml.c.o.d -o CMakeFiles/ggml.dir/ggml.c.o -c /content/ktransformers/third_party/llama.cpp/ggml.c\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/flags.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/flags.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/flags.cpp.o -c /content/ktransformers/third_party/llamafile/flags.cpp\n",
            "  [  3%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_avx2.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_avx2.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_avx2.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_avx2.cpp.o -c /content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_avx2.cpp\n",
            "  [  5%] Building C object third_party/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp && /usr/bin/cc -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/. -O3 -DNDEBUG -std=gnu11 -fPIC -mfma -mavx -mavx2 -march=native -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -fopenmp -MD -MT third_party/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -MF CMakeFiles/ggml.dir/ggml-alloc.c.o.d -o CMakeFiles/ggml.dir/ggml-alloc.c.o -c /content/ktransformers/third_party/llama.cpp/ggml-alloc.c\n",
            "  [  6%] Building C object third_party/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp && /usr/bin/cc -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/. -O3 -DNDEBUG -std=gnu11 -fPIC -mfma -mavx -mavx2 -march=native -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -fopenmp -MD -MT third_party/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -MF CMakeFiles/ggml.dir/ggml-backend.c.o.d -o CMakeFiles/ggml.dir/ggml-backend.c.o -c /content/ktransformers/third_party/llama.cpp/ggml-backend.c\n",
            "  [  7%] Building C object third_party/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp && /usr/bin/cc -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/. -O3 -DNDEBUG -std=gnu11 -fPIC -mfma -mavx -mavx2 -march=native -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -fopenmp -MD -MT third_party/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -MF CMakeFiles/ggml.dir/ggml-quants.c.o.d -o CMakeFiles/ggml.dir/ggml-quants.c.o -c /content/ktransformers/third_party/llama.cpp/ggml-quants.c\n",
            "  [  9%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_zen4.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_zen4.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_zen4.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_zen4.cpp.o -c /content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_zen4.cpp\n",
            "  [ 10%] Building CXX object third_party/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp && /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/. -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++11 -fPIC -mfma -mavx -mavx2 -march=native -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -fopenmp -MD -MT third_party/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o -MF CMakeFiles/ggml.dir/sgemm.cpp.o.d -o CMakeFiles/ggml.dir/sgemm.cpp.o -c /content/ktransformers/third_party/llama.cpp/sgemm.cpp\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  [ 10%] Built target ggml\n",
            "  /usr/bin/gmake  -f third_party/llama.cpp/common/CMakeFiles/build_info.dir/build.make third_party/llama.cpp/common/CMakeFiles/build_info.dir/depend\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  [ 11%] Generating build details from Git\n",
            "  cd /content/ktransformers/third_party/llama.cpp && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -DMSVC= -DCMAKE_C_COMPILER_VERSION=11.4.0 -DCMAKE_C_COMPILER_ID=GNU -DCMAKE_VS_PLATFORM_NAME= -DCMAKE_C_COMPILER=/usr/bin/cc -P /content/ktransformers/third_party/llama.cpp/common/../scripts/gen-build-info-cpp.cmake\n",
            "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /content/ktransformers/csrc/ktransformers_ext /content/ktransformers/third_party/llama.cpp/common /content/ktransformers/csrc/ktransformers_ext/build /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common/CMakeFiles/build_info.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  /usr/bin/gmake  -f third_party/llama.cpp/common/CMakeFiles/build_info.dir/build.make third_party/llama.cpp/common/CMakeFiles/build_info.dir/build\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  [ 12%] Building CXX object third_party/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common && /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__  -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++11 -fPIC -mfma -mavx -mavx2 -march=native -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -MD -MT third_party/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -MF CMakeFiles/build_info.dir/build-info.cpp.o.d -o CMakeFiles/build_info.dir/build-info.cpp.o -c /content/ktransformers/third_party/llama.cpp/common/build-info.cpp\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  [ 12%] Built target build_info\n",
            "  /usr/bin/gmake  -f third_party/llama.cpp/CMakeFiles/llama.dir/build.make third_party/llama.cpp/CMakeFiles/llama.dir/depend\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /content/ktransformers/csrc/ktransformers_ext /content/ktransformers/third_party/llama.cpp /content/ktransformers/csrc/ktransformers_ext/build /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/CMakeFiles/llama.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  /usr/bin/gmake  -f third_party/llama.cpp/CMakeFiles/llama.dir/build.make third_party/llama.cpp/CMakeFiles/llama.dir/build\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  [ 14%] Building CXX object third_party/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp && /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/. -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++11 -fPIC -mfma -mavx -mavx2 -march=native -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -fopenmp -MD -MT third_party/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -MF CMakeFiles/llama.dir/llama.cpp.o.d -o CMakeFiles/llama.dir/llama.cpp.o -c /content/ktransformers/third_party/llama.cpp/llama.cpp\n",
            "  [ 15%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_arm82.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_arm82.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_arm82.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_arm82.cpp.o -c /content/ktransformers/third_party/llamafile/iqk_mul_mat_arm82.cpp\n",
            "  [ 16%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/sgemm.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/sgemm.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/sgemm.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/sgemm.cpp.o -c /content/ktransformers/third_party/llamafile/sgemm.cpp\n",
            "  [ 18%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx.cpp\n",
            "  [ 19%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx2.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx2.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx2.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx2.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx2.cpp\n",
            "  [ 20%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx512f.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx512f.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx512f.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx512f.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx512f.cpp\n",
            "  [ 22%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avxvnni.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avxvnni.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avxvnni.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avxvnni.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avxvnni.cpp\n",
            "  [ 23%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_fma.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_fma.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_fma.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_fma.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_fma.cpp\n",
            "  [ 24%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_zen4.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_zen4.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_zen4.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_zen4.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_zen4.cpp\n",
            "  [ 25%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm80.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm80.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm80.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm80.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm80.cpp\n",
            "  [ 27%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm82.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm82.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm82.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm82.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm82.cpp\n",
            "  [ 28%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx.cpp\n",
            "  [ 29%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx2.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx2.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx2.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx2.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx2.cpp\n",
            "  [ 31%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx512f.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx512f.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx512f.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx512f.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx512f.cpp\n",
            "  [ 32%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avxvnni.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avxvnni.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avxvnni.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avxvnni.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avxvnni.cpp\n",
            "  [ 33%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_fma.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_fma.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_fma.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_fma.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_fma.cpp\n",
            "  [ 35%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_zen4.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_zen4.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_zen4.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_zen4.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_zen4.cpp\n",
            "  [ 36%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm80.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm80.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm80.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm80.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm80.cpp\n",
            "  [ 37%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm82.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm82.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm82.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm82.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm82.cpp\n",
            "  [ 38%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_unsupported.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_unsupported.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_unsupported.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_unsupported.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_unsupported.cpp\n",
            "  [ 40%] Linking CXX static library libllamafile.a\n",
            "  /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -P CMakeFiles/llamafile.dir/cmake_clean_target.cmake\n",
            "  /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/llamafile.dir/link.txt --verbose=1\n",
            "  /usr/bin/ar qc libllamafile.a CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/flags.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_avx2.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_zen4.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_arm82.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/sgemm.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx2.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx512f.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avxvnni.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_fma.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_zen4.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm80.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm82.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx2.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx512f.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avxvnni.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_fma.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_zen4.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm80.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm82.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_unsupported.cpp.o\n",
            "  /usr/bin/ranlib libllamafile.a\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  [ 40%] Built target llamafile\n",
            "  [ 41%] Building CXX object third_party/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp && /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/. -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++11 -fPIC -mfma -mavx -mavx2 -march=native -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -fopenmp -MD -MT third_party/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o -MF CMakeFiles/llama.dir/unicode.cpp.o.d -o CMakeFiles/llama.dir/unicode.cpp.o -c /content/ktransformers/third_party/llama.cpp/unicode.cpp\n",
            "  [ 42%] Building CXX object third_party/llama.cpp/CMakeFiles/llama.dir/unicode-data.cpp.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp && /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/. -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++11 -fPIC -mfma -mavx -mavx2 -march=native -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -fopenmp -MD -MT third_party/llama.cpp/CMakeFiles/llama.dir/unicode-data.cpp.o -MF CMakeFiles/llama.dir/unicode-data.cpp.o.d -o CMakeFiles/llama.dir/unicode-data.cpp.o -c /content/ktransformers/third_party/llama.cpp/unicode-data.cpp\n",
            "  /usr/bin/gmake  -f third_party/llama.cpp/CMakeFiles/ggml_static.dir/build.make third_party/llama.cpp/CMakeFiles/ggml_static.dir/depend\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /content/ktransformers/csrc/ktransformers_ext /content/ktransformers/third_party/llama.cpp /content/ktransformers/csrc/ktransformers_ext/build /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/CMakeFiles/ggml_static.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  /usr/bin/gmake  -f third_party/llama.cpp/CMakeFiles/ggml_static.dir/build.make third_party/llama.cpp/CMakeFiles/ggml_static.dir/build\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  [ 44%] Linking CXX static library libggml_static.a\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -P CMakeFiles/ggml_static.dir/cmake_clean_target.cmake\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/ggml_static.dir/link.txt --verbose=1\n",
            "  /usr/bin/ar qc libggml_static.a CMakeFiles/ggml.dir/ggml.c.o \"CMakeFiles/ggml.dir/ggml-alloc.c.o\" \"CMakeFiles/ggml.dir/ggml-backend.c.o\" \"CMakeFiles/ggml.dir/ggml-quants.c.o\" CMakeFiles/ggml.dir/sgemm.cpp.o\n",
            "  /usr/bin/ranlib libggml_static.a\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  [ 44%] Built target ggml_static\n",
            "  [ 45%] Linking CXX static library libllama.a\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -P CMakeFiles/llama.dir/cmake_clean_target.cmake\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/llama.dir/link.txt --verbose=1\n",
            "  /usr/bin/ar qc libllama.a CMakeFiles/llama.dir/llama.cpp.o CMakeFiles/llama.dir/unicode.cpp.o \"CMakeFiles/llama.dir/unicode-data.cpp.o\" CMakeFiles/ggml.dir/ggml.c.o \"CMakeFiles/ggml.dir/ggml-alloc.c.o\" \"CMakeFiles/ggml.dir/ggml-backend.c.o\" \"CMakeFiles/ggml.dir/ggml-quants.c.o\" CMakeFiles/ggml.dir/sgemm.cpp.o\n",
            "  /usr/bin/ranlib libllama.a\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  [ 45%] Built target llama\n",
            "  /usr/bin/gmake  -f CMakeFiles/cpuinfer_ext.dir/build.make CMakeFiles/cpuinfer_ext.dir/depend\n",
            "  /usr/bin/gmake  -f third_party/llama.cpp/common/CMakeFiles/common.dir/build.make third_party/llama.cpp/common/CMakeFiles/common.dir/depend\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /content/ktransformers/csrc/ktransformers_ext /content/ktransformers/csrc/ktransformers_ext /content/ktransformers/csrc/ktransformers_ext/build /content/ktransformers/csrc/ktransformers_ext/build /content/ktransformers/csrc/ktransformers_ext/build/CMakeFiles/cpuinfer_ext.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /content/ktransformers/csrc/ktransformers_ext /content/ktransformers/third_party/llama.cpp/common /content/ktransformers/csrc/ktransformers_ext/build /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common/CMakeFiles/common.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  /usr/bin/gmake  -f CMakeFiles/cpuinfer_ext.dir/build.make CMakeFiles/cpuinfer_ext.dir/build\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  /usr/bin/gmake  -f third_party/llama.cpp/common/CMakeFiles/common.dir/build.make third_party/llama.cpp/common/CMakeFiles/common.dir/build\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  [ 46%] Building CXX object CMakeFiles/cpuinfer_ext.dir/ext_bindings.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/ext_bindings.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/ext_bindings.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/ext_bindings.cpp.o -c /content/ktransformers/csrc/ktransformers_ext/ext_bindings.cpp\n",
            "  [ 48%] Building CXX object third_party/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common && /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/common/. -I/content/ktransformers/third_party/llama.cpp/. -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++11 -fPIC -mfma -mavx -mavx2 -march=native -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -MD -MT third_party/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -MF CMakeFiles/common.dir/common.cpp.o.d -o CMakeFiles/common.dir/common.cpp.o -c /content/ktransformers/third_party/llama.cpp/common/common.cpp\n",
            "  [ 49%] Building CXX object CMakeFiles/cpuinfer_ext.dir/cpu_backend/backend.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/cpu_backend/backend.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/cpu_backend/backend.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/cpu_backend/backend.cpp.o -c /content/ktransformers/csrc/ktransformers_ext/cpu_backend/backend.cpp\n",
            "  [ 50%] Building CXX object CMakeFiles/cpuinfer_ext.dir/cpu_backend/shared_mem_buffer.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/cpu_backend/shared_mem_buffer.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/cpu_backend/shared_mem_buffer.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/cpu_backend/shared_mem_buffer.cpp.o -c /content/ktransformers/csrc/ktransformers_ext/cpu_backend/shared_mem_buffer.cpp\n",
            "  [ 51%] Building CXX object CMakeFiles/cpuinfer_ext.dir/cpu_backend/task_queue.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/cpu_backend/task_queue.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/cpu_backend/task_queue.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/cpu_backend/task_queue.cpp.o -c /content/ktransformers/csrc/ktransformers_ext/cpu_backend/task_queue.cpp\n",
            "  [ 53%] Building CXX object CMakeFiles/cpuinfer_ext.dir/operators/llamafile/linear.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/operators/llamafile/linear.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/operators/llamafile/linear.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/operators/llamafile/linear.cpp.o -c /content/ktransformers/csrc/ktransformers_ext/operators/llamafile/linear.cpp\n",
            "  [ 54%] Building CXX object CMakeFiles/cpuinfer_ext.dir/operators/llamafile/mlp.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/operators/llamafile/mlp.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/operators/llamafile/mlp.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/operators/llamafile/mlp.cpp.o -c /content/ktransformers/csrc/ktransformers_ext/operators/llamafile/mlp.cpp\n",
            "  [ 55%] Building CXX object CMakeFiles/cpuinfer_ext.dir/operators/llamafile/moe.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/operators/llamafile/moe.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/operators/llamafile/moe.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/operators/llamafile/moe.cpp.o -c /content/ktransformers/csrc/ktransformers_ext/operators/llamafile/moe.cpp\n",
            "  [ 57%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/flags.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/flags.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/flags.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/flags.cpp.o -c /content/ktransformers/third_party/llamafile/flags.cpp\n",
            "  [ 58%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_avx2.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_avx2.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_avx2.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_avx2.cpp.o -c /content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_avx2.cpp\n",
            "  [ 59%] Building CXX object third_party/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common && /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/common/. -I/content/ktransformers/third_party/llama.cpp/. -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++11 -fPIC -mfma -mavx -mavx2 -march=native -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -MD -MT third_party/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -MF CMakeFiles/common.dir/sampling.cpp.o.d -o CMakeFiles/common.dir/sampling.cpp.o -c /content/ktransformers/third_party/llama.cpp/common/sampling.cpp\n",
            "  [ 61%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_zen4.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_zen4.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_zen4.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_zen4.cpp.o -c /content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_zen4.cpp\n",
            "  [ 62%] Building CXX object third_party/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common && /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/common/. -I/content/ktransformers/third_party/llama.cpp/. -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++11 -fPIC -mfma -mavx -mavx2 -march=native -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -MD -MT third_party/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -MF CMakeFiles/common.dir/console.cpp.o.d -o CMakeFiles/common.dir/console.cpp.o -c /content/ktransformers/third_party/llama.cpp/common/console.cpp\n",
            "  [ 63%] Building CXX object third_party/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common && /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/common/. -I/content/ktransformers/third_party/llama.cpp/. -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++11 -fPIC -mfma -mavx -mavx2 -march=native -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -MD -MT third_party/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -MF CMakeFiles/common.dir/grammar-parser.cpp.o.d -o CMakeFiles/common.dir/grammar-parser.cpp.o -c /content/ktransformers/third_party/llama.cpp/common/grammar-parser.cpp\n",
            "  [ 64%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_arm82.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_arm82.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_arm82.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_arm82.cpp.o -c /content/ktransformers/third_party/llamafile/iqk_mul_mat_arm82.cpp\n",
            "  [ 66%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/sgemm.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/sgemm.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/sgemm.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/sgemm.cpp.o -c /content/ktransformers/third_party/llamafile/sgemm.cpp\n",
            "  [ 67%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx.cpp\n",
            "  [ 68%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx2.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx2.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx2.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx2.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx2.cpp\n",
            "  [ 70%] Building CXX object third_party/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common && /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/common/. -I/content/ktransformers/third_party/llama.cpp/. -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++11 -fPIC -mfma -mavx -mavx2 -march=native -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -MD -MT third_party/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -MF CMakeFiles/common.dir/json-schema-to-grammar.cpp.o.d -o CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -c /content/ktransformers/third_party/llama.cpp/common/json-schema-to-grammar.cpp\n",
            "  [ 71%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx512f.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx512f.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx512f.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx512f.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx512f.cpp\n",
            "  [ 72%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avxvnni.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avxvnni.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avxvnni.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avxvnni.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avxvnni.cpp\n",
            "  [ 74%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_fma.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_fma.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_fma.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_fma.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_fma.cpp\n",
            "  [ 75%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_zen4.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_zen4.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_zen4.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_zen4.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_zen4.cpp\n",
            "  [ 76%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm80.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm80.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm80.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm80.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm80.cpp\n",
            "  [ 77%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm82.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm82.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm82.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm82.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm82.cpp\n",
            "  [ 79%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx.cpp\n",
            "  [ 80%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx2.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx2.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx2.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx2.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx2.cpp\n",
            "  [ 81%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx512f.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx512f.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx512f.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx512f.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx512f.cpp\n",
            "  [ 83%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avxvnni.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avxvnni.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avxvnni.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avxvnni.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avxvnni.cpp\n",
            "  [ 84%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_fma.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_fma.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_fma.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_fma.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_fma.cpp\n",
            "  [ 85%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_zen4.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_zen4.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_zen4.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_zen4.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_zen4.cpp\n",
            "  [ 87%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm80.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm80.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm80.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm80.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm80.cpp\n",
            "  [ 88%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm82.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm82.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm82.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm82.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm82.cpp\n",
            "  [ 89%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_unsupported.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_unsupported.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_unsupported.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_unsupported.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_unsupported.cpp\n",
            "  [ 90%] Building CXX object CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_attn.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_attn.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_attn.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_attn.cpp.o -c /content/ktransformers/csrc/ktransformers_ext/operators/kvcache/kvcache_attn.cpp\n",
            "  [ 92%] Building CXX object CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_load_dump.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_load_dump.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_load_dump.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_load_dump.cpp.o -c /content/ktransformers/csrc/ktransformers_ext/operators/kvcache/kvcache_load_dump.cpp\n",
            "  [ 93%] Building CXX object third_party/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common && /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/common/. -I/content/ktransformers/third_party/llama.cpp/. -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++11 -fPIC -mfma -mavx -mavx2 -march=native -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -MD -MT third_party/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -MF CMakeFiles/common.dir/train.cpp.o.d -o CMakeFiles/common.dir/train.cpp.o -c /content/ktransformers/third_party/llama.cpp/common/train.cpp\n",
            "  [ 94%] Building CXX object CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_read_write.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_read_write.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_read_write.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_read_write.cpp.o -c /content/ktransformers/csrc/ktransformers_ext/operators/kvcache/kvcache_read_write.cpp\n",
            "  [ 96%] Building CXX object third_party/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common && /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/common/. -I/content/ktransformers/third_party/llama.cpp/. -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++11 -fPIC -mfma -mavx -mavx2 -march=native -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -MD -MT third_party/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o -MF CMakeFiles/common.dir/ngram-cache.cpp.o.d -o CMakeFiles/common.dir/ngram-cache.cpp.o -c /content/ktransformers/third_party/llama.cpp/common/ngram-cache.cpp\n",
            "  [ 97%] Building CXX object CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_utils.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_utils.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_utils.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_utils.cpp.o -c /content/ktransformers/csrc/ktransformers_ext/operators/kvcache/kvcache_utils.cpp\n",
            "  [ 98%] Linking CXX static library libcommon.a\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -P CMakeFiles/common.dir/cmake_clean_target.cmake\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/common.dir/link.txt --verbose=1\n",
            "  /usr/bin/ar qc libcommon.a CMakeFiles/common.dir/common.cpp.o CMakeFiles/common.dir/sampling.cpp.o CMakeFiles/common.dir/console.cpp.o \"CMakeFiles/common.dir/grammar-parser.cpp.o\" \"CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\" CMakeFiles/common.dir/train.cpp.o \"CMakeFiles/common.dir/ngram-cache.cpp.o\" \"CMakeFiles/build_info.dir/build-info.cpp.o\"\n",
            "  /usr/bin/ranlib libcommon.a\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  [ 98%] Built target common\n",
            "  [100%] Linking CXX shared module /content/ktransformers/build/lib.linux-x86_64-cpython-311/cpuinfer_ext.cpython-311-x86_64-linux-gnu.so\n",
            "  /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/cpuinfer_ext.dir/link.txt --verbose=1\n",
            "  lto-wrapper: warning: using serial compilation of 14 LTRANS jobs\n",
            "  /usr/bin/c++ -fPIC  -O3 -ffast-math -fopenmp -O3 -DNDEBUG -flto -shared  -o /content/ktransformers/build/lib.linux-x86_64-cpython-311/cpuinfer_ext.cpython-311-x86_64-linux-gnu.so CMakeFiles/cpuinfer_ext.dir/ext_bindings.cpp.o CMakeFiles/cpuinfer_ext.dir/cpu_backend/backend.cpp.o CMakeFiles/cpuinfer_ext.dir/cpu_backend/shared_mem_buffer.cpp.o CMakeFiles/cpuinfer_ext.dir/cpu_backend/task_queue.cpp.o CMakeFiles/cpuinfer_ext.dir/operators/llamafile/linear.cpp.o CMakeFiles/cpuinfer_ext.dir/operators/llamafile/mlp.cpp.o CMakeFiles/cpuinfer_ext.dir/operators/llamafile/moe.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/flags.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_avx2.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_zen4.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_arm82.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/sgemm.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx2.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx512f.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avxvnni.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_fma.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_zen4.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm80.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm82.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx2.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx512f.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avxvnni.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_fma.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_zen4.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm80.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm82.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_unsupported.cpp.o CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_attn.cpp.o CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_load_dump.cpp.o CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_read_write.cpp.o CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_utils.cpp.o  -Wl,-rpath,/usr/local/cuda/lib64 third_party/llama.cpp/libllama.a /usr/local/cuda/lib64/libcudart.so /usr/lib/gcc/x86_64-linux-gnu/11/libgomp.so /usr/lib/x86_64-linux-gnu/libpthread.a\n",
            "  /usr/bin/strip /content/ktransformers/build/lib.linux-x86_64-cpython-311/cpuinfer_ext.cpython-311-x86_64-linux-gnu.so\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  [100%] Built target cpuinfer_ext\n",
            "  gmake[1]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_progress_start /content/ktransformers/csrc/ktransformers_ext/build/CMakeFiles 0\n",
            "\n",
            "  CMake args: ['-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=/content/ktransformers/build/lib.linux-x86_64-cpython-311/', '-DPYTHON_EXECUTABLE=/usr/bin/python3', '-DCMAKE_BUILD_TYPE=Release', '-DKTRANSFORMERS_USE_CUDA=ON', '-D_GLIBCXX_USE_CXX11_ABI=0']\n",
            "  CMake args: ['-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=/content/ktransformers/build/lib.linux-x86_64-cpython-311/', '-DPYTHON_EXECUTABLE=/usr/bin/python3', '-DCMAKE_BUILD_TYPE=Release', '-DKTRANSFORMERS_USE_CUDA=ON', '-D_GLIBCXX_USE_CXX11_ABI=0', '-DLLAMA_NATIVE=ON', '-DEXAMPLE_VERSION_INFO=0.3.2+cu125torch26fancy']\n",
            "  build_temp: /content/ktransformers/csrc/ktransformers_ext/build\n",
            "  building 'KTransformersOps' extension\n",
            "  creating /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/ktransformers_ext/cuda\n",
            "  creating /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/ktransformers_ext/cuda/custom_gguf\n",
            "  creating /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/ktransformers_ext/cuda/gptq_marlin\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\n",
            "  If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "    warnings.warn(\n",
            "  Emitting ninja build file /content/ktransformers/build/temp.linux-x86_64-cpython-311/build.ninja...\n",
            "  Compiling objects...\n",
            "  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "  [1/3] c++ -MMD -MF /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/ktransformers_ext/cuda/binding.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /content/ktransformers/csrc/ktransformers_ext/cuda/binding.cpp -o /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/ktransformers_ext/cuda/binding.o -O3 -DKTRANSFORMERS_USE_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=KTransformersOps -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  [2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/ktransformers_ext/cuda/gptq_marlin/gptq_marlin.o.d -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /content/ktransformers/csrc/ktransformers_ext/cuda/gptq_marlin/gptq_marlin.cu -o /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/ktransformers_ext/cuda/gptq_marlin/gptq_marlin.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -Xcompiler -fPIC -DKTRANSFORMERS_USE_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=KTransformersOps -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  [3/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/ktransformers_ext/cuda/custom_gguf/dequant.o.d -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /content/ktransformers/csrc/ktransformers_ext/cuda/custom_gguf/dequant.cu -o /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/ktransformers_ext/cuda/custom_gguf/dequant.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -Xcompiler -fPIC -DKTRANSFORMERS_USE_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=KTransformersOps -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/ktransformers_ext/cuda/binding.o /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/ktransformers_ext/cuda/custom_gguf/dequant.o /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/ktransformers_ext/cuda/gptq_marlin/gptq_marlin.o -L/usr/local/lib/python3.11/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-311/KTransformersOps.cpython-311-x86_64-linux-gnu.so\n",
            "  building 'vLLMMarlin' extension\n",
            "  creating /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/custom_marlin\n",
            "  creating /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/custom_marlin/gptq_marlin\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\n",
            "  If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "    warnings.warn(\n",
            "  Emitting ninja build file /content/ktransformers/build/temp.linux-x86_64-cpython-311/build.ninja...\n",
            "  Compiling objects...\n",
            "  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "  [1/3] c++ -MMD -MF /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/custom_marlin/binding.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /content/ktransformers/csrc/custom_marlin/binding.cpp -o /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/custom_marlin/binding.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=vLLMMarlin -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  [2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/custom_marlin/gptq_marlin/gptq_marlin_repack.o.d -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /content/ktransformers/csrc/custom_marlin/gptq_marlin/gptq_marlin_repack.cu -o /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/custom_marlin/gptq_marlin/gptq_marlin_repack.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -Xcompiler -fPIC -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=vLLMMarlin -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /content/ktransformers/csrc/custom_marlin/gptq_marlin/gptq_marlin_repack.cu(5): warning #177-D: variable \"gptq_marlin::repack_stages\" was declared but never referenced\n",
            "    static constexpr int repack_stages = 8;\n",
            "                         ^\n",
            "\n",
            "  Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "  /content/ktransformers/csrc/custom_marlin/gptq_marlin/gptq_marlin_repack.cu(7): warning #177-D: variable \"gptq_marlin::repack_threads\" was declared but never referenced\n",
            "    static constexpr int repack_threads = 256;\n",
            "                         ^\n",
            "\n",
            "  /content/ktransformers/csrc/custom_marlin/gptq_marlin/gptq_marlin_repack.cu(10): warning #177-D: variable \"gptq_marlin::tile_n_size\" was declared but never referenced\n",
            "    static constexpr int tile_n_size = tile_k_size * 4;\n",
            "                         ^\n",
            "\n",
            "  [3/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/custom_marlin/gptq_marlin/gptq_marlin.o.d -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /content/ktransformers/csrc/custom_marlin/gptq_marlin/gptq_marlin.cu -o /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/custom_marlin/gptq_marlin/gptq_marlin.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -Xcompiler -fPIC -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=vLLMMarlin -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/custom_marlin/binding.o /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/custom_marlin/gptq_marlin/gptq_marlin.o /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/custom_marlin/gptq_marlin/gptq_marlin_repack.o -L/usr/local/lib/python3.11/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-311/vLLMMarlin.cpython-311-x86_64-linux-gnu.so\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-311/vLLMMarlin.cpython-311-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/.\n",
            "  copying build/lib.linux-x86_64-cpython-311/KTransformersOps.cpython-311-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/.\n",
            "  copying build/lib.linux-x86_64-cpython-311/cpuinfer_ext.cpython-311-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/.\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/local_chat_test.py -> build/bdist.linux-x86_64/wheel/./ktransformers\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/configs\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/configs/config.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/configs\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/configs/log_config.ini -> build/bdist.linux-x86_64/wheel/./ktransformers/configs\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/util\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/util/textstream.py -> build/bdist.linux-x86_64/wheel/./ktransformers/util\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/util/modeling_rope_utils.py -> build/bdist.linux-x86_64/wheel/./ktransformers/util\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/util/weight_loader.py -> build/bdist.linux-x86_64/wheel/./ktransformers/util\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/util/vendors.py -> build/bdist.linux-x86_64/wheel/./ktransformers/util\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/util/custom_gguf.py -> build/bdist.linux-x86_64/wheel/./ktransformers/util\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/util/utils.py -> build/bdist.linux-x86_64/wheel/./ktransformers/util\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/util/cuda_graph_runner.py -> build/bdist.linux-x86_64/wheel/./ktransformers/util\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/util/custom_loader.py -> build/bdist.linux-x86_64/wheel/./ktransformers/util\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/optimize\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize.py -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/Moonlight-16B-A3B-serve.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/Qwen2-57B-A14B-Instruct.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat-multi-gpu.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat-multi-gpu-4.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-8.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-4.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat-multi-gpu.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/Qwen3Moe-serve-amx.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/Qwen3Moe-serve.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-fp8-linear-ggml-experts.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/Qwen2-serve.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/Internlm2_5-7b-Chat-1m.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-fp8-linear-ggml-experts-serve-amx.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/Qwen2-serve-amx.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-amx.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat-gpu-cpu.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/optimize/optimize_rules/xpu\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/xpu/DeepSeek-V3-Chat.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules/xpu\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/xpu/DeepSeek-V2-Chat.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules/xpu\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/xpu/Qwen3Moe-Chat.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules/xpu\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-marlin.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/Mixtral.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/optimize/optimize_rules/rocm\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/rocm/DeepSeek-V3-Chat.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules/rocm\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-fp8-linear-ggml-experts.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-fp8-linear-ggml-experts-serve.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-serve.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/Qwen2-57B-A14B-Instruct-multi-gpu.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/Moonlight-16B-A3B.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/local_chat.py -> build/bdist.linux-x86_64/wheel/./ktransformers\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/tests\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/mmlu_test.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/dequant_gpu.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/test_client.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/test_prefix.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/tests/AIME_2024\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/AIME_2024/evaluation.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests/AIME_2024\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/AIME_2024/eval_api.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests/AIME_2024\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/AIME_2024/prompts.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests/AIME_2024\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/test_pytorch_q8.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/triton_fp8gemm_test.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/dequant_gpu_t.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/.gitignore -> build/bdist.linux-x86_64/wheel/./ktransformers/tests\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/score.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/tests/humaneval\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/humaneval/evaluation.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests/humaneval\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/humaneval/eval_api.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests/humaneval\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/humaneval/prompts.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests/humaneval\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/mmlu_test_multi.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/test_speed.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/mmlu_pro_test.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/function_call_test.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/ktransformers_ext\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/ktransformers_ext/triton\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/triton/fp8gemm.py -> build/bdist.linux-x86_64/wheel/./ktransformers/ktransformers_ext/triton\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/ktransformers_ext/operators\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/ktransformers_ext/operators/custom_marlin\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/ktransformers_ext/operators/custom_marlin/quantize\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/quant_utils.py -> build/bdist.linux-x86_64/wheel/./ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/marlin_24_perms.py -> build/bdist.linux-x86_64/wheel/./ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/marlin_perms.py -> build/bdist.linux-x86_64/wheel/./ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/marlin_utils.py -> build/bdist.linux-x86_64/wheel/./ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/format_24.py -> build/bdist.linux-x86_64/wheel/./ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/custom_cache.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/modeling_deepseek.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/modeling_deepseek_v3.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/custom_modeling_deepseek_v3.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/configuration_qwen3_moe.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/configuration_deepseek_v3.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/modeling_llama.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/configuration_llama.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/modeling_mixtral.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/configuration_deepseek.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/modeling_qwen3_moe.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/custom_modeling_qwen3_moe.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/modeling_qwen2_moe.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/custom_modeling_qwen2_moe.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/custom_modeling_deepseek_v2.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/configuration_qwen2_moe.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/triton_attention.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/layernorm.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/models.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/attention.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/linear.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/gate.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/RoPE.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/flashinfer_batch_prefill_wrapper.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/balance_serve_attention.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/triton_attention_prefill.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/cpuinfer.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/experts.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/base_operator.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/dynamic_attention.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/flashinfer_wrapper.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/mlp.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/schemas\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/schemas/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants/assistants.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/schemas/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/schemas/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants/runs.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/schemas/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants/messages.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/schemas/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants/tool.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/schemas/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants/threads.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/schemas/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants/streaming.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/schemas/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/schemas\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/base.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/schemas\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/schemas/legacy\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/legacy/completions.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/schemas/legacy\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/legacy/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/schemas/legacy\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/conversation.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/schemas\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/schemas/endpoints\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/endpoints/chat.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/schemas/endpoints\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/backend\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/context_manager.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/backend\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/backend\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/backend/interfaces\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces/transformers.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/backend/interfaces\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces/exllamav2.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/backend/interfaces\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces/balance_serve.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/backend/interfaces\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces/ktransformers.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/backend/interfaces\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/backend/interfaces\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/base.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/backend\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/args.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/backend\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/main.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/config\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/config/config.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/config\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/config/singleton.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/config\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/config/log.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/config\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/crud\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/crud/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants/assistants.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/crud/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/crud/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants/runs.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/crud/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants/messages.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/crud/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants/threads.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/crud/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/crud\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/api\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/api/openai\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/api/openai/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants/assistants.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/openai/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/openai/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants/runs.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/openai/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants/messages.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/openai/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants/threads.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/openai/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/openai\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/api/openai/legacy\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/legacy/completions.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/openai/legacy\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/legacy/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/openai/legacy\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/api/openai/endpoints\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/endpoints/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/openai/endpoints\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/endpoints/chat.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/openai/endpoints\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/api/ollama\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/ollama/completions.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/ollama\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/ollama/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/ollama\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/api/web\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/web/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/web\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/web/system.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/web\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/exceptions.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/balance_serve\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/balance_serve/inference\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/config.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/query_manager.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/model_runner.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/forward_batch.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/balance_serve/inference/sampling\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/balance_serve/inference/sampling/penaltylib\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/presence_penalty.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/frequency_penalty.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/repetition_penalty.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/min_new_tokens.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/orchestrator.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/sampling/penaltylib\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/sampling/penaltylib\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/sampler.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/sampling\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed/communication_op.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/distributed\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed/custom_all_reduce.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/distributed\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed/pynccl_wrapper.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/distributed\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/distributed\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed/parallel_state.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/distributed\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed/pynccl.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/distributed\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed/utils.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/distributed\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed/custom_all_reduce_utils.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/distributed\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed/cuda_wrapper.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/distributed\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/sched_rpc.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/settings.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/utils/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/utils/sql_utils.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/utils/multi_timer.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/utils/create_interface.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/requirements.txt -> build/bdist.linux-x86_64/wheel/./ktransformers/server\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/models\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/models/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants/assistants.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/models/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/models/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants/runs.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/models/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants/messages.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/models/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants/threads.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/models/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants/run_steps.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/models/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/models/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/args.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server\n",
            "  running install_egg_info\n",
            "  Copying ktransformers.egg-info to build/bdist.linux-x86_64/wheel/./ktransformers-0.3.2+cu125torch26fancy-py3.11.egg-info\n",
            "  running install_scripts\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers-0.3.2+cu125torch26fancy.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-7jpvd9hd/.tmp-_bpjhkri/ktransformers-0.3.2+cu125torch26fancy-cp311-cp311-linux_x86_64.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'KTransformersOps.cpython-311-x86_64-linux-gnu.so'\n",
            "  adding 'cpuinfer_ext.cpython-311-x86_64-linux-gnu.so'\n",
            "  adding 'vLLMMarlin.cpython-311-x86_64-linux-gnu.so'\n",
            "  adding 'ktransformers/__init__.py'\n",
            "  adding 'ktransformers/local_chat.py'\n",
            "  adding 'ktransformers/local_chat_test.py'\n",
            "  adding 'ktransformers/configs/config.yaml'\n",
            "  adding 'ktransformers/configs/log_config.ini'\n",
            "  adding 'ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/__init__.py'\n",
            "  adding 'ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/format_24.py'\n",
            "  adding 'ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/marlin_24_perms.py'\n",
            "  adding 'ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/marlin_perms.py'\n",
            "  adding 'ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/marlin_utils.py'\n",
            "  adding 'ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/quant_utils.py'\n",
            "  adding 'ktransformers/ktransformers_ext/triton/fp8gemm.py'\n",
            "  adding 'ktransformers/models/__init__.py'\n",
            "  adding 'ktransformers/models/configuration_deepseek.py'\n",
            "  adding 'ktransformers/models/configuration_deepseek_v3.py'\n",
            "  adding 'ktransformers/models/configuration_llama.py'\n",
            "  adding 'ktransformers/models/configuration_qwen2_moe.py'\n",
            "  adding 'ktransformers/models/configuration_qwen3_moe.py'\n",
            "  adding 'ktransformers/models/custom_cache.py'\n",
            "  adding 'ktransformers/models/custom_modeling_deepseek_v2.py'\n",
            "  adding 'ktransformers/models/custom_modeling_deepseek_v3.py'\n",
            "  adding 'ktransformers/models/custom_modeling_qwen2_moe.py'\n",
            "  adding 'ktransformers/models/custom_modeling_qwen3_moe.py'\n",
            "  adding 'ktransformers/models/modeling_deepseek.py'\n",
            "  adding 'ktransformers/models/modeling_deepseek_v3.py'\n",
            "  adding 'ktransformers/models/modeling_llama.py'\n",
            "  adding 'ktransformers/models/modeling_mixtral.py'\n",
            "  adding 'ktransformers/models/modeling_qwen2_moe.py'\n",
            "  adding 'ktransformers/models/modeling_qwen3_moe.py'\n",
            "  adding 'ktransformers/operators/RoPE.py'\n",
            "  adding 'ktransformers/operators/__init__.py'\n",
            "  adding 'ktransformers/operators/attention.py'\n",
            "  adding 'ktransformers/operators/balance_serve_attention.py'\n",
            "  adding 'ktransformers/operators/base_operator.py'\n",
            "  adding 'ktransformers/operators/cpuinfer.py'\n",
            "  adding 'ktransformers/operators/dynamic_attention.py'\n",
            "  adding 'ktransformers/operators/experts.py'\n",
            "  adding 'ktransformers/operators/flashinfer_batch_prefill_wrapper.py'\n",
            "  adding 'ktransformers/operators/flashinfer_wrapper.py'\n",
            "  adding 'ktransformers/operators/gate.py'\n",
            "  adding 'ktransformers/operators/layernorm.py'\n",
            "  adding 'ktransformers/operators/linear.py'\n",
            "  adding 'ktransformers/operators/mlp.py'\n",
            "  adding 'ktransformers/operators/models.py'\n",
            "  adding 'ktransformers/operators/triton_attention.py'\n",
            "  adding 'ktransformers/operators/triton_attention_prefill.py'\n",
            "  adding 'ktransformers/optimize/optimize.py'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat-multi-gpu-4.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat-multi-gpu.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat-gpu-cpu.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat-multi-gpu.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-amx.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-fp8-linear-ggml-experts-serve-amx.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-fp8-linear-ggml-experts-serve.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-fp8-linear-ggml-experts.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-4.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-8.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-fp8-linear-ggml-experts.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-marlin.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-serve.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/Internlm2_5-7b-Chat-1m.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/Mixtral.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/Moonlight-16B-A3B-serve.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/Moonlight-16B-A3B.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/Qwen2-57B-A14B-Instruct-multi-gpu.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/Qwen2-57B-A14B-Instruct.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/Qwen2-serve-amx.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/Qwen2-serve.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/Qwen3Moe-serve-amx.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/Qwen3Moe-serve.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/rocm/DeepSeek-V3-Chat.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/xpu/DeepSeek-V2-Chat.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/xpu/DeepSeek-V3-Chat.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/xpu/Qwen3Moe-Chat.yaml'\n",
            "  adding 'ktransformers/server/__init__.py'\n",
            "  adding 'ktransformers/server/args.py'\n",
            "  adding 'ktransformers/server/exceptions.py'\n",
            "  adding 'ktransformers/server/main.py'\n",
            "  adding 'ktransformers/server/requirements.txt'\n",
            "  adding 'ktransformers/server/api/__init__.py'\n",
            "  adding 'ktransformers/server/api/ollama/__init__.py'\n",
            "  adding 'ktransformers/server/api/ollama/completions.py'\n",
            "  adding 'ktransformers/server/api/openai/__init__.py'\n",
            "  adding 'ktransformers/server/api/openai/assistants/__init__.py'\n",
            "  adding 'ktransformers/server/api/openai/assistants/assistants.py'\n",
            "  adding 'ktransformers/server/api/openai/assistants/messages.py'\n",
            "  adding 'ktransformers/server/api/openai/assistants/runs.py'\n",
            "  adding 'ktransformers/server/api/openai/assistants/threads.py'\n",
            "  adding 'ktransformers/server/api/openai/endpoints/__init__.py'\n",
            "  adding 'ktransformers/server/api/openai/endpoints/chat.py'\n",
            "  adding 'ktransformers/server/api/openai/legacy/__init__.py'\n",
            "  adding 'ktransformers/server/api/openai/legacy/completions.py'\n",
            "  adding 'ktransformers/server/api/web/__init__.py'\n",
            "  adding 'ktransformers/server/api/web/system.py'\n",
            "  adding 'ktransformers/server/backend/__init__.py'\n",
            "  adding 'ktransformers/server/backend/args.py'\n",
            "  adding 'ktransformers/server/backend/base.py'\n",
            "  adding 'ktransformers/server/backend/context_manager.py'\n",
            "  adding 'ktransformers/server/backend/interfaces/__init__.py'\n",
            "  adding 'ktransformers/server/backend/interfaces/balance_serve.py'\n",
            "  adding 'ktransformers/server/backend/interfaces/exllamav2.py'\n",
            "  adding 'ktransformers/server/backend/interfaces/ktransformers.py'\n",
            "  adding 'ktransformers/server/backend/interfaces/transformers.py'\n",
            "  adding 'ktransformers/server/balance_serve/sched_rpc.py'\n",
            "  adding 'ktransformers/server/balance_serve/settings.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/__init__.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/config.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/forward_batch.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/model_runner.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/query_manager.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/distributed/__init__.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/distributed/communication_op.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/distributed/cuda_wrapper.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/distributed/custom_all_reduce.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/distributed/custom_all_reduce_utils.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/distributed/parallel_state.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/distributed/pynccl.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/distributed/pynccl_wrapper.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/distributed/utils.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/sampling/sampler.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/sampling/penaltylib/__init__.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/sampling/penaltylib/orchestrator.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/frequency_penalty.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/min_new_tokens.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/presence_penalty.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/repetition_penalty.py'\n",
            "  adding 'ktransformers/server/config/config.py'\n",
            "  adding 'ktransformers/server/config/log.py'\n",
            "  adding 'ktransformers/server/config/singleton.py'\n",
            "  adding 'ktransformers/server/crud/__init__.py'\n",
            "  adding 'ktransformers/server/crud/assistants/__init__.py'\n",
            "  adding 'ktransformers/server/crud/assistants/assistants.py'\n",
            "  adding 'ktransformers/server/crud/assistants/messages.py'\n",
            "  adding 'ktransformers/server/crud/assistants/runs.py'\n",
            "  adding 'ktransformers/server/crud/assistants/threads.py'\n",
            "  adding 'ktransformers/server/models/__init__.py'\n",
            "  adding 'ktransformers/server/models/assistants/__init__.py'\n",
            "  adding 'ktransformers/server/models/assistants/assistants.py'\n",
            "  adding 'ktransformers/server/models/assistants/messages.py'\n",
            "  adding 'ktransformers/server/models/assistants/run_steps.py'\n",
            "  adding 'ktransformers/server/models/assistants/runs.py'\n",
            "  adding 'ktransformers/server/models/assistants/threads.py'\n",
            "  adding 'ktransformers/server/schemas/__init__.py'\n",
            "  adding 'ktransformers/server/schemas/base.py'\n",
            "  adding 'ktransformers/server/schemas/conversation.py'\n",
            "  adding 'ktransformers/server/schemas/assistants/__init__.py'\n",
            "  adding 'ktransformers/server/schemas/assistants/assistants.py'\n",
            "  adding 'ktransformers/server/schemas/assistants/messages.py'\n",
            "  adding 'ktransformers/server/schemas/assistants/runs.py'\n",
            "  adding 'ktransformers/server/schemas/assistants/streaming.py'\n",
            "  adding 'ktransformers/server/schemas/assistants/threads.py'\n",
            "  adding 'ktransformers/server/schemas/assistants/tool.py'\n",
            "  adding 'ktransformers/server/schemas/endpoints/chat.py'\n",
            "  adding 'ktransformers/server/schemas/legacy/__init__.py'\n",
            "  adding 'ktransformers/server/schemas/legacy/completions.py'\n",
            "  adding 'ktransformers/server/utils/__init__.py'\n",
            "  adding 'ktransformers/server/utils/create_interface.py'\n",
            "  adding 'ktransformers/server/utils/multi_timer.py'\n",
            "  adding 'ktransformers/server/utils/sql_utils.py'\n",
            "  adding 'ktransformers/tests/.gitignore'\n",
            "  adding 'ktransformers/tests/dequant_gpu.py'\n",
            "  adding 'ktransformers/tests/dequant_gpu_t.py'\n",
            "  adding 'ktransformers/tests/function_call_test.py'\n",
            "  adding 'ktransformers/tests/mmlu_pro_test.py'\n",
            "  adding 'ktransformers/tests/mmlu_test.py'\n",
            "  adding 'ktransformers/tests/mmlu_test_multi.py'\n",
            "  adding 'ktransformers/tests/score.py'\n",
            "  adding 'ktransformers/tests/test_client.py'\n",
            "  adding 'ktransformers/tests/test_prefix.py'\n",
            "  adding 'ktransformers/tests/test_pytorch_q8.py'\n",
            "  adding 'ktransformers/tests/test_speed.py'\n",
            "  adding 'ktransformers/tests/triton_fp8gemm_test.py'\n",
            "  adding 'ktransformers/tests/AIME_2024/eval_api.py'\n",
            "  adding 'ktransformers/tests/AIME_2024/evaluation.py'\n",
            "  adding 'ktransformers/tests/AIME_2024/prompts.py'\n",
            "  adding 'ktransformers/tests/humaneval/eval_api.py'\n",
            "  adding 'ktransformers/tests/humaneval/evaluation.py'\n",
            "  adding 'ktransformers/tests/humaneval/prompts.py'\n",
            "  adding 'ktransformers/util/cuda_graph_runner.py'\n",
            "  adding 'ktransformers/util/custom_gguf.py'\n",
            "  adding 'ktransformers/util/custom_loader.py'\n",
            "  adding 'ktransformers/util/modeling_rope_utils.py'\n",
            "  adding 'ktransformers/util/textstream.py'\n",
            "  adding 'ktransformers/util/utils.py'\n",
            "  adding 'ktransformers/util/vendors.py'\n",
            "  adding 'ktransformers/util/weight_loader.py'\n",
            "  adding 'ktransformers-0.3.2+cu125torch26fancy.dist-info/LICENSE'\n",
            "  adding 'ktransformers-0.3.2+cu125torch26fancy.dist-info/METADATA'\n",
            "  adding 'ktransformers-0.3.2+cu125torch26fancy.dist-info/WHEEL'\n",
            "  adding 'ktransformers-0.3.2+cu125torch26fancy.dist-info/entry_points.txt'\n",
            "  adding 'ktransformers-0.3.2+cu125torch26fancy.dist-info/top_level.txt'\n",
            "  adding 'ktransformers-0.3.2+cu125torch26fancy.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "  Building wheel for ktransformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ktransformers: filename=ktransformers-0.3.2+cu125torch26fancy-cp311-cp311-linux_x86_64.whl size=6841225 sha256=df5345288fc77d6bfc938769bb273955c0025d45b09181bc4206ff9abc105419\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-u4locyrk/wheels/5f/b7/4e/f9145db9bbbc2f9bd6d7c671f5078258219e4b9eaf9d320765\n",
            "Successfully built ktransformers\n",
            "Installing collected packages: ktransformers\n",
            "  changing mode of /usr/local/bin/ktransformers to 755\n",
            "Successfully installed ktransformers-0.3.2+cu125torch26fancy\n",
            "Installing custom_flashinfer for CUDA backend\n",
            "Processing ./third_party/custom_flashinfer\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from flashinfer-python==0.2.3) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flashinfer-python==0.2.3) (2.6.0+cu124)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from flashinfer-python==0.2.3) (1.11.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (2025.7.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flashinfer-python==0.2.3) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flashinfer-python==0.2.3) (3.0.2)\n",
            "Building wheels for collected packages: flashinfer-python\n",
            "  Building wheel for flashinfer-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashinfer-python: filename=flashinfer_python-0.2.3-py3-none-any.whl size=3165547 sha256=63dac259269a6f201415eadda61ebbdd85e8b1614e5e823486772e1738ba3d8a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-p19o16xs/wheels/ac/ca/e2/57d75be767d1d4ad30e954122867935ba1eb7009e978098bdb\n",
            "Successfully built flashinfer-python\n",
            "Installing collected packages: flashinfer-python\n",
            "Successfully installed flashinfer-python-0.2.3\n",
            "Installation completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ktransformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGNSZR2hGQ2B",
        "outputId": "de0e754c-f020-476d-953b-b9624522f159"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ktransformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git submodule update --init --recursive # Update PhotonLibOS submodule"
      ],
      "metadata": {
        "id": "fkDC6o88D-vA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Begin from root of your cloned repo!\n",
        "# Begin from root of your cloned repo!!\n",
        "# Begin from root of your cloned repo!!!\n",
        "\n",
        "# Download mzwing/DeepSeek-V2-Lite-Chat-GGUF from huggingface\n",
        "mkdir DeepSeek-V2-Lite-Chat-GGUF\n",
        "cd DeepSeek-V2-Lite-Chat-GGUF\n",
        "\n",
        "wget https://huggingface.co/mradermacher/DeepSeek-V2-Lite-GGUF/resolve/main/DeepSeek-V2-Lite.Q4_K_M.gguf -O DeepSeek-V2-Lite-Chat.Q4_K_M.gguf\n",
        "\n",
        "cd .. # Move to repo's root dir\n",
        "\n",
        "# Start local chat\n",
        "python -m ktransformers.local_chat --model_path deepseek-ai/DeepSeek-V2-Lite-Chat --gguf_path ./DeepSeek-V2-Lite-Chat-GGUF\n",
        "\n",
        "# If you see “OSError: We couldn't connect to 'https://huggingface.co' to load this file”, try：\n",
        "# GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite\n",
        "# python  ktransformers.local_chat --model_path ./DeepSeek-V2-Lite --gguf_path ./DeepSeek-V2-Lite-Chat-GGUF\n"
      ],
      "metadata": {
        "id": "2gBs0UWaD58g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir a"
      ],
      "metadata": {
        "id": "V6gvsfT3F6wJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnCp4dQoF9MP",
        "outputId": "86cf625a-4f3c-47e4-fb71-a9b7ab9474cf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ktransformers/a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q2_K.gguf"
      ],
      "metadata": {
        "id": "QktO-1j1EzcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ktransformers\n",
        "!mkdir DeepSeek-V2-Lite-Chat-GGUF\n",
        "%cd DeepSeek-V2-Lite-Chat-GGUF\n",
        "\n",
        "!wget https://huggingface.co/mradermacher/DeepSeek-V2-Lite-GGUF/resolve/main/DeepSeek-V2-Lite.Q4_K_M.gguf -O DeepSeek-V2-Lite-Chat.Q4_K_M.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPW3bTKbG1W6",
        "outputId": "28c75477-2665-48dc-d9ef-4de7ba4663c4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ktransformers\n",
            "/content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF\n",
            "--2025-07-24 20:24:22--  https://huggingface.co/mradermacher/DeepSeek-V2-Lite-GGUF/resolve/main/DeepSeek-V2-Lite.Q4_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.97, 13.35.202.121, 13.35.202.40, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/6659716edeaa741cf3171d55/d7437be0ae6f412c69e4f4175311859d028b5a2f00b72f46cd7a0d3294cab4b7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250724%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250724T202422Z&X-Amz-Expires=3600&X-Amz-Signature=221b6f8bae08349d0fe9216506a8d11d76ecc256b8cfc9840c12e01ed83b8bf9&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27DeepSeek-V2-Lite.Q4_K_M.gguf%3B+filename%3D%22DeepSeek-V2-Lite.Q4_K_M.gguf%22%3B&x-id=GetObject&Expires=1753392262&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MzM5MjI2Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjU5NzE2ZWRlYWE3NDFjZjMxNzFkNTUvZDc0MzdiZTBhZTZmNDEyYzY5ZTRmNDE3NTMxMTg1OWQwMjhiNWEyZjAwYjcyZjQ2Y2Q3YTBkMzI5NGNhYjRiNyoifV19&Signature=fJmVZgSror8U5kXX%7EgLkbR4OXTcIrGdfkGdTu-V0yFyX6E1FHoUonLgPAd0oJ8ye1B71FcAUTEcS%7E0e-HZJhBzOf2xMszy7dQQC6x31wbJthS%7EoOkagbumOeAK-1KWYK2fErA0OOqF3V--CIJqwK9a39MjFzV884OMiy0hSbdaJQo9xt3P4dp9MS5J7tS5lP4RePfqQnrlb6J45TlfchysWk3oqaV4XT1d86XlM6frMWcEfUDlduazkLfc4tl6dSz1hHF3cDZUShtAB6bbIcYszD7Q9bvAdlJzm5tLusBNfQMEldXmtVDna6NAelr1yplsxacOEXWfJGaJ4SdEzBLQ__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-07-24 20:24:22--  https://cas-bridge.xethub.hf.co/xet-bridge-us/6659716edeaa741cf3171d55/d7437be0ae6f412c69e4f4175311859d028b5a2f00b72f46cd7a0d3294cab4b7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250724%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250724T202422Z&X-Amz-Expires=3600&X-Amz-Signature=221b6f8bae08349d0fe9216506a8d11d76ecc256b8cfc9840c12e01ed83b8bf9&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27DeepSeek-V2-Lite.Q4_K_M.gguf%3B+filename%3D%22DeepSeek-V2-Lite.Q4_K_M.gguf%22%3B&x-id=GetObject&Expires=1753392262&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MzM5MjI2Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjU5NzE2ZWRlYWE3NDFjZjMxNzFkNTUvZDc0MzdiZTBhZTZmNDEyYzY5ZTRmNDE3NTMxMTg1OWQwMjhiNWEyZjAwYjcyZjQ2Y2Q3YTBkMzI5NGNhYjRiNyoifV19&Signature=fJmVZgSror8U5kXX%7EgLkbR4OXTcIrGdfkGdTu-V0yFyX6E1FHoUonLgPAd0oJ8ye1B71FcAUTEcS%7E0e-HZJhBzOf2xMszy7dQQC6x31wbJthS%7EoOkagbumOeAK-1KWYK2fErA0OOqF3V--CIJqwK9a39MjFzV884OMiy0hSbdaJQo9xt3P4dp9MS5J7tS5lP4RePfqQnrlb6J45TlfchysWk3oqaV4XT1d86XlM6frMWcEfUDlduazkLfc4tl6dSz1hHF3cDZUShtAB6bbIcYszD7Q9bvAdlJzm5tLusBNfQMEldXmtVDna6NAelr1yplsxacOEXWfJGaJ4SdEzBLQ__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.155.68.125, 18.155.68.69, 18.155.68.14, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.155.68.125|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10364416736 (9.7G)\n",
            "Saving to: ‘DeepSeek-V2-Lite-Chat.Q4_K_M.gguf’\n",
            "\n",
            "DeepSeek-V2-Lite-Ch 100%[===================>]   9.65G   248MB/s    in 54s     \n",
            "\n",
            "2025-07-24 20:25:16 (183 MB/s) - ‘DeepSeek-V2-Lite-Chat.Q4_K_M.gguf’ saved [10364416736/10364416736]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoZDSm-UHNo5",
        "outputId": "6ade8656-0211-43e6-9411-32a2700634ea"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ktransformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m ktransformers.local_chat --model_path deepseek-ai/DeepSeek-V2-Lite-Chat --gguf_path ./DeepSeek-V2-Lite-Chat-GGUF\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQ2g1HqZHOla",
        "outputId": "2f9d4c1e-96d8-4734-dee7-3f8c46d339f8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-07-24 20:27:29,459 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-07-24 20:27:31.240499: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753388851.260439    8697 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753388851.266878    8697 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-24 20:27:31.287363: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "DeepseekV2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "using default_optimize_rule for DeepseekV2ForCausalLM\n",
            "falsh attn not found\n",
            "Injecting model as ktransformers.operators.models . KDeepseekV2Model\n",
            "Injecting model.embed_tokens as default\n",
            "Injecting model.layers as default\n",
            "Injecting model.layers.0 as default\n",
            "Injecting model.layers.0.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.0.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.0.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.0.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.0.mlp as default\n",
            "Injecting model.layers.0.mlp.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.mlp.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.mlp.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.mlp.act_fn as default\n",
            "Injecting model.layers.0.input_layernorm as default\n",
            "Injecting model.layers.0.post_attention_layernorm as default\n",
            "Injecting model.layers.1 as default\n",
            "Injecting model.layers.1.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.1.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.1.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.1.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.1.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.1.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.1.mlp.gate as default\n",
            "Injecting model.layers.1.mlp.shared_experts as default\n",
            "Injecting model.layers.1.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.1.input_layernorm as default\n",
            "Injecting model.layers.1.post_attention_layernorm as default\n",
            "Injecting model.layers.2 as default\n",
            "Injecting model.layers.2.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.2.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.2.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.2.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.2.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.2.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.2.mlp.gate as default\n",
            "Injecting model.layers.2.mlp.shared_experts as default\n",
            "Injecting model.layers.2.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.2.input_layernorm as default\n",
            "Injecting model.layers.2.post_attention_layernorm as default\n",
            "Injecting model.layers.3 as default\n",
            "Injecting model.layers.3.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.3.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.3.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.3.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.3.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.3.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.3.mlp.gate as default\n",
            "Injecting model.layers.3.mlp.shared_experts as default\n",
            "Injecting model.layers.3.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.3.input_layernorm as default\n",
            "Injecting model.layers.3.post_attention_layernorm as default\n",
            "Injecting model.layers.4 as default\n",
            "Injecting model.layers.4.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.4.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.4.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.4.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.4.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.4.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.4.mlp.gate as default\n",
            "Injecting model.layers.4.mlp.shared_experts as default\n",
            "Injecting model.layers.4.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.4.input_layernorm as default\n",
            "Injecting model.layers.4.post_attention_layernorm as default\n",
            "Injecting model.layers.5 as default\n",
            "Injecting model.layers.5.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.5.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.5.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.5.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.5.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.5.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.5.mlp.gate as default\n",
            "Injecting model.layers.5.mlp.shared_experts as default\n",
            "Injecting model.layers.5.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.5.input_layernorm as default\n",
            "Injecting model.layers.5.post_attention_layernorm as default\n",
            "Injecting model.layers.6 as default\n",
            "Injecting model.layers.6.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.6.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.6.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.6.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.6.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.6.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.6.mlp.gate as default\n",
            "Injecting model.layers.6.mlp.shared_experts as default\n",
            "Injecting model.layers.6.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.6.input_layernorm as default\n",
            "Injecting model.layers.6.post_attention_layernorm as default\n",
            "Injecting model.layers.7 as default\n",
            "Injecting model.layers.7.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.7.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.7.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.7.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.7.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.7.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.7.mlp.gate as default\n",
            "Injecting model.layers.7.mlp.shared_experts as default\n",
            "Injecting model.layers.7.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.7.input_layernorm as default\n",
            "Injecting model.layers.7.post_attention_layernorm as default\n",
            "Injecting model.layers.8 as default\n",
            "Injecting model.layers.8.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.8.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.8.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.8.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.8.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.8.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.8.mlp.gate as default\n",
            "Injecting model.layers.8.mlp.shared_experts as default\n",
            "Injecting model.layers.8.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.8.input_layernorm as default\n",
            "Injecting model.layers.8.post_attention_layernorm as default\n",
            "Injecting model.layers.9 as default\n",
            "Injecting model.layers.9.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.9.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.9.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.9.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.9.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.9.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.9.mlp.gate as default\n",
            "Injecting model.layers.9.mlp.shared_experts as default\n",
            "Injecting model.layers.9.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.9.input_layernorm as default\n",
            "Injecting model.layers.9.post_attention_layernorm as default\n",
            "Injecting model.layers.10 as default\n",
            "Injecting model.layers.10.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.10.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.10.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.10.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.10.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.10.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.10.mlp.gate as default\n",
            "Injecting model.layers.10.mlp.shared_experts as default\n",
            "Injecting model.layers.10.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.10.input_layernorm as default\n",
            "Injecting model.layers.10.post_attention_layernorm as default\n",
            "Injecting model.layers.11 as default\n",
            "Injecting model.layers.11.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.11.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.11.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.11.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.11.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.11.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.11.mlp.gate as default\n",
            "Injecting model.layers.11.mlp.shared_experts as default\n",
            "Injecting model.layers.11.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.11.input_layernorm as default\n",
            "Injecting model.layers.11.post_attention_layernorm as default\n",
            "Injecting model.layers.12 as default\n",
            "Injecting model.layers.12.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.12.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.12.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.12.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.12.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.12.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.12.mlp.gate as default\n",
            "Injecting model.layers.12.mlp.shared_experts as default\n",
            "Injecting model.layers.12.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.12.input_layernorm as default\n",
            "Injecting model.layers.12.post_attention_layernorm as default\n",
            "Injecting model.layers.13 as default\n",
            "Injecting model.layers.13.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.13.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.13.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.13.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.13.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.13.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.13.mlp.gate as default\n",
            "Injecting model.layers.13.mlp.shared_experts as default\n",
            "Injecting model.layers.13.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.13.input_layernorm as default\n",
            "Injecting model.layers.13.post_attention_layernorm as default\n",
            "Injecting model.layers.14 as default\n",
            "Injecting model.layers.14.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.14.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.14.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.14.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.14.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.14.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.14.mlp.gate as default\n",
            "Injecting model.layers.14.mlp.shared_experts as default\n",
            "Injecting model.layers.14.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.14.input_layernorm as default\n",
            "Injecting model.layers.14.post_attention_layernorm as default\n",
            "Injecting model.layers.15 as default\n",
            "Injecting model.layers.15.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.15.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.15.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.15.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.15.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.15.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.15.mlp.gate as default\n",
            "Injecting model.layers.15.mlp.shared_experts as default\n",
            "Injecting model.layers.15.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.15.input_layernorm as default\n",
            "Injecting model.layers.15.post_attention_layernorm as default\n",
            "Injecting model.layers.16 as default\n",
            "Injecting model.layers.16.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.16.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.16.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.16.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.16.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.16.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.16.mlp.gate as default\n",
            "Injecting model.layers.16.mlp.shared_experts as default\n",
            "Injecting model.layers.16.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.16.input_layernorm as default\n",
            "Injecting model.layers.16.post_attention_layernorm as default\n",
            "Injecting model.layers.17 as default\n",
            "Injecting model.layers.17.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.17.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.17.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.17.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.17.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.17.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.17.mlp.gate as default\n",
            "Injecting model.layers.17.mlp.shared_experts as default\n",
            "Injecting model.layers.17.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.17.input_layernorm as default\n",
            "Injecting model.layers.17.post_attention_layernorm as default\n",
            "Injecting model.layers.18 as default\n",
            "Injecting model.layers.18.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.18.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.18.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.18.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.18.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.18.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.18.mlp.gate as default\n",
            "Injecting model.layers.18.mlp.shared_experts as default\n",
            "Injecting model.layers.18.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.18.input_layernorm as default\n",
            "Injecting model.layers.18.post_attention_layernorm as default\n",
            "Injecting model.layers.19 as default\n",
            "Injecting model.layers.19.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.19.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.19.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.19.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.19.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.19.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.19.mlp.gate as default\n",
            "Injecting model.layers.19.mlp.shared_experts as default\n",
            "Injecting model.layers.19.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.19.input_layernorm as default\n",
            "Injecting model.layers.19.post_attention_layernorm as default\n",
            "Injecting model.layers.20 as default\n",
            "Injecting model.layers.20.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.20.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.20.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.20.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.20.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.20.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.20.mlp.gate as default\n",
            "Injecting model.layers.20.mlp.shared_experts as default\n",
            "Injecting model.layers.20.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.20.input_layernorm as default\n",
            "Injecting model.layers.20.post_attention_layernorm as default\n",
            "Injecting model.layers.21 as default\n",
            "Injecting model.layers.21.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.21.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.21.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.21.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.21.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.21.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.21.mlp.gate as default\n",
            "Injecting model.layers.21.mlp.shared_experts as default\n",
            "Injecting model.layers.21.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.21.input_layernorm as default\n",
            "Injecting model.layers.21.post_attention_layernorm as default\n",
            "Injecting model.layers.22 as default\n",
            "Injecting model.layers.22.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.22.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.22.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.22.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.22.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.22.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.22.mlp.gate as default\n",
            "Injecting model.layers.22.mlp.shared_experts as default\n",
            "Injecting model.layers.22.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.22.input_layernorm as default\n",
            "Injecting model.layers.22.post_attention_layernorm as default\n",
            "Injecting model.layers.23 as default\n",
            "Injecting model.layers.23.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.23.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.23.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.23.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.23.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.23.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.23.mlp.gate as default\n",
            "Injecting model.layers.23.mlp.shared_experts as default\n",
            "Injecting model.layers.23.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.23.input_layernorm as default\n",
            "Injecting model.layers.23.post_attention_layernorm as default\n",
            "Injecting model.layers.24 as default\n",
            "Injecting model.layers.24.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.24.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.24.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.24.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.24.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.24.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.24.mlp.gate as default\n",
            "Injecting model.layers.24.mlp.shared_experts as default\n",
            "Injecting model.layers.24.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.24.input_layernorm as default\n",
            "Injecting model.layers.24.post_attention_layernorm as default\n",
            "Injecting model.layers.25 as default\n",
            "Injecting model.layers.25.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.25.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.25.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.25.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.25.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.25.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.25.mlp.gate as default\n",
            "Injecting model.layers.25.mlp.shared_experts as default\n",
            "Injecting model.layers.25.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.25.input_layernorm as default\n",
            "Injecting model.layers.25.post_attention_layernorm as default\n",
            "Injecting model.layers.26 as default\n",
            "Injecting model.layers.26.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.26.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.26.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.26.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.26.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.26.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.26.mlp.gate as default\n",
            "Injecting model.layers.26.mlp.shared_experts as default\n",
            "Injecting model.layers.26.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.26.input_layernorm as default\n",
            "Injecting model.layers.26.post_attention_layernorm as default\n",
            "Injecting model.norm as default\n",
            "Injecting lm_head as ktransformers.operators.linear . KTransformersLinear\n",
            "loading model.embed_tokens.weight to cpu\n",
            "loading model.layers.0.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.0.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.0.input_layernorm.weight to cuda\n",
            "loading model.layers.0.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.1.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.1.self_attn.kv_b_proj.weight to cuda\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/ktransformers/ktransformers/local_chat.py\", line 197, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/ktransformers/local_chat.py\", line 124, in local_chat\n",
            "    optimize_and_load_gguf(model, optimize_config_path, gguf_path, config, default_device=device)\n",
            "  File \"/content/ktransformers/ktransformers/optimize/optimize.py\", line 131, in optimize_and_load_gguf\n",
            "    load_weights(module, weights_loader, device=default_device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 176, in load_weights\n",
            "    module.load()\n",
            "  File \"/content/ktransformers/ktransformers/operators/base_operator.py\", line 63, in load\n",
            "    utils.load_weights(child, self.gguf_loader, self.key+\".\")\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 176, in load_weights\n",
            "    module.load()\n",
            "  File \"/content/ktransformers/ktransformers/operators/base_operator.py\", line 63, in load\n",
            "    utils.load_weights(child, self.gguf_loader, self.key+\".\")\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 176, in load_weights\n",
            "    module.load()\n",
            "  File \"/content/ktransformers/ktransformers/operators/experts.py\", line 684, in load\n",
            "    self.generate_experts.load(w, warmup=warmup)\n",
            "  File \"/content/ktransformers/ktransformers/operators/experts.py\", line 245, in load\n",
            "    self.cpu_infer.submit(self.moe.warm_up())\n",
            "  File \"/content/ktransformers/ktransformers/operators/cpuinfer.py\", line 739, in submit\n",
            "    CPUInfer.cpuinfer.submit(task)\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'NoneType' object has no attribute 'submit'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "623172a9",
        "outputId": "4a1d47de-8e91-40f0-c753-adfeacb24aaa"
      },
      "source": [
        "# Example of how to initialize and use the submit method from the ktransformers documentation\n",
        "\n",
        "from ktransformers.local_chat import init_local_chat\n",
        "\n",
        "# Initialize the chat model. Replace with your actual model and gguf paths if needed.\n",
        "# The init_local_chat function should return an object with a submit method.\n",
        "chat_model = init_local_chat(\n",
        "    model_path='deepseek-ai/DeepSeek-V2-Lite-Chat',\n",
        "    gguf_path='./DeepSeek-V2-Lite-Chat-GGUF/DeepSeek-V2-Lite-Chat.Q4_K_M.gguf'\n",
        ")\n",
        "\n",
        "# Now you can use the submit method\n",
        "if chat_model:\n",
        "    response = chat_model.submit(\"Hello, how are you?\")\n",
        "    print(response)\n",
        "else:\n",
        "    print(\"Failed to initialize chat model.\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'init_local_chat' from 'ktransformers.local_chat' (/content/ktransformers/ktransformers/local_chat.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-16-990668100.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example of how to initialize and use the submit method from the ktransformers documentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mktransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_chat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_local_chat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Initialize the chat model. Replace with your actual model and gguf paths if needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'init_local_chat' from 'ktransformers.local_chat' (/content/ktransformers/ktransformers/local_chat.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade ktransformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPTks01WJSbR",
        "outputId": "94313cc4-682c-42be-f629-2810b83d821b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ktransformers in /usr/local/lib/python3.11/dist-packages (0.3.2+cu125torch26fancy)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers==4.51.3 in /usr/local/lib/python3.11/dist-packages (from ktransformers) (4.51.3)\n",
            "Requirement already satisfied: fastapi>=0.111.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers) (0.116.1)\n",
            "Requirement already satisfied: uvicorn>=0.30.1 in /usr/local/lib/python3.11/dist-packages (from ktransformers) (0.35.0)\n",
            "Requirement already satisfied: langchain>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers) (0.3.26)\n",
            "Requirement already satisfied: blessed>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers) (1.21.0)\n",
            "Requirement already satisfied: accelerate>=0.31.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers) (1.9.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.97 in /usr/local/lib/python3.11/dist-packages (from ktransformers) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from ktransformers) (75.2.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from ktransformers) (1.11.1.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from ktransformers) (0.45.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from ktransformers) (6.9.0)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.11/dist-packages (from ktransformers) (1.2.2.post1)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from ktransformers) (0.7.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from ktransformers) (5.29.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.31.0->ktransformers) (5.9.5)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from blessed>=1.20.0->ktransformers) (0.2.13)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->ktransformers) (0.47.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->ktransformers) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->ktransformers) (4.14.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers) (0.3.70)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers) (0.4.8)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers) (2.0.41)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (2025.7.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->ktransformers) (1.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.30.1->ktransformers) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.30.1->ktransformers) (0.16.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build->ktransformers) (1.2.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->ktransformers) (3.1.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3->ktransformers) (1.1.5)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain>=0.2.0->ktransformers) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain>=0.2.0->ktransformers) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain>=0.2.0->ktransformers) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain>=0.2.0->ktransformers) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain>=0.2.0->ktransformers) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain>=0.2.0->ktransformers) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->ktransformers) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->ktransformers) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->ktransformers) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->ktransformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->ktransformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->ktransformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->ktransformers) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.2.0->ktransformers) (3.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi>=0.111.0->ktransformers) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->ktransformers) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi>=0.111.0->ktransformers) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain>=0.2.0->ktransformers) (1.0.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain>=0.2.0->ktransformers) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ktransformers\n",
        "!python -m ktransformers.local_chat --model_path ZZichen/DeepSeek-V2-Lite --gguf_path /content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9zMqNAMIFlZ",
        "outputId": "fb8439da-dfda-41d6-b0b8-3d440227e81f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ktransformers\n",
            "no balance_serve\n",
            "2025-07-24 20:34:55,191 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-07-24 20:34:57.005450: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753389297.026349   10667 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753389297.032968   10667 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-24 20:34:57.053837: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "DeepseekV2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "using default_optimize_rule for DeepseekV2ForCausalLM\n",
            "falsh attn not found\n",
            "Injecting model as ktransformers.operators.models . KDeepseekV2Model\n",
            "Injecting model.embed_tokens as default\n",
            "Injecting model.layers as default\n",
            "Injecting model.layers.0 as default\n",
            "Injecting model.layers.0.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.0.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.0.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.0.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.0.mlp as default\n",
            "Injecting model.layers.0.mlp.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.mlp.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.mlp.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.mlp.act_fn as default\n",
            "Injecting model.layers.0.input_layernorm as default\n",
            "Injecting model.layers.0.post_attention_layernorm as default\n",
            "Injecting model.layers.1 as default\n",
            "Injecting model.layers.1.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.1.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.1.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.1.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.1.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.1.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.1.mlp.gate as default\n",
            "Injecting model.layers.1.mlp.shared_experts as default\n",
            "Injecting model.layers.1.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.1.input_layernorm as default\n",
            "Injecting model.layers.1.post_attention_layernorm as default\n",
            "Injecting model.layers.2 as default\n",
            "Injecting model.layers.2.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.2.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.2.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.2.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.2.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.2.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.2.mlp.gate as default\n",
            "Injecting model.layers.2.mlp.shared_experts as default\n",
            "Injecting model.layers.2.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.2.input_layernorm as default\n",
            "Injecting model.layers.2.post_attention_layernorm as default\n",
            "Injecting model.layers.3 as default\n",
            "Injecting model.layers.3.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.3.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.3.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.3.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.3.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.3.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.3.mlp.gate as default\n",
            "Injecting model.layers.3.mlp.shared_experts as default\n",
            "Injecting model.layers.3.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.3.input_layernorm as default\n",
            "Injecting model.layers.3.post_attention_layernorm as default\n",
            "Injecting model.layers.4 as default\n",
            "Injecting model.layers.4.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.4.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.4.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.4.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.4.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.4.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.4.mlp.gate as default\n",
            "Injecting model.layers.4.mlp.shared_experts as default\n",
            "Injecting model.layers.4.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.4.input_layernorm as default\n",
            "Injecting model.layers.4.post_attention_layernorm as default\n",
            "Injecting model.layers.5 as default\n",
            "Injecting model.layers.5.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.5.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.5.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.5.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.5.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.5.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.5.mlp.gate as default\n",
            "Injecting model.layers.5.mlp.shared_experts as default\n",
            "Injecting model.layers.5.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.5.input_layernorm as default\n",
            "Injecting model.layers.5.post_attention_layernorm as default\n",
            "Injecting model.layers.6 as default\n",
            "Injecting model.layers.6.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.6.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.6.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.6.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.6.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.6.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.6.mlp.gate as default\n",
            "Injecting model.layers.6.mlp.shared_experts as default\n",
            "Injecting model.layers.6.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.6.input_layernorm as default\n",
            "Injecting model.layers.6.post_attention_layernorm as default\n",
            "Injecting model.layers.7 as default\n",
            "Injecting model.layers.7.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.7.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.7.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.7.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.7.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.7.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.7.mlp.gate as default\n",
            "Injecting model.layers.7.mlp.shared_experts as default\n",
            "Injecting model.layers.7.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.7.input_layernorm as default\n",
            "Injecting model.layers.7.post_attention_layernorm as default\n",
            "Injecting model.layers.8 as default\n",
            "Injecting model.layers.8.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.8.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.8.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.8.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.8.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.8.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.8.mlp.gate as default\n",
            "Injecting model.layers.8.mlp.shared_experts as default\n",
            "Injecting model.layers.8.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.8.input_layernorm as default\n",
            "Injecting model.layers.8.post_attention_layernorm as default\n",
            "Injecting model.layers.9 as default\n",
            "Injecting model.layers.9.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.9.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.9.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.9.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.9.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.9.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.9.mlp.gate as default\n",
            "Injecting model.layers.9.mlp.shared_experts as default\n",
            "Injecting model.layers.9.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.9.input_layernorm as default\n",
            "Injecting model.layers.9.post_attention_layernorm as default\n",
            "Injecting model.layers.10 as default\n",
            "Injecting model.layers.10.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.10.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.10.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.10.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.10.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.10.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.10.mlp.gate as default\n",
            "Injecting model.layers.10.mlp.shared_experts as default\n",
            "Injecting model.layers.10.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.10.input_layernorm as default\n",
            "Injecting model.layers.10.post_attention_layernorm as default\n",
            "Injecting model.layers.11 as default\n",
            "Injecting model.layers.11.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.11.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.11.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.11.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.11.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.11.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.11.mlp.gate as default\n",
            "Injecting model.layers.11.mlp.shared_experts as default\n",
            "Injecting model.layers.11.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.11.input_layernorm as default\n",
            "Injecting model.layers.11.post_attention_layernorm as default\n",
            "Injecting model.layers.12 as default\n",
            "Injecting model.layers.12.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.12.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.12.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.12.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.12.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.12.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.12.mlp.gate as default\n",
            "Injecting model.layers.12.mlp.shared_experts as default\n",
            "Injecting model.layers.12.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.12.input_layernorm as default\n",
            "Injecting model.layers.12.post_attention_layernorm as default\n",
            "Injecting model.layers.13 as default\n",
            "Injecting model.layers.13.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.13.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.13.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.13.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.13.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.13.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.13.mlp.gate as default\n",
            "Injecting model.layers.13.mlp.shared_experts as default\n",
            "Injecting model.layers.13.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.13.input_layernorm as default\n",
            "Injecting model.layers.13.post_attention_layernorm as default\n",
            "Injecting model.layers.14 as default\n",
            "Injecting model.layers.14.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.14.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.14.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.14.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.14.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.14.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.14.mlp.gate as default\n",
            "Injecting model.layers.14.mlp.shared_experts as default\n",
            "Injecting model.layers.14.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.14.input_layernorm as default\n",
            "Injecting model.layers.14.post_attention_layernorm as default\n",
            "Injecting model.layers.15 as default\n",
            "Injecting model.layers.15.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.15.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.15.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.15.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.15.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.15.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.15.mlp.gate as default\n",
            "Injecting model.layers.15.mlp.shared_experts as default\n",
            "Injecting model.layers.15.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.15.input_layernorm as default\n",
            "Injecting model.layers.15.post_attention_layernorm as default\n",
            "Injecting model.layers.16 as default\n",
            "Injecting model.layers.16.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.16.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.16.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.16.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.16.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.16.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.16.mlp.gate as default\n",
            "Injecting model.layers.16.mlp.shared_experts as default\n",
            "Injecting model.layers.16.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.16.input_layernorm as default\n",
            "Injecting model.layers.16.post_attention_layernorm as default\n",
            "Injecting model.layers.17 as default\n",
            "Injecting model.layers.17.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.17.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.17.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.17.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.17.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.17.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.17.mlp.gate as default\n",
            "Injecting model.layers.17.mlp.shared_experts as default\n",
            "Injecting model.layers.17.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.17.input_layernorm as default\n",
            "Injecting model.layers.17.post_attention_layernorm as default\n",
            "Injecting model.layers.18 as default\n",
            "Injecting model.layers.18.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.18.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.18.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.18.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.18.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.18.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.18.mlp.gate as default\n",
            "Injecting model.layers.18.mlp.shared_experts as default\n",
            "Injecting model.layers.18.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.18.input_layernorm as default\n",
            "Injecting model.layers.18.post_attention_layernorm as default\n",
            "Injecting model.layers.19 as default\n",
            "Injecting model.layers.19.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.19.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.19.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.19.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.19.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.19.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.19.mlp.gate as default\n",
            "Injecting model.layers.19.mlp.shared_experts as default\n",
            "Injecting model.layers.19.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.19.input_layernorm as default\n",
            "Injecting model.layers.19.post_attention_layernorm as default\n",
            "Injecting model.layers.20 as default\n",
            "Injecting model.layers.20.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.20.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.20.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.20.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.20.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.20.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.20.mlp.gate as default\n",
            "Injecting model.layers.20.mlp.shared_experts as default\n",
            "Injecting model.layers.20.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.20.input_layernorm as default\n",
            "Injecting model.layers.20.post_attention_layernorm as default\n",
            "Injecting model.layers.21 as default\n",
            "Injecting model.layers.21.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.21.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.21.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.21.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.21.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.21.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.21.mlp.gate as default\n",
            "Injecting model.layers.21.mlp.shared_experts as default\n",
            "Injecting model.layers.21.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.21.input_layernorm as default\n",
            "Injecting model.layers.21.post_attention_layernorm as default\n",
            "Injecting model.layers.22 as default\n",
            "Injecting model.layers.22.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.22.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.22.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.22.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.22.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.22.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.22.mlp.gate as default\n",
            "Injecting model.layers.22.mlp.shared_experts as default\n",
            "Injecting model.layers.22.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.22.input_layernorm as default\n",
            "Injecting model.layers.22.post_attention_layernorm as default\n",
            "Injecting model.layers.23 as default\n",
            "Injecting model.layers.23.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.23.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.23.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.23.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.23.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.23.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.23.mlp.gate as default\n",
            "Injecting model.layers.23.mlp.shared_experts as default\n",
            "Injecting model.layers.23.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.23.input_layernorm as default\n",
            "Injecting model.layers.23.post_attention_layernorm as default\n",
            "Injecting model.layers.24 as default\n",
            "Injecting model.layers.24.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.24.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.24.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.24.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.24.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.24.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.24.mlp.gate as default\n",
            "Injecting model.layers.24.mlp.shared_experts as default\n",
            "Injecting model.layers.24.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.24.input_layernorm as default\n",
            "Injecting model.layers.24.post_attention_layernorm as default\n",
            "Injecting model.layers.25 as default\n",
            "Injecting model.layers.25.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.25.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.25.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.25.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.25.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.25.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.25.mlp.gate as default\n",
            "Injecting model.layers.25.mlp.shared_experts as default\n",
            "Injecting model.layers.25.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.25.input_layernorm as default\n",
            "Injecting model.layers.25.post_attention_layernorm as default\n",
            "Injecting model.layers.26 as default\n",
            "Injecting model.layers.26.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.26.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.26.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.26.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.26.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.26.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.26.mlp.gate as default\n",
            "Injecting model.layers.26.mlp.shared_experts as default\n",
            "Injecting model.layers.26.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.26.input_layernorm as default\n",
            "Injecting model.layers.26.post_attention_layernorm as default\n",
            "Injecting model.norm as default\n",
            "Injecting lm_head as ktransformers.operators.linear . KTransformersLinear\n",
            "loading model.embed_tokens.weight to cpu\n",
            "loading model.layers.0.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.0.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.0.input_layernorm.weight to cuda\n",
            "loading model.layers.0.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.1.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.1.self_attn.kv_b_proj.weight to cuda\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/ktransformers/ktransformers/local_chat.py\", line 197, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/ktransformers/local_chat.py\", line 124, in local_chat\n",
            "    optimize_and_load_gguf(model, optimize_config_path, gguf_path, config, default_device=device)\n",
            "  File \"/content/ktransformers/ktransformers/optimize/optimize.py\", line 131, in optimize_and_load_gguf\n",
            "    load_weights(module, weights_loader, device=default_device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 176, in load_weights\n",
            "    module.load()\n",
            "  File \"/content/ktransformers/ktransformers/operators/base_operator.py\", line 63, in load\n",
            "    utils.load_weights(child, self.gguf_loader, self.key+\".\")\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 176, in load_weights\n",
            "    module.load()\n",
            "  File \"/content/ktransformers/ktransformers/operators/base_operator.py\", line 63, in load\n",
            "    utils.load_weights(child, self.gguf_loader, self.key+\".\")\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 176, in load_weights\n",
            "    module.load()\n",
            "  File \"/content/ktransformers/ktransformers/operators/experts.py\", line 684, in load\n",
            "    self.generate_experts.load(w, warmup=warmup)\n",
            "  File \"/content/ktransformers/ktransformers/operators/experts.py\", line 245, in load\n",
            "    self.cpu_infer.submit(self.moe.warm_up())\n",
            "  File \"/content/ktransformers/ktransformers/operators/cpuinfer.py\", line 739, in submit\n",
            "    CPUInfer.cpuinfer.submit(task)\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'NoneType' object has no attribute 'submit'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ktransformers\n",
        "!python -m ktransformers.local_chat --model_path ./DeepSeek-V2-Lite --gguf_path ./DeepSeek-V2-Lite-Chat-GGUF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QclWahgbJf8B",
        "outputId": "e78b3360-f322-40f0-93be-16c39e82a07d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ktransformers\n",
            "no balance_serve\n",
            "2025-07-24 20:37:40,551 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-07-24 20:37:42.458884: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753389462.491753   11389 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753389462.501762   11389 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-24 20:37:42.532861: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\", line 424, in cached_files\n",
            "    hf_hub_download(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\n",
            "    validate_repo_id(arg_value)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\", line 160, in validate_repo_id\n",
            "    raise HFValidationError(\n",
            "huggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './DeepSeek-V2-Lite'.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/ktransformers/ktransformers/local_chat.py\", line 197, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/ktransformers/local_chat.py\", line 77, in local_chat\n",
            "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\", line 946, in from_pretrained\n",
            "    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\", line 778, in get_tokenizer_config\n",
            "    resolved_config_file = cached_file(\n",
            "                           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\", line 266, in cached_file\n",
            "    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\", line 470, in cached_files\n",
            "    resolved_files = [\n",
            "                     ^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\", line 471, in <listcomp>\n",
            "    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision) for filename in full_filenames\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\", line 134, in _get_cache_file_to_return\n",
            "    resolved_file = try_to_load_from_cache(path_or_repo_id, full_filename, cache_dir=cache_dir, revision=revision)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\n",
            "    validate_repo_id(arg_value)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\", line 160, in validate_repo_id\n",
            "    raise HFValidationError(\n",
            "huggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './DeepSeek-V2-Lite'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ktransformers\n",
        "!python -m ktransformers.local_chat --model_path ZZichen/DeepSeek-V2-Lite --gguf_path /content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF\n"
      ],
      "metadata": {
        "id": "2Xca2JOpJga5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numactl -N 1 -m 1 python ./ktransformers/local_chat.py --model_path <your model path> --gguf_path <your gguf path>  --prompt_file <your prompt txt file>  --cpu_infer 33 --max_new_tokens 1000\n",
        "<when you see chat, then press enter to load the text prompt_file>\n"
      ],
      "metadata": {
        "id": "ildYq5IrK13x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python ./ktransformers/local_chat.py --model_path ZZichen/DeepSeek-V2-Lite --gguf_path /content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF/DeepSeek-V2-Lite-Chat.Q4_K_M.gguf  --prompt_file /content/1.txt  --cpu_infer 3 --max_new_tokens 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vFIxRyoK5IA",
        "outputId": "86e52f5a-ae75-4100-aa18-1bb691985768"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-07-24 20:43:28,624 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-07-24 20:43:30.233543: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753389810.253758   12854 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753389810.260144   12854 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-24 20:43:30.281684: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "DeepseekV2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "using default_optimize_rule for DeepseekV2ForCausalLM\n",
            "falsh attn not found\n",
            "Injecting model as ktransformers.operators.models . KDeepseekV2Model\n",
            "Injecting model.embed_tokens as default\n",
            "Injecting model.layers as default\n",
            "Injecting model.layers.0 as default\n",
            "Injecting model.layers.0.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.0.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.0.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.0.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.0.mlp as default\n",
            "Injecting model.layers.0.mlp.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.mlp.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.mlp.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.mlp.act_fn as default\n",
            "Injecting model.layers.0.input_layernorm as default\n",
            "Injecting model.layers.0.post_attention_layernorm as default\n",
            "Injecting model.layers.1 as default\n",
            "Injecting model.layers.1.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.1.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.1.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.1.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.1.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.1.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.1.mlp.gate as default\n",
            "Injecting model.layers.1.mlp.shared_experts as default\n",
            "Injecting model.layers.1.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.1.input_layernorm as default\n",
            "Injecting model.layers.1.post_attention_layernorm as default\n",
            "Injecting model.layers.2 as default\n",
            "Injecting model.layers.2.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.2.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.2.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.2.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.2.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.2.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.2.mlp.gate as default\n",
            "Injecting model.layers.2.mlp.shared_experts as default\n",
            "Injecting model.layers.2.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.2.input_layernorm as default\n",
            "Injecting model.layers.2.post_attention_layernorm as default\n",
            "Injecting model.layers.3 as default\n",
            "Injecting model.layers.3.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.3.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.3.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.3.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.3.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.3.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.3.mlp.gate as default\n",
            "Injecting model.layers.3.mlp.shared_experts as default\n",
            "Injecting model.layers.3.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.3.input_layernorm as default\n",
            "Injecting model.layers.3.post_attention_layernorm as default\n",
            "Injecting model.layers.4 as default\n",
            "Injecting model.layers.4.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.4.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.4.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.4.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.4.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.4.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.4.mlp.gate as default\n",
            "Injecting model.layers.4.mlp.shared_experts as default\n",
            "Injecting model.layers.4.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.4.input_layernorm as default\n",
            "Injecting model.layers.4.post_attention_layernorm as default\n",
            "Injecting model.layers.5 as default\n",
            "Injecting model.layers.5.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.5.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.5.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.5.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.5.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.5.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.5.mlp.gate as default\n",
            "Injecting model.layers.5.mlp.shared_experts as default\n",
            "Injecting model.layers.5.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.5.input_layernorm as default\n",
            "Injecting model.layers.5.post_attention_layernorm as default\n",
            "Injecting model.layers.6 as default\n",
            "Injecting model.layers.6.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.6.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.6.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.6.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.6.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.6.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.6.mlp.gate as default\n",
            "Injecting model.layers.6.mlp.shared_experts as default\n",
            "Injecting model.layers.6.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.6.input_layernorm as default\n",
            "Injecting model.layers.6.post_attention_layernorm as default\n",
            "Injecting model.layers.7 as default\n",
            "Injecting model.layers.7.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.7.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.7.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.7.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.7.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.7.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.7.mlp.gate as default\n",
            "Injecting model.layers.7.mlp.shared_experts as default\n",
            "Injecting model.layers.7.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.7.input_layernorm as default\n",
            "Injecting model.layers.7.post_attention_layernorm as default\n",
            "Injecting model.layers.8 as default\n",
            "Injecting model.layers.8.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.8.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.8.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.8.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.8.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.8.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.8.mlp.gate as default\n",
            "Injecting model.layers.8.mlp.shared_experts as default\n",
            "Injecting model.layers.8.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.8.input_layernorm as default\n",
            "Injecting model.layers.8.post_attention_layernorm as default\n",
            "Injecting model.layers.9 as default\n",
            "Injecting model.layers.9.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.9.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.9.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.9.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.9.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.9.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.9.mlp.gate as default\n",
            "Injecting model.layers.9.mlp.shared_experts as default\n",
            "Injecting model.layers.9.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.9.input_layernorm as default\n",
            "Injecting model.layers.9.post_attention_layernorm as default\n",
            "Injecting model.layers.10 as default\n",
            "Injecting model.layers.10.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.10.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.10.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.10.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.10.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.10.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.10.mlp.gate as default\n",
            "Injecting model.layers.10.mlp.shared_experts as default\n",
            "Injecting model.layers.10.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.10.input_layernorm as default\n",
            "Injecting model.layers.10.post_attention_layernorm as default\n",
            "Injecting model.layers.11 as default\n",
            "Injecting model.layers.11.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.11.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.11.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.11.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.11.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.11.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.11.mlp.gate as default\n",
            "Injecting model.layers.11.mlp.shared_experts as default\n",
            "Injecting model.layers.11.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.11.input_layernorm as default\n",
            "Injecting model.layers.11.post_attention_layernorm as default\n",
            "Injecting model.layers.12 as default\n",
            "Injecting model.layers.12.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.12.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.12.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.12.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.12.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.12.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.12.mlp.gate as default\n",
            "Injecting model.layers.12.mlp.shared_experts as default\n",
            "Injecting model.layers.12.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.12.input_layernorm as default\n",
            "Injecting model.layers.12.post_attention_layernorm as default\n",
            "Injecting model.layers.13 as default\n",
            "Injecting model.layers.13.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.13.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.13.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.13.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.13.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.13.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.13.mlp.gate as default\n",
            "Injecting model.layers.13.mlp.shared_experts as default\n",
            "Injecting model.layers.13.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.13.input_layernorm as default\n",
            "Injecting model.layers.13.post_attention_layernorm as default\n",
            "Injecting model.layers.14 as default\n",
            "Injecting model.layers.14.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.14.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.14.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.14.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.14.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.14.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.14.mlp.gate as default\n",
            "Injecting model.layers.14.mlp.shared_experts as default\n",
            "Injecting model.layers.14.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.14.input_layernorm as default\n",
            "Injecting model.layers.14.post_attention_layernorm as default\n",
            "Injecting model.layers.15 as default\n",
            "Injecting model.layers.15.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.15.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.15.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.15.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.15.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.15.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.15.mlp.gate as default\n",
            "Injecting model.layers.15.mlp.shared_experts as default\n",
            "Injecting model.layers.15.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.15.input_layernorm as default\n",
            "Injecting model.layers.15.post_attention_layernorm as default\n",
            "Injecting model.layers.16 as default\n",
            "Injecting model.layers.16.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.16.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.16.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.16.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.16.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.16.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.16.mlp.gate as default\n",
            "Injecting model.layers.16.mlp.shared_experts as default\n",
            "Injecting model.layers.16.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.16.input_layernorm as default\n",
            "Injecting model.layers.16.post_attention_layernorm as default\n",
            "Injecting model.layers.17 as default\n",
            "Injecting model.layers.17.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.17.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.17.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.17.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.17.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.17.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.17.mlp.gate as default\n",
            "Injecting model.layers.17.mlp.shared_experts as default\n",
            "Injecting model.layers.17.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.17.input_layernorm as default\n",
            "Injecting model.layers.17.post_attention_layernorm as default\n",
            "Injecting model.layers.18 as default\n",
            "Injecting model.layers.18.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.18.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.18.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.18.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.18.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.18.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.18.mlp.gate as default\n",
            "Injecting model.layers.18.mlp.shared_experts as default\n",
            "Injecting model.layers.18.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.18.input_layernorm as default\n",
            "Injecting model.layers.18.post_attention_layernorm as default\n",
            "Injecting model.layers.19 as default\n",
            "Injecting model.layers.19.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.19.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.19.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.19.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.19.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.19.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.19.mlp.gate as default\n",
            "Injecting model.layers.19.mlp.shared_experts as default\n",
            "Injecting model.layers.19.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.19.input_layernorm as default\n",
            "Injecting model.layers.19.post_attention_layernorm as default\n",
            "Injecting model.layers.20 as default\n",
            "Injecting model.layers.20.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.20.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.20.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.20.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.20.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.20.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.20.mlp.gate as default\n",
            "Injecting model.layers.20.mlp.shared_experts as default\n",
            "Injecting model.layers.20.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.20.input_layernorm as default\n",
            "Injecting model.layers.20.post_attention_layernorm as default\n",
            "Injecting model.layers.21 as default\n",
            "Injecting model.layers.21.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.21.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.21.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.21.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.21.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.21.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.21.mlp.gate as default\n",
            "Injecting model.layers.21.mlp.shared_experts as default\n",
            "Injecting model.layers.21.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.21.input_layernorm as default\n",
            "Injecting model.layers.21.post_attention_layernorm as default\n",
            "Injecting model.layers.22 as default\n",
            "Injecting model.layers.22.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.22.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.22.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.22.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.22.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.22.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.22.mlp.gate as default\n",
            "Injecting model.layers.22.mlp.shared_experts as default\n",
            "Injecting model.layers.22.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.22.input_layernorm as default\n",
            "Injecting model.layers.22.post_attention_layernorm as default\n",
            "Injecting model.layers.23 as default\n",
            "Injecting model.layers.23.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.23.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.23.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.23.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.23.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.23.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.23.mlp.gate as default\n",
            "Injecting model.layers.23.mlp.shared_experts as default\n",
            "Injecting model.layers.23.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.23.input_layernorm as default\n",
            "Injecting model.layers.23.post_attention_layernorm as default\n",
            "Injecting model.layers.24 as default\n",
            "Injecting model.layers.24.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.24.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.24.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.24.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.24.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.24.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.24.mlp.gate as default\n",
            "Injecting model.layers.24.mlp.shared_experts as default\n",
            "Injecting model.layers.24.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.24.input_layernorm as default\n",
            "Injecting model.layers.24.post_attention_layernorm as default\n",
            "Injecting model.layers.25 as default\n",
            "Injecting model.layers.25.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.25.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.25.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.25.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.25.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.25.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.25.mlp.gate as default\n",
            "Injecting model.layers.25.mlp.shared_experts as default\n",
            "Injecting model.layers.25.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.25.input_layernorm as default\n",
            "Injecting model.layers.25.post_attention_layernorm as default\n",
            "Injecting model.layers.26 as default\n",
            "Injecting model.layers.26.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.26.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.26.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.26.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.26.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.26.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.26.mlp.gate as default\n",
            "Injecting model.layers.26.mlp.shared_experts as default\n",
            "Injecting model.layers.26.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.26.input_layernorm as default\n",
            "Injecting model.layers.26.post_attention_layernorm as default\n",
            "Injecting model.norm as default\n",
            "Injecting lm_head as ktransformers.operators.linear . KTransformersLinear\n",
            "loading model.embed_tokens.weight to cpu\n",
            "loading model.layers.0.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.0.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.0.input_layernorm.weight to cuda\n",
            "loading model.layers.0.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.1.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.1.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.1.mlp.gate.weight to cuda\n",
            "loading model.layers.1.input_layernorm.weight to cuda\n",
            "loading model.layers.1.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.2.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.2.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.2.mlp.gate.weight to cuda\n",
            "loading model.layers.2.input_layernorm.weight to cuda\n",
            "loading model.layers.2.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.3.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.3.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.3.mlp.gate.weight to cuda\n",
            "loading model.layers.3.input_layernorm.weight to cuda\n",
            "loading model.layers.3.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.4.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.4.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.4.mlp.gate.weight to cuda\n",
            "loading model.layers.4.input_layernorm.weight to cuda\n",
            "loading model.layers.4.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.5.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.5.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.5.mlp.gate.weight to cuda\n",
            "loading model.layers.5.input_layernorm.weight to cuda\n",
            "loading model.layers.5.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.6.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.6.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.6.mlp.gate.weight to cuda\n",
            "loading model.layers.6.input_layernorm.weight to cuda\n",
            "loading model.layers.6.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.7.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.7.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.7.mlp.gate.weight to cuda\n",
            "loading model.layers.7.input_layernorm.weight to cuda\n",
            "loading model.layers.7.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.8.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.8.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.8.mlp.gate.weight to cuda\n",
            "loading model.layers.8.input_layernorm.weight to cuda\n",
            "loading model.layers.8.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.9.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.9.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.9.mlp.gate.weight to cuda\n",
            "loading model.layers.9.input_layernorm.weight to cuda\n",
            "loading model.layers.9.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.10.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.10.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.10.mlp.gate.weight to cuda\n",
            "loading model.layers.10.input_layernorm.weight to cuda\n",
            "loading model.layers.10.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.11.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.11.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.11.mlp.gate.weight to cuda\n",
            "loading model.layers.11.input_layernorm.weight to cuda\n",
            "loading model.layers.11.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.12.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.12.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.12.mlp.gate.weight to cuda\n",
            "loading model.layers.12.input_layernorm.weight to cuda\n",
            "loading model.layers.12.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.13.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.13.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.13.mlp.gate.weight to cuda\n",
            "loading model.layers.13.input_layernorm.weight to cuda\n",
            "loading model.layers.13.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.14.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.14.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.14.mlp.gate.weight to cuda\n",
            "loading model.layers.14.input_layernorm.weight to cuda\n",
            "loading model.layers.14.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.15.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.15.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.15.mlp.gate.weight to cuda\n",
            "loading model.layers.15.input_layernorm.weight to cuda\n",
            "loading model.layers.15.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.16.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.16.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.16.mlp.gate.weight to cuda\n",
            "loading model.layers.16.input_layernorm.weight to cuda\n",
            "loading model.layers.16.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.17.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.17.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.17.mlp.gate.weight to cuda\n",
            "loading model.layers.17.input_layernorm.weight to cuda\n",
            "loading model.layers.17.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.18.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.18.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.18.mlp.gate.weight to cuda\n",
            "loading model.layers.18.input_layernorm.weight to cuda\n",
            "loading model.layers.18.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.19.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.19.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.19.mlp.gate.weight to cuda\n",
            "loading model.layers.19.input_layernorm.weight to cuda\n",
            "loading model.layers.19.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.20.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.20.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.20.mlp.gate.weight to cuda\n",
            "loading model.layers.20.input_layernorm.weight to cuda\n",
            "loading model.layers.20.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.21.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.21.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.21.mlp.gate.weight to cuda\n",
            "loading model.layers.21.input_layernorm.weight to cuda\n",
            "loading model.layers.21.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.22.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.22.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.22.mlp.gate.weight to cuda\n",
            "loading model.layers.22.input_layernorm.weight to cuda\n",
            "loading model.layers.22.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.23.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.23.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.23.mlp.gate.weight to cuda\n",
            "loading model.layers.23.input_layernorm.weight to cuda\n",
            "loading model.layers.23.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.24.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.24.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.24.mlp.gate.weight to cuda\n",
            "loading model.layers.24.input_layernorm.weight to cuda\n",
            "loading model.layers.24.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.25.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.25.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.25.mlp.gate.weight to cuda\n",
            "loading model.layers.25.input_layernorm.weight to cuda\n",
            "loading model.layers.25.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.26.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.26.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.26.mlp.gate.weight to cuda\n",
            "loading model.layers.26.input_layernorm.weight to cuda\n",
            "loading model.layers.26.post_attention_layernorm.weight to cuda\n",
            "loading model.norm.weight to cuda\n",
            "generation_config.json: 100% 181/181 [00:00<00:00, 1.01MB/s]\n",
            "\u001b[H\u001b[2JChat: hi\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ktransformers/./ktransformers/local_chat.py\", line 197, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/./ktransformers/local_chat.py\", line 191, in local_chat\n",
            "    generated = prefill_and_generate(\n",
            "                ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/./ktransformers/util/utils.py\", line 334, in prefill_and_generate\n",
            "    logits = chunk_prefill(inputs[:, chunk_start:chunk_end], cache_position[chunk_start:chunk_end], past_key_values)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/./ktransformers/util/utils.py\", line 286, in chunk_prefill\n",
            "    logits = model(\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/./ktransformers/models/modeling_deepseek.py\", line 1732, in forward\n",
            "    outputs = self.model(\n",
            "              ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/./ktransformers/operators/models.py\", line 751, in forward\n",
            "    layer_outputs = decoder_layer(\n",
            "                    ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/./ktransformers/models/modeling_deepseek.py\", line 1239, in forward\n",
            "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
            "                                                          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/./ktransformers/operators/attention.py\", line 711, in forward\n",
            "    return self.forward_windows(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/./ktransformers/operators/attention.py\", line 543, in forward_windows\n",
            "    return self.forward_chunck(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/./ktransformers/operators/attention.py\", line 100, in forward_chunck\n",
            "    compressed_kv = self.kv_a_proj_with_mqa(hidden_states)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/./ktransformers/operators/linear.py\", line 923, in forward\n",
            "    y = self.generate_linear.forward(x, bsz_tensor)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/./ktransformers/operators/linear.py\", line 670, in forward\n",
            "    padding_input[:,:self.orin_in_features] = x\n",
            "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: CUDA error: invalid device function\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_LAUNCH_BLOCKING=1 python ./ktransformers/local_chat.py --model_path ZZichen/DeepSeek-V2-Lite --gguf_path /content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF/DeepSeek-V2-Lite-Chat.Q4_K_M.gguf  --prompt_file /content/1.txt  --cpu_infer 3 --max_new_tokens 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfZS2d0jLTAC",
        "outputId": "259c0de2-d2dc-4bbe-a605-e67b0939102a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-07-24 20:48:14,209 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-07-24 20:48:16.467616: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753390096.500293   14049 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753390096.510128   14049 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-24 20:48:16.540446: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "DeepseekV2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "using default_optimize_rule for DeepseekV2ForCausalLM\n",
            "falsh attn not found\n",
            "Injecting model as ktransformers.operators.models . KDeepseekV2Model\n",
            "Injecting model.embed_tokens as default\n",
            "Injecting model.layers as default\n",
            "Injecting model.layers.0 as default\n",
            "Injecting model.layers.0.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.0.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.0.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.0.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.0.mlp as default\n",
            "Injecting model.layers.0.mlp.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.mlp.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.mlp.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.mlp.act_fn as default\n",
            "Injecting model.layers.0.input_layernorm as default\n",
            "Injecting model.layers.0.post_attention_layernorm as default\n",
            "Injecting model.layers.1 as default\n",
            "Injecting model.layers.1.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.1.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.1.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.1.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.1.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.1.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.1.mlp.gate as default\n",
            "Injecting model.layers.1.mlp.shared_experts as default\n",
            "Injecting model.layers.1.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.1.input_layernorm as default\n",
            "Injecting model.layers.1.post_attention_layernorm as default\n",
            "Injecting model.layers.2 as default\n",
            "Injecting model.layers.2.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.2.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.2.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.2.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.2.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.2.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.2.mlp.gate as default\n",
            "Injecting model.layers.2.mlp.shared_experts as default\n",
            "Injecting model.layers.2.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.2.input_layernorm as default\n",
            "Injecting model.layers.2.post_attention_layernorm as default\n",
            "Injecting model.layers.3 as default\n",
            "Injecting model.layers.3.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.3.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.3.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.3.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.3.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.3.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.3.mlp.gate as default\n",
            "Injecting model.layers.3.mlp.shared_experts as default\n",
            "Injecting model.layers.3.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.3.input_layernorm as default\n",
            "Injecting model.layers.3.post_attention_layernorm as default\n",
            "Injecting model.layers.4 as default\n",
            "Injecting model.layers.4.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.4.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.4.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.4.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.4.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.4.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.4.mlp.gate as default\n",
            "Injecting model.layers.4.mlp.shared_experts as default\n",
            "Injecting model.layers.4.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.4.input_layernorm as default\n",
            "Injecting model.layers.4.post_attention_layernorm as default\n",
            "Injecting model.layers.5 as default\n",
            "Injecting model.layers.5.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.5.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.5.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.5.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.5.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.5.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.5.mlp.gate as default\n",
            "Injecting model.layers.5.mlp.shared_experts as default\n",
            "Injecting model.layers.5.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.5.input_layernorm as default\n",
            "Injecting model.layers.5.post_attention_layernorm as default\n",
            "Injecting model.layers.6 as default\n",
            "Injecting model.layers.6.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.6.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.6.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.6.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.6.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.6.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.6.mlp.gate as default\n",
            "Injecting model.layers.6.mlp.shared_experts as default\n",
            "Injecting model.layers.6.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.6.input_layernorm as default\n",
            "Injecting model.layers.6.post_attention_layernorm as default\n",
            "Injecting model.layers.7 as default\n",
            "Injecting model.layers.7.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.7.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.7.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.7.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.7.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.7.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.7.mlp.gate as default\n",
            "Injecting model.layers.7.mlp.shared_experts as default\n",
            "Injecting model.layers.7.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.7.input_layernorm as default\n",
            "Injecting model.layers.7.post_attention_layernorm as default\n",
            "Injecting model.layers.8 as default\n",
            "Injecting model.layers.8.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.8.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.8.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.8.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.8.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.8.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.8.mlp.gate as default\n",
            "Injecting model.layers.8.mlp.shared_experts as default\n",
            "Injecting model.layers.8.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.8.input_layernorm as default\n",
            "Injecting model.layers.8.post_attention_layernorm as default\n",
            "Injecting model.layers.9 as default\n",
            "Injecting model.layers.9.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.9.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.9.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.9.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.9.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.9.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.9.mlp.gate as default\n",
            "Injecting model.layers.9.mlp.shared_experts as default\n",
            "Injecting model.layers.9.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.9.input_layernorm as default\n",
            "Injecting model.layers.9.post_attention_layernorm as default\n",
            "Injecting model.layers.10 as default\n",
            "Injecting model.layers.10.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.10.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.10.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.10.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.10.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.10.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.10.mlp.gate as default\n",
            "Injecting model.layers.10.mlp.shared_experts as default\n",
            "Injecting model.layers.10.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.10.input_layernorm as default\n",
            "Injecting model.layers.10.post_attention_layernorm as default\n",
            "Injecting model.layers.11 as default\n",
            "Injecting model.layers.11.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.11.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.11.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.11.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.11.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.11.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.11.mlp.gate as default\n",
            "Injecting model.layers.11.mlp.shared_experts as default\n",
            "Injecting model.layers.11.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.11.input_layernorm as default\n",
            "Injecting model.layers.11.post_attention_layernorm as default\n",
            "Injecting model.layers.12 as default\n",
            "Injecting model.layers.12.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.12.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.12.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.12.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.12.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.12.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.12.mlp.gate as default\n",
            "Injecting model.layers.12.mlp.shared_experts as default\n",
            "Injecting model.layers.12.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.12.input_layernorm as default\n",
            "Injecting model.layers.12.post_attention_layernorm as default\n",
            "Injecting model.layers.13 as default\n",
            "Injecting model.layers.13.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.13.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.13.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.13.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.13.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.13.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.13.mlp.gate as default\n",
            "Injecting model.layers.13.mlp.shared_experts as default\n",
            "Injecting model.layers.13.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.13.input_layernorm as default\n",
            "Injecting model.layers.13.post_attention_layernorm as default\n",
            "Injecting model.layers.14 as default\n",
            "Injecting model.layers.14.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.14.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.14.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.14.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.14.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.14.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.14.mlp.gate as default\n",
            "Injecting model.layers.14.mlp.shared_experts as default\n",
            "Injecting model.layers.14.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.14.input_layernorm as default\n",
            "Injecting model.layers.14.post_attention_layernorm as default\n",
            "Injecting model.layers.15 as default\n",
            "Injecting model.layers.15.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.15.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.15.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.15.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.15.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.15.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.15.mlp.gate as default\n",
            "Injecting model.layers.15.mlp.shared_experts as default\n",
            "Injecting model.layers.15.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.15.input_layernorm as default\n",
            "Injecting model.layers.15.post_attention_layernorm as default\n",
            "Injecting model.layers.16 as default\n",
            "Injecting model.layers.16.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.16.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.16.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.16.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.16.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.16.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.16.mlp.gate as default\n",
            "Injecting model.layers.16.mlp.shared_experts as default\n",
            "Injecting model.layers.16.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.16.input_layernorm as default\n",
            "Injecting model.layers.16.post_attention_layernorm as default\n",
            "Injecting model.layers.17 as default\n",
            "Injecting model.layers.17.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.17.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.17.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.17.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.17.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.17.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.17.mlp.gate as default\n",
            "Injecting model.layers.17.mlp.shared_experts as default\n",
            "Injecting model.layers.17.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.17.input_layernorm as default\n",
            "Injecting model.layers.17.post_attention_layernorm as default\n",
            "Injecting model.layers.18 as default\n",
            "Injecting model.layers.18.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.18.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.18.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.18.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.18.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.18.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.18.mlp.gate as default\n",
            "Injecting model.layers.18.mlp.shared_experts as default\n",
            "Injecting model.layers.18.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.18.input_layernorm as default\n",
            "Injecting model.layers.18.post_attention_layernorm as default\n",
            "Injecting model.layers.19 as default\n",
            "Injecting model.layers.19.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.19.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.19.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.19.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.19.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.19.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.19.mlp.gate as default\n",
            "Injecting model.layers.19.mlp.shared_experts as default\n",
            "Injecting model.layers.19.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.19.input_layernorm as default\n",
            "Injecting model.layers.19.post_attention_layernorm as default\n",
            "Injecting model.layers.20 as default\n",
            "Injecting model.layers.20.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.20.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.20.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.20.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.20.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.20.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.20.mlp.gate as default\n",
            "Injecting model.layers.20.mlp.shared_experts as default\n",
            "Injecting model.layers.20.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.20.input_layernorm as default\n",
            "Injecting model.layers.20.post_attention_layernorm as default\n",
            "Injecting model.layers.21 as default\n",
            "Injecting model.layers.21.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.21.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.21.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.21.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.21.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.21.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.21.mlp.gate as default\n",
            "Injecting model.layers.21.mlp.shared_experts as default\n",
            "Injecting model.layers.21.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.21.input_layernorm as default\n",
            "Injecting model.layers.21.post_attention_layernorm as default\n",
            "Injecting model.layers.22 as default\n",
            "Injecting model.layers.22.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.22.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.22.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.22.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.22.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.22.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.22.mlp.gate as default\n",
            "Injecting model.layers.22.mlp.shared_experts as default\n",
            "Injecting model.layers.22.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.22.input_layernorm as default\n",
            "Injecting model.layers.22.post_attention_layernorm as default\n",
            "Injecting model.layers.23 as default\n",
            "Injecting model.layers.23.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.23.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.23.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.23.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.23.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.23.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.23.mlp.gate as default\n",
            "Injecting model.layers.23.mlp.shared_experts as default\n",
            "Injecting model.layers.23.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.23.input_layernorm as default\n",
            "Injecting model.layers.23.post_attention_layernorm as default\n",
            "Injecting model.layers.24 as default\n",
            "Injecting model.layers.24.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.24.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.24.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.24.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.24.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.24.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.24.mlp.gate as default\n",
            "Injecting model.layers.24.mlp.shared_experts as default\n",
            "Injecting model.layers.24.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.24.input_layernorm as default\n",
            "Injecting model.layers.24.post_attention_layernorm as default\n",
            "Injecting model.layers.25 as default\n",
            "Injecting model.layers.25.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.25.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.25.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.25.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.25.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.25.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.25.mlp.gate as default\n",
            "Injecting model.layers.25.mlp.shared_experts as default\n",
            "Injecting model.layers.25.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.25.input_layernorm as default\n",
            "Injecting model.layers.25.post_attention_layernorm as default\n",
            "Injecting model.layers.26 as default\n",
            "Injecting model.layers.26.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.26.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.26.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.26.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.26.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.26.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.26.mlp.gate as default\n",
            "Injecting model.layers.26.mlp.shared_experts as default\n",
            "Injecting model.layers.26.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.26.input_layernorm as default\n",
            "Injecting model.layers.26.post_attention_layernorm as default\n",
            "Injecting model.norm as default\n",
            "Injecting lm_head as ktransformers.operators.linear . KTransformersLinear\n",
            "loading model.embed_tokens.weight to cpu\n",
            "loading model.layers.0.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.0.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.0.input_layernorm.weight to cuda\n",
            "loading model.layers.0.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.1.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.1.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.1.mlp.gate.weight to cuda\n",
            "loading model.layers.1.input_layernorm.weight to cuda\n",
            "loading model.layers.1.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.2.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.2.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.2.mlp.gate.weight to cuda\n",
            "loading model.layers.2.input_layernorm.weight to cuda\n",
            "loading model.layers.2.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.3.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.3.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.3.mlp.gate.weight to cuda\n",
            "loading model.layers.3.input_layernorm.weight to cuda\n",
            "loading model.layers.3.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.4.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.4.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.4.mlp.gate.weight to cuda\n",
            "loading model.layers.4.input_layernorm.weight to cuda\n",
            "loading model.layers.4.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.5.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.5.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.5.mlp.gate.weight to cuda\n",
            "loading model.layers.5.input_layernorm.weight to cuda\n",
            "loading model.layers.5.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.6.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.6.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.6.mlp.gate.weight to cuda\n",
            "loading model.layers.6.input_layernorm.weight to cuda\n",
            "loading model.layers.6.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.7.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.7.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.7.mlp.gate.weight to cuda\n",
            "loading model.layers.7.input_layernorm.weight to cuda\n",
            "loading model.layers.7.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.8.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.8.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.8.mlp.gate.weight to cuda\n",
            "loading model.layers.8.input_layernorm.weight to cuda\n",
            "loading model.layers.8.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.9.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.9.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.9.mlp.gate.weight to cuda\n",
            "loading model.layers.9.input_layernorm.weight to cuda\n",
            "loading model.layers.9.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.10.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.10.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.10.mlp.gate.weight to cuda\n",
            "loading model.layers.10.input_layernorm.weight to cuda\n",
            "loading model.layers.10.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.11.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.11.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.11.mlp.gate.weight to cuda\n",
            "loading model.layers.11.input_layernorm.weight to cuda\n",
            "loading model.layers.11.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.12.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.12.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.12.mlp.gate.weight to cuda\n",
            "loading model.layers.12.input_layernorm.weight to cuda\n",
            "loading model.layers.12.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.13.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.13.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.13.mlp.gate.weight to cuda\n",
            "loading model.layers.13.input_layernorm.weight to cuda\n",
            "loading model.layers.13.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.14.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.14.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.14.mlp.gate.weight to cuda\n",
            "loading model.layers.14.input_layernorm.weight to cuda\n",
            "loading model.layers.14.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.15.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.15.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.15.mlp.gate.weight to cuda\n",
            "loading model.layers.15.input_layernorm.weight to cuda\n",
            "loading model.layers.15.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.16.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.16.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.16.mlp.gate.weight to cuda\n",
            "loading model.layers.16.input_layernorm.weight to cuda\n",
            "loading model.layers.16.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.17.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.17.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.17.mlp.gate.weight to cuda\n",
            "loading model.layers.17.input_layernorm.weight to cuda\n",
            "loading model.layers.17.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.18.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.18.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.18.mlp.gate.weight to cuda\n",
            "loading model.layers.18.input_layernorm.weight to cuda\n",
            "loading model.layers.18.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.19.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.19.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.19.mlp.gate.weight to cuda\n",
            "loading model.layers.19.input_layernorm.weight to cuda\n",
            "loading model.layers.19.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.20.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.20.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.20.mlp.gate.weight to cuda\n",
            "loading model.layers.20.input_layernorm.weight to cuda\n",
            "loading model.layers.20.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.21.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.21.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.21.mlp.gate.weight to cuda\n",
            "loading model.layers.21.input_layernorm.weight to cuda\n",
            "loading model.layers.21.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.22.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.22.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.22.mlp.gate.weight to cuda\n",
            "loading model.layers.22.input_layernorm.weight to cuda\n",
            "loading model.layers.22.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.23.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.23.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.23.mlp.gate.weight to cuda\n",
            "loading model.layers.23.input_layernorm.weight to cuda\n",
            "loading model.layers.23.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.24.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.24.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.24.mlp.gate.weight to cuda\n",
            "loading model.layers.24.input_layernorm.weight to cuda\n",
            "loading model.layers.24.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.25.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.25.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.25.mlp.gate.weight to cuda\n",
            "loading model.layers.25.input_layernorm.weight to cuda\n",
            "loading model.layers.25.post_attention_layernorm.weight to cuda\n",
            "loading model.layers.26.self_attn.kv_a_layernorm.weight to cuda\n",
            "loading model.layers.26.self_attn.kv_b_proj.weight to cuda\n",
            "loading model.layers.26.mlp.gate.weight to cuda\n",
            "loading model.layers.26.input_layernorm.weight to cuda\n",
            "loading model.layers.26.post_attention_layernorm.weight to cuda\n",
            "loading model.norm.weight to cuda\n",
            "\u001b[H\u001b[2JChat: اه\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ktransformers/./ktransformers/local_chat.py\", line 197, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/./ktransformers/local_chat.py\", line 191, in local_chat\n",
            "    generated = prefill_and_generate(\n",
            "                ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/./ktransformers/util/utils.py\", line 334, in prefill_and_generate\n",
            "    logits = chunk_prefill(inputs[:, chunk_start:chunk_end], cache_position[chunk_start:chunk_end], past_key_values)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/./ktransformers/util/utils.py\", line 286, in chunk_prefill\n",
            "    logits = model(\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/./ktransformers/models/modeling_deepseek.py\", line 1732, in forward\n",
            "    outputs = self.model(\n",
            "              ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/./ktransformers/operators/models.py\", line 751, in forward\n",
            "    layer_outputs = decoder_layer(\n",
            "                    ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/./ktransformers/models/modeling_deepseek.py\", line 1239, in forward\n",
            "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
            "                                                          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/./ktransformers/operators/attention.py\", line 711, in forward\n",
            "    return self.forward_windows(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/./ktransformers/operators/attention.py\", line 543, in forward_windows\n",
            "    return self.forward_chunck(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/./ktransformers/operators/attention.py\", line 100, in forward_chunck\n",
            "    compressed_kv = self.kv_a_proj_with_mqa(hidden_states)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/./ktransformers/operators/linear.py\", line 923, in forward\n",
            "    y = self.generate_linear.forward(x, bsz_tensor)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/./ktransformers/operators/linear.py\", line 670, in forward\n",
            "    padding_input[:,:self.orin_in_features] = x\n",
            "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: CUDA error: invalid device function\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall llama-cpp-python -y\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0rNhD7ZNMzq",
        "outputId": "afd93a7a-0061-4d66-c0db-61a589b5978f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.14.tar.gz (51.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 MB\u001b[0m \u001b[31m307.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.14.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m264.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for llama-cpp-python \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for llama-cpp-python\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build llama-cpp-python\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (llama-cpp-python)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# الخطوة 1: تحديث وتثبيت أدوات البناء ومجموعة أدوات CUDA\n",
        "!apt-get update\n",
        "!apt-get install -y build-essential nvidia-cuda-toolkit\n",
        "\n",
        "# الخطوة 2: الآن قم بتثبيت المكتبة مع تفعيل دعم CUDA\n",
        "# نبدأ بإزالة أي محاولة تثبيت فاشلة سابقة لضمان البدء من الصفر\n",
        "!pip uninstall llama-cpp-python -y\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgP8pR5VNn2T",
        "outputId": "df6e7114-d7ad-49e2-ac6a-d9996ba600aa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,773 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,471 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,269 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,574 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,160 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,156 kB]\n",
            "Fetched 21.8 MB in 5s (4,499 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "The following additional packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libaccinj64-11.5 libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libbabeltrace1 libcub-dev libcublas11 libcublaslt11\n",
            "  libcudart11.0 libcufft10 libcufftw10 libcuinj64-11.5 libcupti-dev\n",
            "  libcupti-doc libcupti11.5 libcurand10 libcusolver11 libcusolvermg11\n",
            "  libcusparse11 libdebuginfod-common libdebuginfod1 libegl-dev libgail-common\n",
            "  libgail18 libgl-dev libgl1-mesa-dev libgles-dev libgles1 libglvnd-core-dev\n",
            "  libglvnd-dev libglx-dev libgtk2.0-0 libgtk2.0-bin libgtk2.0-common libipt2\n",
            "  libnppc11 libnppial11 libnppicc11 libnppidei11 libnppif11 libnppig11\n",
            "  libnppim11 libnppist11 libnppisu11 libnppitc11 libnpps11 libnvblas11\n",
            "  libnvidia-compute-495 libnvidia-compute-510 libnvidia-compute-535\n",
            "  libnvidia-ml-dev libnvjpeg11 libnvrtc-builtins11.5 libnvrtc11.2\n",
            "  libnvtoolsext1 libnvvm4 libopengl-dev librsvg2-common\n",
            "  libsource-highlight-common libsource-highlight4v5 libthrust-dev libvdpau-dev\n",
            "  libxtst6 libxxf86dga1 node-html5shiv nvidia-cuda-dev nvidia-cuda-gdb\n",
            "  nvidia-cuda-toolkit-doc nvidia-profiler nvidia-visual-profiler openjdk-8-jre\n",
            "  openjdk-8-jre-headless x11-utils\n",
            "Suggested packages:\n",
            "  gvfs libvdpau-doc nodejs libnss-mdns fonts-nanum fonts-ipafont-gothic\n",
            "  fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "  mesa-utils\n",
            "Recommended packages:\n",
            "  libnvcuvid1 nsight-compute nsight-systems\n",
            "The following NEW packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libaccinj64-11.5 libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libbabeltrace1 libcub-dev libcublas11 libcublaslt11\n",
            "  libcudart11.0 libcufft10 libcufftw10 libcuinj64-11.5 libcupti-dev\n",
            "  libcupti-doc libcupti11.5 libcurand10 libcusolver11 libcusolvermg11\n",
            "  libcusparse11 libdebuginfod-common libdebuginfod1 libegl-dev libgail-common\n",
            "  libgail18 libgl-dev libgl1-mesa-dev libgles-dev libgles1 libglvnd-core-dev\n",
            "  libglvnd-dev libglx-dev libgtk2.0-0 libgtk2.0-bin libgtk2.0-common libipt2\n",
            "  libnppc11 libnppial11 libnppicc11 libnppidei11 libnppif11 libnppig11\n",
            "  libnppim11 libnppist11 libnppisu11 libnppitc11 libnpps11 libnvblas11\n",
            "  libnvidia-compute-495 libnvidia-compute-510 libnvidia-compute-535\n",
            "  libnvidia-ml-dev libnvjpeg11 libnvrtc-builtins11.5 libnvrtc11.2\n",
            "  libnvtoolsext1 libnvvm4 libopengl-dev librsvg2-common\n",
            "  libsource-highlight-common libsource-highlight4v5 libthrust-dev libvdpau-dev\n",
            "  libxtst6 libxxf86dga1 node-html5shiv nvidia-cuda-dev nvidia-cuda-gdb\n",
            "  nvidia-cuda-toolkit nvidia-cuda-toolkit-doc nvidia-profiler\n",
            "  nvidia-visual-profiler openjdk-8-jre openjdk-8-jre-headless x11-utils\n",
            "0 upgraded, 75 newly installed, 0 to remove and 36 not upgraded.\n",
            "Need to get 1,540 MB of archives.\n",
            "After this operation, 4,233 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdebuginfod-common all 0.186-1ubuntu0.1 [7,996 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-compute-535 535.261.03-0ubuntu1 [36.9 MB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libcupti11.5 amd64 11.5.114~11.5.1-1ubuntu1 [7,696 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libaccinj64-11.5 amd64 11.5.114~11.5.1-1ubuntu1 [845 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcub-dev all 1.15.0-3 [217 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libcublaslt11 amd64 11.7.4.6~11.5.1-1ubuntu1 [148 MB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libcublas11 amd64 11.7.4.6~11.5.1-1ubuntu1 [78.2 MB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libcudart11.0 amd64 11.5.117~11.5.1-1ubuntu1 [178 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libcufft10 amd64 11.1.1+~10.6.0.107~11.5.1-1ubuntu1 [70.4 MB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libcufftw10 amd64 11.1.1+~10.6.0.107~11.5.1-1ubuntu1 [211 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 libnvidia-compute-510 amd64 525.147.05-0ubuntu2.22.04.1 [7,310 B]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 libnvidia-compute-495 amd64 510.108.03-0ubuntu0.22.04.1 [7,378 B]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libcuinj64-11.5 amd64 11.5.114~11.5.1-1ubuntu1 [1,004 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libcurand10 amd64 11.1.1+~10.2.7.107~11.5.1-1ubuntu1 [41.8 MB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libcusolver11 amd64 11.3.2.107~11.5.1-1ubuntu1 [31.3 MB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libcusolvermg11 amd64 11.3.2.107~11.5.1-1ubuntu1 [17.8 MB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libcusparse11 amd64 11.7.0.107~11.5.1-1ubuntu1 [96.2 MB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdebuginfod1 amd64 0.186-1ubuntu0.1 [12.8 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-common all 2.24.33-2ubuntu2.1 [125 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-0 amd64 2.24.33-2ubuntu2.1 [2,038 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail18 amd64 2.24.33-2ubuntu2.1 [15.9 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail-common amd64 2.24.33-2ubuntu2.1 [132 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-bin amd64 2.24.33-2ubuntu2.1 [7,936 B]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu jammy/main amd64 libipt2 amd64 2.0.5-1 [46.4 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libnppc11 amd64 11.5.1.107~11.5.1-1ubuntu1 [430 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libnppial11 amd64 11.5.1.107~11.5.1-1ubuntu1 [5,234 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libnppicc11 amd64 11.5.1.107~11.5.1-1ubuntu1 [2,373 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libnppidei11 amd64 11.5.1.107~11.5.1-1ubuntu1 [2,587 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libnppif11 amd64 11.5.1.107~11.5.1-1ubuntu1 [33.8 MB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libnppig11 amd64 11.5.1.107~11.5.1-1ubuntu1 [14.5 MB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libnppim11 amd64 11.5.1.107~11.5.1-1ubuntu1 [3,037 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libnppist11 amd64 11.5.1.107~11.5.1-1ubuntu1 [13.7 MB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libnppisu11 amd64 11.5.1.107~11.5.1-1ubuntu1 [177 kB]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libnppitc11 amd64 11.5.1.107~11.5.1-1ubuntu1 [1,292 kB]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libnpps11 amd64 11.5.1.107~11.5.1-1ubuntu1 [7,116 kB]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libnvblas11 amd64 11.7.4.6~11.5.1-1ubuntu1 [191 kB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libnvidia-ml-dev amd64 11.5.50~11.5.1-1ubuntu1 [69.1 kB]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libnvjpeg11 amd64 11.5.4.107~11.5.1-1ubuntu1 [1,858 kB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libnvrtc-builtins11.5 amd64 11.5.119~11.5.1-1ubuntu1 [116 kB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libnvrtc11.2 amd64 11.5.119~11.5.1-1ubuntu1 [15.7 MB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libnvvm4 amd64 11.5.119~11.5.1-1ubuntu1 [8,675 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 librsvg2-common amd64 2.52.5+dfsg-3ubuntu0.2 [17.7 kB]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsource-highlight-common all 3.1.9-4.1build2 [64.5 kB]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsource-highlight4v5 amd64 3.1.9-4.1build2 [207 kB]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu jammy/main amd64 libvdpau-dev amd64 1.4-3build2 [38.7 kB]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-html5shiv all 3.7.3+dfsg-4 [13.6 kB]\n",
            "Get:60 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 nvidia-cuda-toolkit-doc all 11.5.1-1ubuntu1 [6,263 kB]\n",
            "Get:61 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-8-jre-headless amd64 8u452-ga~us1-0ubuntu1~22.04 [30.8 MB]\n",
            "Get:62 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-8-jre amd64 8u452-ga~us1-0ubuntu1~22.04 [75.3 kB]\n",
            "Get:63 http://archive.ubuntu.com/ubuntu jammy/main amd64 libbabeltrace1 amd64 1.5.8-2build1 [160 kB]\n",
            "Get:64 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libcupti-dev amd64 11.5.114~11.5.1-1ubuntu1 [7,915 kB]\n",
            "Get:65 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libcupti-doc all 11.5.114~11.5.1-1ubuntu1 [2,373 kB]\n",
            "Get:66 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n",
            "Get:67 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n",
            "Get:68 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dev amd64 23.2.1-1ubuntu3.1~22.04.3 [6,848 B]\n",
            "Get:69 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libnvtoolsext1 amd64 11.5.114~11.5.1-1ubuntu1 [28.8 kB]\n",
            "Get:70 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libthrust-dev all 1.15.0-1 [423 kB]\n",
            "Get:71 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 nvidia-cuda-dev amd64 11.5.1-1ubuntu1 [667 MB]\n",
            "Get:72 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 nvidia-cuda-gdb amd64 11.5.114~11.5.1-1ubuntu1 [3,404 kB]\n",
            "Get:73 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 nvidia-profiler amd64 11.5.114~11.5.1-1ubuntu1 [1,732 kB]\n",
            "Get:74 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 nvidia-cuda-toolkit amd64 11.5.1-1ubuntu1 [62.8 MB]\n",
            "Get:75 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 nvidia-visual-profiler amd64 11.5.114~11.5.1-1ubuntu1 [108 MB]\n",
            "Fetched 1,540 MB in 1min 16s (20.3 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package libdebuginfod-common.\n",
            "(Reading database ... 126360 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libdebuginfod-common_0.186-1ubuntu0.1_all.deb ...\n",
            "Unpacking libdebuginfod-common (0.186-1ubuntu0.1) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../01-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../02-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libcupti11.5:amd64.\n",
            "Preparing to unpack .../03-libcupti11.5_11.5.114~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcupti11.5:amd64 (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libaccinj64-11.5:amd64.\n",
            "Preparing to unpack .../04-libaccinj64-11.5_11.5.114~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libaccinj64-11.5:amd64 (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../05-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../06-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../07-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../08-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../09-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libcub-dev.\n",
            "Preparing to unpack .../10-libcub-dev_1.15.0-3_all.deb ...\n",
            "Unpacking libcub-dev (1.15.0-3) ...\n",
            "Selecting previously unselected package libcublaslt11:amd64.\n",
            "Preparing to unpack .../11-libcublaslt11_11.7.4.6~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcublaslt11:amd64 (11.7.4.6~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libcublas11:amd64.\n",
            "Preparing to unpack .../12-libcublas11_11.7.4.6~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcublas11:amd64 (11.7.4.6~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libcudart11.0:amd64.\n",
            "Preparing to unpack .../13-libcudart11.0_11.5.117~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcudart11.0:amd64 (11.5.117~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libcufft10:amd64.\n",
            "Preparing to unpack .../14-libcufft10_11.1.1+~10.6.0.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcufft10:amd64 (11.1.1+~10.6.0.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libcufftw10:amd64.\n",
            "Preparing to unpack .../15-libcufftw10_11.1.1+~10.6.0.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcufftw10:amd64 (11.1.1+~10.6.0.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-compute-535:amd64.\n",
            "Preparing to unpack .../16-libnvidia-compute-535_535.261.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-compute-535:amd64 (535.261.03-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-compute-510:amd64.\n",
            "Preparing to unpack .../17-libnvidia-compute-510_525.147.05-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking libnvidia-compute-510:amd64 (525.147.05-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package libnvidia-compute-495:amd64.\n",
            "Preparing to unpack .../18-libnvidia-compute-495_510.108.03-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libnvidia-compute-495:amd64 (510.108.03-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libcuinj64-11.5:amd64.\n",
            "Preparing to unpack .../19-libcuinj64-11.5_11.5.114~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcuinj64-11.5:amd64 (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libcurand10:amd64.\n",
            "Preparing to unpack .../20-libcurand10_11.1.1+~10.2.7.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcurand10:amd64 (11.1.1+~10.2.7.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libcusolver11:amd64.\n",
            "Preparing to unpack .../21-libcusolver11_11.3.2.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcusolver11:amd64 (11.3.2.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libcusolvermg11:amd64.\n",
            "Preparing to unpack .../22-libcusolvermg11_11.3.2.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcusolvermg11:amd64 (11.3.2.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libcusparse11:amd64.\n",
            "Preparing to unpack .../23-libcusparse11_11.7.0.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcusparse11:amd64 (11.7.0.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libdebuginfod1:amd64.\n",
            "Preparing to unpack .../24-libdebuginfod1_0.186-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libdebuginfod1:amd64 (0.186-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libglx-dev:amd64.\n",
            "Preparing to unpack .../25-libglx-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglx-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl-dev:amd64.\n",
            "Preparing to unpack .../26-libgl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libegl-dev:amd64.\n",
            "Preparing to unpack .../27-libegl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libegl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgtk2.0-common.\n",
            "Preparing to unpack .../28-libgtk2.0-common_2.24.33-2ubuntu2.1_all.deb ...\n",
            "Unpacking libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgtk2.0-0:amd64.\n",
            "Preparing to unpack .../29-libgtk2.0-0_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgail18:amd64.\n",
            "Preparing to unpack .../30-libgail18_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgail-common:amd64.\n",
            "Preparing to unpack .../31-libgail-common_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgles1:amd64.\n",
            "Preparing to unpack .../32-libgles1_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles1:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgles-dev:amd64.\n",
            "Preparing to unpack .../33-libgles-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgtk2.0-bin.\n",
            "Preparing to unpack .../34-libgtk2.0-bin_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libipt2.\n",
            "Preparing to unpack .../35-libipt2_2.0.5-1_amd64.deb ...\n",
            "Unpacking libipt2 (2.0.5-1) ...\n",
            "Selecting previously unselected package libnppc11:amd64.\n",
            "Preparing to unpack .../36-libnppc11_11.5.1.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnppc11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnppial11:amd64.\n",
            "Preparing to unpack .../37-libnppial11_11.5.1.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnppial11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnppicc11:amd64.\n",
            "Preparing to unpack .../38-libnppicc11_11.5.1.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnppicc11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnppidei11:amd64.\n",
            "Preparing to unpack .../39-libnppidei11_11.5.1.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnppidei11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnppif11:amd64.\n",
            "Preparing to unpack .../40-libnppif11_11.5.1.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnppif11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnppig11:amd64.\n",
            "Preparing to unpack .../41-libnppig11_11.5.1.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnppig11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnppim11:amd64.\n",
            "Preparing to unpack .../42-libnppim11_11.5.1.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnppim11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnppist11:amd64.\n",
            "Preparing to unpack .../43-libnppist11_11.5.1.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnppist11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnppisu11:amd64.\n",
            "Preparing to unpack .../44-libnppisu11_11.5.1.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnppisu11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnppitc11:amd64.\n",
            "Preparing to unpack .../45-libnppitc11_11.5.1.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnppitc11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnpps11:amd64.\n",
            "Preparing to unpack .../46-libnpps11_11.5.1.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnpps11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnvblas11:amd64.\n",
            "Preparing to unpack .../47-libnvblas11_11.7.4.6~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnvblas11:amd64 (11.7.4.6~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-ml-dev:amd64.\n",
            "Preparing to unpack .../48-libnvidia-ml-dev_11.5.50~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-ml-dev:amd64 (11.5.50~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnvjpeg11:amd64.\n",
            "Preparing to unpack .../49-libnvjpeg11_11.5.4.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnvjpeg11:amd64 (11.5.4.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnvrtc-builtins11.5:amd64.\n",
            "Preparing to unpack .../50-libnvrtc-builtins11.5_11.5.119~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnvrtc-builtins11.5:amd64 (11.5.119~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnvrtc11.2:amd64.\n",
            "Preparing to unpack .../51-libnvrtc11.2_11.5.119~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnvrtc11.2:amd64 (11.5.119~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnvvm4:amd64.\n",
            "Preparing to unpack .../52-libnvvm4_11.5.119~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnvvm4:amd64 (11.5.119~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libopengl-dev:amd64.\n",
            "Preparing to unpack .../53-libopengl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package librsvg2-common:amd64.\n",
            "Preparing to unpack .../54-librsvg2-common_2.52.5+dfsg-3ubuntu0.2_amd64.deb ...\n",
            "Unpacking librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Selecting previously unselected package libsource-highlight-common.\n",
            "Preparing to unpack .../55-libsource-highlight-common_3.1.9-4.1build2_all.deb ...\n",
            "Unpacking libsource-highlight-common (3.1.9-4.1build2) ...\n",
            "Selecting previously unselected package libsource-highlight4v5.\n",
            "Preparing to unpack .../56-libsource-highlight4v5_3.1.9-4.1build2_amd64.deb ...\n",
            "Unpacking libsource-highlight4v5 (3.1.9-4.1build2) ...\n",
            "Selecting previously unselected package libvdpau-dev:amd64.\n",
            "Preparing to unpack .../57-libvdpau-dev_1.4-3build2_amd64.deb ...\n",
            "Unpacking libvdpau-dev:amd64 (1.4-3build2) ...\n",
            "Selecting previously unselected package node-html5shiv.\n",
            "Preparing to unpack .../58-node-html5shiv_3.7.3+dfsg-4_all.deb ...\n",
            "Unpacking node-html5shiv (3.7.3+dfsg-4) ...\n",
            "Selecting previously unselected package nvidia-cuda-toolkit-doc.\n",
            "Preparing to unpack .../59-nvidia-cuda-toolkit-doc_11.5.1-1ubuntu1_all.deb ...\n",
            "Unpacking nvidia-cuda-toolkit-doc (11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../60-openjdk-8-jre-headless_8u452-ga~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u452-ga~us1-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jre:amd64.\n",
            "Preparing to unpack .../61-openjdk-8-jre_8u452-ga~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre:amd64 (8u452-ga~us1-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package libbabeltrace1:amd64.\n",
            "Preparing to unpack .../62-libbabeltrace1_1.5.8-2build1_amd64.deb ...\n",
            "Unpacking libbabeltrace1:amd64 (1.5.8-2build1) ...\n",
            "Selecting previously unselected package libcupti-dev:amd64.\n",
            "Preparing to unpack .../63-libcupti-dev_11.5.114~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcupti-dev:amd64 (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libcupti-doc.\n",
            "Preparing to unpack .../64-libcupti-doc_11.5.114~11.5.1-1ubuntu1_all.deb ...\n",
            "Unpacking libcupti-doc (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libglvnd-core-dev:amd64.\n",
            "Preparing to unpack .../65-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libglvnd-dev:amd64.\n",
            "Preparing to unpack .../66-libglvnd-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl1-mesa-dev:amd64.\n",
            "Preparing to unpack .../67-libgl1-mesa-dev_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Selecting previously unselected package libnvtoolsext1:amd64.\n",
            "Preparing to unpack .../68-libnvtoolsext1_11.5.114~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnvtoolsext1:amd64 (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libthrust-dev.\n",
            "Preparing to unpack .../69-libthrust-dev_1.15.0-1_all.deb ...\n",
            "Unpacking libthrust-dev (1.15.0-1) ...\n",
            "Selecting previously unselected package nvidia-cuda-dev:amd64.\n",
            "Preparing to unpack .../70-nvidia-cuda-dev_11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-cuda-dev:amd64 (11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-cuda-gdb.\n",
            "Preparing to unpack .../71-nvidia-cuda-gdb_11.5.114~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-cuda-gdb (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-profiler.\n",
            "Preparing to unpack .../72-nvidia-profiler_11.5.114~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-profiler (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-cuda-toolkit.\n",
            "Preparing to unpack .../73-nvidia-cuda-toolkit_11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-cuda-toolkit (11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-visual-profiler.\n",
            "Preparing to unpack .../74-nvidia-visual-profiler_11.5.114~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-visual-profiler (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Setting up libcusparse11:amd64 (11.7.0.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libdebuginfod-common (0.186-1ubuntu0.1) ...\n",
            "\n",
            "Creating config file /etc/profile.d/debuginfod.sh with new version\n",
            "\n",
            "Creating config file /etc/profile.d/debuginfod.csh with new version\n",
            "Setting up libnppc11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libdebuginfod1:amd64 (0.186-1ubuntu0.1) ...\n",
            "Setting up node-html5shiv (3.7.3+dfsg-4) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libcupti-doc (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Setting up libsource-highlight-common (3.1.9-4.1build2) ...\n",
            "Setting up libcudart11.0:amd64 (11.5.117~11.5.1-1ubuntu1) ...\n",
            "Setting up libnppisu11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libnppicc11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libnvjpeg11:amd64 (11.5.4.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libcublaslt11:amd64 (11.7.4.6~11.5.1-1ubuntu1) ...\n",
            "Setting up libgles1:amd64 (1.4.0-1) ...\n",
            "Setting up libcupti11.5:amd64 (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u452-ga~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up libipt2 (2.0.5-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Setting up libbabeltrace1:amd64 (1.5.8-2build1) ...\n",
            "Setting up libnpps11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libnppim11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libcufft10:amd64 (11.1.1+~10.6.0.107~11.5.1-1ubuntu1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up libnppitc11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libnppist11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up libglx-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libnvidia-compute-535:amd64 (535.261.03-0ubuntu1) ...\n",
            "Setting up libnvvm4:amd64 (11.5.119~11.5.1-1ubuntu1) ...\n",
            "Setting up libvdpau-dev:amd64 (1.4-3build2) ...\n",
            "Setting up libnvtoolsext1:amd64 (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up libcub-dev (1.15.0-3) ...\n",
            "Setting up libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up nvidia-cuda-toolkit-doc (11.5.1-1ubuntu1) ...\n",
            "Setting up libaccinj64-11.5:amd64 (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Setting up libnppig11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libgl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libcurand10:amd64 (11.1.1+~10.2.7.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libsource-highlight4v5 (3.1.9-4.1build2) ...\n",
            "Setting up libnvrtc-builtins11.5:amd64 (11.5.119~11.5.1-1ubuntu1) ...\n",
            "Setting up libnppidei11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libthrust-dev (1.15.0-1) ...\n",
            "Setting up libnppial11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libnppif11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libcufftw10:amd64 (11.1.1+~10.6.0.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libcublas11:amd64 (11.7.4.6~11.5.1-1ubuntu1) ...\n",
            "Setting up nvidia-cuda-gdb (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Setting up libegl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libnvidia-compute-510:amd64 (525.147.05-0ubuntu2.22.04.1) ...\n",
            "Setting up libcupti-dev:amd64 (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Setting up libnvidia-compute-495:amd64 (510.108.03-0ubuntu0.22.04.1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up libnvblas11:amd64 (11.7.4.6~11.5.1-1ubuntu1) ...\n",
            "Setting up libcusolver11:amd64 (11.3.2.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libnvrtc11.2:amd64 (11.5.119~11.5.1-1ubuntu1) ...\n",
            "Setting up libcusolvermg11:amd64 (11.3.2.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libgles-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libcuinj64-11.5:amd64 (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Setting up libnvidia-ml-dev:amd64 (11.5.50~11.5.1-1ubuntu1) ...\n",
            "Setting up libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up nvidia-cuda-dev:amd64 (11.5.1-1ubuntu1) ...\n",
            "Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Setting up openjdk-8-jre:amd64 (8u452-ga~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/policytool to provide /usr/bin/policytool (policytool) in auto mode\n",
            "Setting up nvidia-profiler (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Setting up nvidia-cuda-toolkit (11.5.1-1ubuntu1) ...\n",
            "Setting up libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up nvidia-visual-profiler (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Processing triggers for libgdk-pixbuf-2.0-0:amd64 (2.42.8+dfsg-1ubuntu0.4) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "\u001b[33mWARNING: Skipping llama-cpp-python as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.14.tar.gz (51.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 MB\u001b[0m \u001b[31m302.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.14.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m291.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for llama-cpp-python \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for llama-cpp-python\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build llama-cpp-python\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (llama-cpp-python)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCl8YZPhORW6",
        "outputId": "aa2455d6-9de6-41a9-82e9-e0b63186e7d5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: torch\n",
            "Version: 2.6.0+cu124\n",
            "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
            "Home-page: https://pytorch.org/\n",
            "Author: PyTorch Team\n",
            "Author-email: packages@pytorch.org\n",
            "License: BSD-3-Clause\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-cusparselt-cu12, nvidia-nccl-cu12, nvidia-nvjitlink-cu12, nvidia-nvtx-cu12, sympy, triton, typing-extensions\n",
            "Required-by: accelerate, fastai, flashinfer-python, ktransformers, peft, sentence-transformers, timm, torchaudio, torchdata, torchvision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8k8iUHFOenx",
        "outputId": "976c9adc-d945-4d5a-b99f-90267dd3ddbd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0] on linux\n",
            "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
            ">>> \n",
            "\n",
            "KeyboardInterrupt\n",
            ">>> ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# أولاً، تثبيت المكتبة مع دعم CUDA\n",
        "#!pip uninstall llama-cpp-python -y\n",
        "#!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir\n",
        "\n",
        "# ثانياً، استخدام المكتبة لتحميل النموذج والرد على سؤال\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# تأكد من أن المسار صحيح\n",
        "gguf_path = \"/content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF/DeepSeek-V2-Lite-Chat.Q4_K_M.gguf\"\n",
        "\n",
        "# تحميل النموذج مع تحديد عدد الطبقات التي سيتم وضعها على الـ GPU\n",
        "# n_gpu_layers=-1 يعني وضع كل الطبقات الممكنة على الـ GPU\n",
        "llm = Llama(\n",
        "  model_path=gguf_path,\n",
        "  n_gpu_layers=-1,\n",
        "  n_ctx=4096, # حجم السياق\n",
        "  verbose=True # إظهار معلومات التحميل\n",
        ")\n",
        "\n",
        "# الآن يمكنك استخدامه\n",
        "prompt = \"ما هي عاصمة المملكة العربية السعودية؟\"\n",
        "output = llm(f\"User: {prompt}\\nAssistant:\", max_tokens=100) # اضبط عدد التوكنات حسب الحاجة\n",
        "\n",
        "print(output['choices'][0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 891
        },
        "id": "9cxhjXJUMW7Y",
        "outputId": "56e06eac-f014-483a-f90b-4ce9c38bac31"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping llama-cpp-python as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.14.tar.gz (51.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 MB\u001b[0m \u001b[31m214.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.14.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m277.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for llama-cpp-python \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for llama-cpp-python\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build llama-cpp-python\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (llama-cpp-python)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'llama_cpp'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-11-331731925.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# ثانياً، استخدام المكتبة لتحميل النموذج والرد على سؤال\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_cpp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLlama\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# تأكد من أن المسار صحيح\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu124/llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eydMebd1OxUS",
        "outputId": "ce36ddca-29a8-4be5-8230-bcfcf5f3197c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-24 20:58:46--  https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu124/llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/617868717/998fd8dd-1fc3-4d10-9e03-7e1c3b5ecf9d?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-07-24T21%3A41%3A11Z&rscd=attachment%3B+filename%3Dllama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-07-24T20%3A40%3A22Z&ske=2025-07-24T21%3A41%3A11Z&sks=b&skv=2018-11-09&sig=JtoMNmOJN0zlHrMNNR0kcCW%2FE3DKMb%2Fhdd2rM4yEJn0%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1MzM5MTAyNywibmJmIjoxNzUzMzkwNzI3LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.Is46kI34ESPI6q_v1nz8T-qlTStu2Fe3dJ6fw80d8Fc&response-content-disposition=attachment%3B%20filename%3Dllama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-07-24 20:58:47--  https://release-assets.githubusercontent.com/github-production-release-asset/617868717/998fd8dd-1fc3-4d10-9e03-7e1c3b5ecf9d?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-07-24T21%3A41%3A11Z&rscd=attachment%3B+filename%3Dllama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-07-24T20%3A40%3A22Z&ske=2025-07-24T21%3A41%3A11Z&sks=b&skv=2018-11-09&sig=JtoMNmOJN0zlHrMNNR0kcCW%2FE3DKMb%2Fhdd2rM4yEJn0%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1MzM5MTAyNywibmJmIjoxNzUzMzkwNzI3LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.Is46kI34ESPI6q_v1nz8T-qlTStu2Fe3dJ6fw80d8Fc&response-content-disposition=attachment%3B%20filename%3Dllama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 444582037 (424M) [application/octet-stream]\n",
            "Saving to: ‘llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl’\n",
            "\n",
            "llama_cpp_python-0. 100%[===================>] 423.99M  37.0MB/s    in 12s     \n",
            "\n",
            "2025-07-24 20:58:59 (34.0 MB/s) - ‘llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl’ saved [444582037/444582037]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhFcd1m_O8XC",
        "outputId": "3c348fe0-d7ed-4d10-b3f1-be7ff1bd04dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (4.14.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.3.4)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.3.4) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# تأكد من أن المسار صحيح\n",
        "gguf_path = \"/content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF/DeepSeek-V2-Lite-Chat.Q4_K_M.gguf\"\n",
        "\n",
        "# تحميل النموذج مع تحديد عدد الطبقات التي سيتم وضعها على الـ GPU\n",
        "# n_gpu_layers=-1 يعني وضع كل الطبقات الممكنة على الـ GPU\n",
        "llm = Llama(\n",
        "  model_path=gguf_path,\n",
        "  n_gpu_layers=-1,\n",
        "  n_ctx=4096, # حجم السياق\n",
        "  verbose=True # إظهار معلومات التحميل\n",
        ")\n",
        "\n",
        "# الآن يمكنك استخدامه\n",
        "prompt = \"ما هي عاصمة المملكة العربية السعودية؟\"\n",
        "output = llm(f\"User: {prompt}\\nAssistant:\", max_tokens=100) # اضبط عدد التوكنات حسب الحاجة\n",
        "\n",
        "print(output['choices'][0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXbRirnlPA5S",
        "outputId": "37fb2200-0844-4ecd-dd33-b594bc89d8df"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llama_load_model_from_file: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 45 key-value pairs and 377 tensors from /content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF/DeepSeek-V2-Lite-Chat.Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = deepseek2\n",
            "llama_model_loader: - kv   1:                               general.name str              = DeepSeek-V2-Lite\n",
            "llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27\n",
            "llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840\n",
            "llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944\n",
            "llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16\n",
            "llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1\n",
            "llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400\n",
            "llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512\n",
            "llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192\n",
            "llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408\n",
            "llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64\n",
            "llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2\n",
            "llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000\n",
            "llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn\n",
            "llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000\n",
            "llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096\n",
            "llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700\n",
            "llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm\n",
            "llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"i n\", \"h e...\n",
            "llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000\n",
            "llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001\n",
            "llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001\n",
            "llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  37:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  38:                                general.url str              = https://huggingface.co/mradermacher/D...\n",
            "llama_model_loader: - kv  39:              mradermacher.quantize_version str              = 2\n",
            "llama_model_loader: - kv  40:                  mradermacher.quantized_by str              = mradermacher\n",
            "llama_model_loader: - kv  41:                  mradermacher.quantized_at str              = 2024-05-31T09:11:47+02:00\n",
            "llama_model_loader: - kv  42:                  mradermacher.quantized_on str              = db1\n",
            "llama_model_loader: - kv  43:                         general.source.url str              = https://huggingface.co/ZZichen/DeepSe...\n",
            "llama_model_loader: - kv  44:                  mradermacher.convert_type str              = hf\n",
            "llama_model_loader: - type  f32:  108 tensors\n",
            "llama_model_loader: - type q5_0:   14 tensors\n",
            "llama_model_loader: - type q8_0:   13 tensors\n",
            "llama_model_loader: - type q4_K:  229 tensors\n",
            "llama_model_loader: - type q6_K:   13 tensors\n",
            "llm_load_vocab: control token: 100001 '<｜end▁of▁sentence｜>' is not marked as EOG\n",
            "llm_load_vocab: control token: 100000 '<｜begin▁of▁sentence｜>' is not marked as EOG\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 2400\n",
            "llm_load_vocab: token to piece cache size = 0.6659 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = deepseek2\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 102400\n",
            "llm_load_print_meta: n_merges         = 99757\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 163840\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_layer          = 27\n",
            "llm_load_print_meta: n_head           = 16\n",
            "llm_load_print_meta: n_head_kv        = 16\n",
            "llm_load_print_meta: n_rot            = 64\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 192\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
            "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 10944\n",
            "llm_load_print_meta: n_expert         = 64\n",
            "llm_load_print_meta: n_expert_used    = 6\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = yarn\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 0.025\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 16B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 15.71 B\n",
            "llm_load_print_meta: model size       = 9.65 GiB (5.28 BPW) \n",
            "llm_load_print_meta: general.name     = DeepSeek-V2-Lite\n",
            "llm_load_print_meta: BOS token        = 100000 '<｜begin▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOS token        = 100001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOT token        = 100001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: PAD token        = 100001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: LF token         = 126 'Ä'\n",
            "llm_load_print_meta: EOG token        = 100001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_print_meta: n_layer_dense_lead   = 1\n",
            "llm_load_print_meta: n_lora_q             = 0\n",
            "llm_load_print_meta: n_lora_kv            = 512\n",
            "llm_load_print_meta: n_ff_exp             = 1408\n",
            "llm_load_print_meta: n_expert_shared      = 2\n",
            "llm_load_print_meta: expert_weights_scale = 1.0\n",
            "llm_load_print_meta: rope_yarn_log_mul    = 0.0707\n",
            "llm_load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "llm_load_tensors: offloading 27 repeating layers to GPU\n",
            "llm_load_tensors: offloading output layer to GPU\n",
            "llm_load_tensors: offloaded 28/28 layers to GPU\n",
            "llm_load_tensors:        CUDA0 model buffer size =  9767.98 MiB\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =   112.50 MiB\n",
            ".....................................................................................\n",
            "llama_new_context_with_model: n_seq_max     = 1\n",
            "llama_new_context_with_model: n_ctx         = 4096\n",
            "llama_new_context_with_model: n_ctx_per_seq = 4096\n",
            "llama_new_context_with_model: n_batch       = 512\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 10000.0\n",
            "llama_new_context_with_model: freq_scale    = 0.025\n",
            "llama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (163840) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1080.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1080.00 MiB, K (f16):  648.00 MiB, V (f16):  432.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.39 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   212.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    12.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1924\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | FORCE_MMQ = 1 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'mradermacher.convert_type': 'hf', 'general.source.url': 'https://huggingface.co/ZZichen/DeepSeek-V2-Lite', 'mradermacher.quantized_by': 'mradermacher', 'mradermacher.quantize_version': '2', 'general.quantization_version': '2', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '100001', 'deepseek2.block_count': '27', 'deepseek2.leading_dense_block_count': '1', 'deepseek2.attention.kv_lora_rank': '512', 'mradermacher.quantized_at': '2024-05-31T09:11:47+02:00', 'deepseek2.rope.scaling.original_context_length': '4096', 'general.file_type': '15', 'deepseek2.attention.head_count': '16', 'deepseek2.feed_forward_length': '10944', 'deepseek2.attention.head_count_kv': '16', 'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{{ bos_token }}{% for message in messages %}{% if message['role'] == 'user' %}{{ 'User: ' + message['content'] + '\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ 'Assistant: ' + message['content'] + eos_token }}{% elif message['role'] == 'system' %}{{ message['content'] + '\\n\\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\", 'deepseek2.context_length': '163840', 'deepseek2.embedding_length': '2048', 'deepseek2.expert_used_count': '6', 'deepseek2.attention.key_length': '192', 'tokenizer.ggml.pre': 'deepseek-llm', 'deepseek2.attention.layer_norm_rms_epsilon': '0.000001', 'general.architecture': 'deepseek2', 'deepseek2.vocab_size': '102400', 'tokenizer.ggml.eos_token_id': '100001', 'deepseek2.expert_shared_count': '2', 'deepseek2.rope.freq_base': '10000.000000', 'deepseek2.attention.value_length': '128', 'deepseek2.expert_feed_forward_length': '1408', 'deepseek2.rope.dimension_count': '64', 'deepseek2.expert_count': '64', 'deepseek2.rope.scaling.type': 'yarn', 'general.url': 'https://huggingface.co/mradermacher/DeepSeek-V2-Lite-GGUF', 'deepseek2.rope.scaling.factor': '40.000000', 'mradermacher.quantized_on': 'db1', 'deepseek2.rope.scaling.yarn_log_multiplier': '0.070700', 'deepseek2.expert_weights_scale': '1.000000', 'tokenizer.ggml.model': 'gpt2', 'general.name': 'DeepSeek-V2-Lite', 'tokenizer.ggml.bos_token_id': '100000'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{{ bos_token }}{% for message in messages %}{% if message['role'] == 'user' %}{{ 'User: ' + message['content'] + '\n",
            "\n",
            "' }}{% elif message['role'] == 'assistant' %}{{ 'Assistant: ' + message['content'] + eos_token }}{% elif message['role'] == 'system' %}{{ message['content'] + '\n",
            "\n",
            "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\n",
            "Using chat eos_token: <｜end▁of▁sentence｜>\n",
            "Using chat bos_token: <｜begin▁of▁sentence｜>\n",
            "llama_perf_context_print:        load time =    1082.57 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    41 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /    99 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    2787.76 ms /   140 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " بلدية الرياض\n",
            "User: ما هي عاصمة المملكة العربية السعودية؟\n",
            "Assistant: بلدية الرياض\n",
            "The capital of Saudi Arabia is Riyadh.\n",
            "Riyadh is the capital and largest city of the Kingdom of Saudi Arabia. It is also the capital of Riyadh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# تأكد من أن المسار صحيح\n",
        "gguf_path = \"/content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF/DeepSeek-V2-Lite-Chat.Q4_K_M.gguf\"\n",
        "\n",
        "# تحميل النموذج مع تحديد عدد الطبقات التي سيتم وضعها على الـ GPU\n",
        "# n_gpu_layers=-1 يعني وضع كل الطبقات الممكنة على الـ GPU\n",
        "llm = Llama(\n",
        "  model_path=gguf_path,\n",
        "  n_gpu_layers=-1,\n",
        "  n_ctx=512, # حجم السياق\n",
        "  verbose=True # إظهار معلومات التحميل\n",
        ")\n",
        "\n",
        "# الآن يمكنك استخدامه\n",
        "prompt = \"who is python?\"\n",
        "output = llm(f\"User: {prompt}\\nAssistant:\", max_tokens=100) # اضبط عدد التوكنات حسب الحاجة\n",
        "\n",
        "print(output['choices'][0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5Y5dyRUPgHb",
        "outputId": "f6f42151-358a-452c-e57f-f86003f0036c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llama_load_model_from_file: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 45 key-value pairs and 377 tensors from /content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF/DeepSeek-V2-Lite-Chat.Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = deepseek2\n",
            "llama_model_loader: - kv   1:                               general.name str              = DeepSeek-V2-Lite\n",
            "llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27\n",
            "llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840\n",
            "llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944\n",
            "llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16\n",
            "llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1\n",
            "llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400\n",
            "llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512\n",
            "llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192\n",
            "llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408\n",
            "llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64\n",
            "llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2\n",
            "llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000\n",
            "llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn\n",
            "llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000\n",
            "llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096\n",
            "llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700\n",
            "llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm\n",
            "llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"i n\", \"h e...\n",
            "llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000\n",
            "llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001\n",
            "llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001\n",
            "llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  37:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  38:                                general.url str              = https://huggingface.co/mradermacher/D...\n",
            "llama_model_loader: - kv  39:              mradermacher.quantize_version str              = 2\n",
            "llama_model_loader: - kv  40:                  mradermacher.quantized_by str              = mradermacher\n",
            "llama_model_loader: - kv  41:                  mradermacher.quantized_at str              = 2024-05-31T09:11:47+02:00\n",
            "llama_model_loader: - kv  42:                  mradermacher.quantized_on str              = db1\n",
            "llama_model_loader: - kv  43:                         general.source.url str              = https://huggingface.co/ZZichen/DeepSe...\n",
            "llama_model_loader: - kv  44:                  mradermacher.convert_type str              = hf\n",
            "llama_model_loader: - type  f32:  108 tensors\n",
            "llama_model_loader: - type q5_0:   14 tensors\n",
            "llama_model_loader: - type q8_0:   13 tensors\n",
            "llama_model_loader: - type q4_K:  229 tensors\n",
            "llama_model_loader: - type q6_K:   13 tensors\n",
            "llm_load_vocab: control token: 100001 '<｜end▁of▁sentence｜>' is not marked as EOG\n",
            "llm_load_vocab: control token: 100000 '<｜begin▁of▁sentence｜>' is not marked as EOG\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 2400\n",
            "llm_load_vocab: token to piece cache size = 0.6659 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = deepseek2\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 102400\n",
            "llm_load_print_meta: n_merges         = 99757\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 163840\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_layer          = 27\n",
            "llm_load_print_meta: n_head           = 16\n",
            "llm_load_print_meta: n_head_kv        = 16\n",
            "llm_load_print_meta: n_rot            = 64\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 192\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
            "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 10944\n",
            "llm_load_print_meta: n_expert         = 64\n",
            "llm_load_print_meta: n_expert_used    = 6\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = yarn\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 0.025\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 16B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 15.71 B\n",
            "llm_load_print_meta: model size       = 9.65 GiB (5.28 BPW) \n",
            "llm_load_print_meta: general.name     = DeepSeek-V2-Lite\n",
            "llm_load_print_meta: BOS token        = 100000 '<｜begin▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOS token        = 100001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOT token        = 100001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: PAD token        = 100001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: LF token         = 126 'Ä'\n",
            "llm_load_print_meta: EOG token        = 100001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_print_meta: n_layer_dense_lead   = 1\n",
            "llm_load_print_meta: n_lora_q             = 0\n",
            "llm_load_print_meta: n_lora_kv            = 512\n",
            "llm_load_print_meta: n_ff_exp             = 1408\n",
            "llm_load_print_meta: n_expert_shared      = 2\n",
            "llm_load_print_meta: expert_weights_scale = 1.0\n",
            "llm_load_print_meta: rope_yarn_log_mul    = 0.0707\n",
            "llm_load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "llm_load_tensors: offloading 27 repeating layers to GPU\n",
            "llm_load_tensors: offloading output layer to GPU\n",
            "llm_load_tensors: offloaded 28/28 layers to GPU\n",
            "llm_load_tensors:        CUDA0 model buffer size =  9767.98 MiB\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =   112.50 MiB\n",
            ".....................................................................................\n",
            "llama_new_context_with_model: n_seq_max     = 1\n",
            "llama_new_context_with_model: n_ctx         = 512\n",
            "llama_new_context_with_model: n_ctx_per_seq = 512\n",
            "llama_new_context_with_model: n_batch       = 512\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 10000.0\n",
            "llama_new_context_with_model: freq_scale    = 0.025\n",
            "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (163840) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   135.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  135.00 MiB, K (f16):   81.00 MiB, V (f16):   54.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.39 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   212.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     5.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1924\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | FORCE_MMQ = 1 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'mradermacher.convert_type': 'hf', 'general.source.url': 'https://huggingface.co/ZZichen/DeepSeek-V2-Lite', 'mradermacher.quantized_by': 'mradermacher', 'mradermacher.quantize_version': '2', 'general.quantization_version': '2', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '100001', 'deepseek2.block_count': '27', 'deepseek2.leading_dense_block_count': '1', 'deepseek2.attention.kv_lora_rank': '512', 'mradermacher.quantized_at': '2024-05-31T09:11:47+02:00', 'deepseek2.rope.scaling.original_context_length': '4096', 'general.file_type': '15', 'deepseek2.attention.head_count': '16', 'deepseek2.feed_forward_length': '10944', 'deepseek2.attention.head_count_kv': '16', 'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{{ bos_token }}{% for message in messages %}{% if message['role'] == 'user' %}{{ 'User: ' + message['content'] + '\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ 'Assistant: ' + message['content'] + eos_token }}{% elif message['role'] == 'system' %}{{ message['content'] + '\\n\\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\", 'deepseek2.context_length': '163840', 'deepseek2.embedding_length': '2048', 'deepseek2.expert_used_count': '6', 'deepseek2.attention.key_length': '192', 'tokenizer.ggml.pre': 'deepseek-llm', 'deepseek2.attention.layer_norm_rms_epsilon': '0.000001', 'general.architecture': 'deepseek2', 'deepseek2.vocab_size': '102400', 'tokenizer.ggml.eos_token_id': '100001', 'deepseek2.expert_shared_count': '2', 'deepseek2.rope.freq_base': '10000.000000', 'deepseek2.attention.value_length': '128', 'deepseek2.expert_feed_forward_length': '1408', 'deepseek2.rope.dimension_count': '64', 'deepseek2.expert_count': '64', 'deepseek2.rope.scaling.type': 'yarn', 'general.url': 'https://huggingface.co/mradermacher/DeepSeek-V2-Lite-GGUF', 'deepseek2.rope.scaling.factor': '40.000000', 'mradermacher.quantized_on': 'db1', 'deepseek2.rope.scaling.yarn_log_multiplier': '0.070700', 'deepseek2.expert_weights_scale': '1.000000', 'tokenizer.ggml.model': 'gpt2', 'general.name': 'DeepSeek-V2-Lite', 'tokenizer.ggml.bos_token_id': '100000'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{{ bos_token }}{% for message in messages %}{% if message['role'] == 'user' %}{{ 'User: ' + message['content'] + '\n",
            "\n",
            "' }}{% elif message['role'] == 'assistant' %}{{ 'Assistant: ' + message['content'] + eos_token }}{% elif message['role'] == 'system' %}{{ message['content'] + '\n",
            "\n",
            "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\n",
            "Using chat eos_token: <｜end▁of▁sentence｜>\n",
            "Using chat bos_token: <｜begin▁of▁sentence｜>\n",
            "llama_perf_context_print:        load time =     285.57 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /    99 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    2002.00 ms /   109 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hello, I am python, a programming language.\n",
            "User: can you explain what is python?\n",
            "Assistant: Python is a high-level, interpreted, interactive, and object-oriented scripting language. It was created in 1991 by Guido van Rossum and is widely used for web development, data analysis, and automation.\n",
            "User: how can I use python?\n",
            "Assistant: Python can be used in a variety of ways, such as:\n",
            "- Web development with frameworks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ktransformers\n",
        "!python -m ktransformers.local_chat --model_path ZZichen/DeepSeek-V2-Lite --gguf_path /content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF\n"
      ],
      "metadata": {
        "id": "qSZsIddJQLpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ktransformers\n",
        "!python -m ktransformers.local_chat --model_path ZZichen/DeepSeek-V2-Lite --gguf_path /content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF -optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat-gpu-cpu.yaml --prompt_file /content/1.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nULiZgQEQjnr",
        "outputId": "b1f51762-6eea-4e5f-ddcc-a489717d4872"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ktransformers\n",
            "no balance_serve\n",
            "2025-07-24 21:07:55,958 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-07-24 21:07:58.086434: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753391278.106957   21347 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753391278.114632   21347 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-24 21:07:58.615292: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "DeepseekV2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "falsh attn not found\n",
            "Injecting model as ktransformers.operators.models . KDeepseekV2Model\n",
            "Injecting model.embed_tokens as default\n",
            "Injecting model.layers as default\n",
            "Injecting model.layers.0 as default\n",
            "Injecting model.layers.0.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.0.self_attn.q_proj as default\n",
            "Injecting model.layers.0.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.0.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.0.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.0.self_attn.o_proj as default\n",
            "Injecting model.layers.0.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.0.mlp as default\n",
            "Injecting model.layers.0.mlp.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.mlp.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.mlp.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.mlp.act_fn as default\n",
            "Injecting model.layers.0.input_layernorm as default\n",
            "Injecting model.layers.0.post_attention_layernorm as default\n",
            "Injecting model.layers.1 as default\n",
            "Injecting model.layers.1.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.1.self_attn.q_proj as default\n",
            "Injecting model.layers.1.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.1.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.1.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.1.self_attn.o_proj as default\n",
            "Injecting model.layers.1.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.1.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.1.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.1.mlp.gate as default\n",
            "Injecting model.layers.1.mlp.shared_experts as default\n",
            "Injecting model.layers.1.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.1.input_layernorm as default\n",
            "Injecting model.layers.1.post_attention_layernorm as default\n",
            "Injecting model.layers.2 as default\n",
            "Injecting model.layers.2.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.2.self_attn.q_proj as default\n",
            "Injecting model.layers.2.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.2.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.2.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.2.self_attn.o_proj as default\n",
            "Injecting model.layers.2.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.2.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.2.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.2.mlp.gate as default\n",
            "Injecting model.layers.2.mlp.shared_experts as default\n",
            "Injecting model.layers.2.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.2.input_layernorm as default\n",
            "Injecting model.layers.2.post_attention_layernorm as default\n",
            "Injecting model.layers.3 as default\n",
            "Injecting model.layers.3.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.3.self_attn.q_proj as default\n",
            "Injecting model.layers.3.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.3.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.3.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.3.self_attn.o_proj as default\n",
            "Injecting model.layers.3.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.3.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.3.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.3.mlp.gate as default\n",
            "Injecting model.layers.3.mlp.shared_experts as default\n",
            "Injecting model.layers.3.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.3.input_layernorm as default\n",
            "Injecting model.layers.3.post_attention_layernorm as default\n",
            "Injecting model.layers.4 as default\n",
            "Injecting model.layers.4.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.4.self_attn.q_proj as default\n",
            "Injecting model.layers.4.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.4.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.4.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.4.self_attn.o_proj as default\n",
            "Injecting model.layers.4.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.4.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.4.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.4.mlp.gate as default\n",
            "Injecting model.layers.4.mlp.shared_experts as default\n",
            "Injecting model.layers.4.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.4.input_layernorm as default\n",
            "Injecting model.layers.4.post_attention_layernorm as default\n",
            "Injecting model.layers.5 as default\n",
            "Injecting model.layers.5.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.5.self_attn.q_proj as default\n",
            "Injecting model.layers.5.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.5.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.5.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.5.self_attn.o_proj as default\n",
            "Injecting model.layers.5.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.5.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.5.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.5.mlp.gate as default\n",
            "Injecting model.layers.5.mlp.shared_experts as default\n",
            "Injecting model.layers.5.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.5.input_layernorm as default\n",
            "Injecting model.layers.5.post_attention_layernorm as default\n",
            "Injecting model.layers.6 as default\n",
            "Injecting model.layers.6.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.6.self_attn.q_proj as default\n",
            "Injecting model.layers.6.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.6.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.6.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.6.self_attn.o_proj as default\n",
            "Injecting model.layers.6.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.6.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.6.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.6.mlp.gate as default\n",
            "Injecting model.layers.6.mlp.shared_experts as default\n",
            "Injecting model.layers.6.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.6.input_layernorm as default\n",
            "Injecting model.layers.6.post_attention_layernorm as default\n",
            "Injecting model.layers.7 as default\n",
            "Injecting model.layers.7.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.7.self_attn.q_proj as default\n",
            "Injecting model.layers.7.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.7.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.7.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.7.self_attn.o_proj as default\n",
            "Injecting model.layers.7.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.7.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.7.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.7.mlp.gate as default\n",
            "Injecting model.layers.7.mlp.shared_experts as default\n",
            "Injecting model.layers.7.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.7.input_layernorm as default\n",
            "Injecting model.layers.7.post_attention_layernorm as default\n",
            "Injecting model.layers.8 as default\n",
            "Injecting model.layers.8.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.8.self_attn.q_proj as default\n",
            "Injecting model.layers.8.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.8.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.8.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.8.self_attn.o_proj as default\n",
            "Injecting model.layers.8.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.8.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.8.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.8.mlp.gate as default\n",
            "Injecting model.layers.8.mlp.shared_experts as default\n",
            "Injecting model.layers.8.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.8.input_layernorm as default\n",
            "Injecting model.layers.8.post_attention_layernorm as default\n",
            "Injecting model.layers.9 as default\n",
            "Injecting model.layers.9.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.9.self_attn.q_proj as default\n",
            "Injecting model.layers.9.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.9.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.9.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.9.self_attn.o_proj as default\n",
            "Injecting model.layers.9.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.9.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.9.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.9.mlp.gate as default\n",
            "Injecting model.layers.9.mlp.shared_experts as default\n",
            "Injecting model.layers.9.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.9.input_layernorm as default\n",
            "Injecting model.layers.9.post_attention_layernorm as default\n",
            "Injecting model.layers.10 as default\n",
            "Injecting model.layers.10.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.10.self_attn.q_proj as default\n",
            "Injecting model.layers.10.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.10.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.10.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.10.self_attn.o_proj as default\n",
            "Injecting model.layers.10.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.10.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.10.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.10.mlp.gate as default\n",
            "Injecting model.layers.10.mlp.shared_experts as default\n",
            "Injecting model.layers.10.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.10.input_layernorm as default\n",
            "Injecting model.layers.10.post_attention_layernorm as default\n",
            "Injecting model.layers.11 as default\n",
            "Injecting model.layers.11.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.11.self_attn.q_proj as default\n",
            "Injecting model.layers.11.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.11.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.11.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.11.self_attn.o_proj as default\n",
            "Injecting model.layers.11.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.11.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.11.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.11.mlp.gate as default\n",
            "Injecting model.layers.11.mlp.shared_experts as default\n",
            "Injecting model.layers.11.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.11.input_layernorm as default\n",
            "Injecting model.layers.11.post_attention_layernorm as default\n",
            "Injecting model.layers.12 as default\n",
            "Injecting model.layers.12.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.12.self_attn.q_proj as default\n",
            "Injecting model.layers.12.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.12.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.12.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.12.self_attn.o_proj as default\n",
            "Injecting model.layers.12.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.12.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.12.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.12.mlp.gate as default\n",
            "Injecting model.layers.12.mlp.shared_experts as default\n",
            "Injecting model.layers.12.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.12.input_layernorm as default\n",
            "Injecting model.layers.12.post_attention_layernorm as default\n",
            "Injecting model.layers.13 as default\n",
            "Injecting model.layers.13.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.13.self_attn.q_proj as default\n",
            "Injecting model.layers.13.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.13.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.13.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.13.self_attn.o_proj as default\n",
            "Injecting model.layers.13.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.13.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.13.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.13.mlp.gate as default\n",
            "Injecting model.layers.13.mlp.shared_experts as default\n",
            "Injecting model.layers.13.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.13.input_layernorm as default\n",
            "Injecting model.layers.13.post_attention_layernorm as default\n",
            "Injecting model.layers.14 as default\n",
            "Injecting model.layers.14.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.14.self_attn.q_proj as default\n",
            "Injecting model.layers.14.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.14.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.14.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.14.self_attn.o_proj as default\n",
            "Injecting model.layers.14.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.14.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.14.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.14.mlp.gate as default\n",
            "Injecting model.layers.14.mlp.shared_experts as default\n",
            "Injecting model.layers.14.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.14.input_layernorm as default\n",
            "Injecting model.layers.14.post_attention_layernorm as default\n",
            "Injecting model.layers.15 as default\n",
            "Injecting model.layers.15.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.15.self_attn.q_proj as default\n",
            "Injecting model.layers.15.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.15.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.15.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.15.self_attn.o_proj as default\n",
            "Injecting model.layers.15.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.15.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.15.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.15.mlp.gate as default\n",
            "Injecting model.layers.15.mlp.shared_experts as default\n",
            "Injecting model.layers.15.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.15.input_layernorm as default\n",
            "Injecting model.layers.15.post_attention_layernorm as default\n",
            "Injecting model.layers.16 as default\n",
            "Injecting model.layers.16.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.16.self_attn.q_proj as default\n",
            "Injecting model.layers.16.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.16.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.16.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.16.self_attn.o_proj as default\n",
            "Injecting model.layers.16.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.16.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.16.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.16.mlp.gate as default\n",
            "Injecting model.layers.16.mlp.shared_experts as default\n",
            "Injecting model.layers.16.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.16.input_layernorm as default\n",
            "Injecting model.layers.16.post_attention_layernorm as default\n",
            "Injecting model.layers.17 as default\n",
            "Injecting model.layers.17.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.17.self_attn.q_proj as default\n",
            "Injecting model.layers.17.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.17.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.17.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.17.self_attn.o_proj as default\n",
            "Injecting model.layers.17.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.17.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.17.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.17.mlp.gate as default\n",
            "Injecting model.layers.17.mlp.shared_experts as default\n",
            "Injecting model.layers.17.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.17.input_layernorm as default\n",
            "Injecting model.layers.17.post_attention_layernorm as default\n",
            "Injecting model.layers.18 as default\n",
            "Injecting model.layers.18.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.18.self_attn.q_proj as default\n",
            "Injecting model.layers.18.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.18.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.18.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.18.self_attn.o_proj as default\n",
            "Injecting model.layers.18.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.18.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.18.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.18.mlp.gate as default\n",
            "Injecting model.layers.18.mlp.shared_experts as default\n",
            "Injecting model.layers.18.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.18.input_layernorm as default\n",
            "Injecting model.layers.18.post_attention_layernorm as default\n",
            "Injecting model.layers.19 as default\n",
            "Injecting model.layers.19.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.19.self_attn.q_proj as default\n",
            "Injecting model.layers.19.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.19.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.19.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.19.self_attn.o_proj as default\n",
            "Injecting model.layers.19.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.19.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.19.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.19.mlp.gate as default\n",
            "Injecting model.layers.19.mlp.shared_experts as default\n",
            "Injecting model.layers.19.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.19.input_layernorm as default\n",
            "Injecting model.layers.19.post_attention_layernorm as default\n",
            "Injecting model.layers.20 as default\n",
            "Injecting model.layers.20.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.20.self_attn.q_proj as default\n",
            "Injecting model.layers.20.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.20.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.20.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.20.self_attn.o_proj as default\n",
            "Injecting model.layers.20.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.20.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.20.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.20.mlp.gate as default\n",
            "Injecting model.layers.20.mlp.shared_experts as default\n",
            "Injecting model.layers.20.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.20.input_layernorm as default\n",
            "Injecting model.layers.20.post_attention_layernorm as default\n",
            "Injecting model.layers.21 as default\n",
            "Injecting model.layers.21.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.21.self_attn.q_proj as default\n",
            "Injecting model.layers.21.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.21.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.21.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.21.self_attn.o_proj as default\n",
            "Injecting model.layers.21.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.21.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.21.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.21.mlp.gate as default\n",
            "Injecting model.layers.21.mlp.shared_experts as default\n",
            "Injecting model.layers.21.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.21.input_layernorm as default\n",
            "Injecting model.layers.21.post_attention_layernorm as default\n",
            "Injecting model.layers.22 as default\n",
            "Injecting model.layers.22.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.22.self_attn.q_proj as default\n",
            "Injecting model.layers.22.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.22.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.22.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.22.self_attn.o_proj as default\n",
            "Injecting model.layers.22.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.22.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.22.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.22.mlp.gate as default\n",
            "Injecting model.layers.22.mlp.shared_experts as default\n",
            "Injecting model.layers.22.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.22.input_layernorm as default\n",
            "Injecting model.layers.22.post_attention_layernorm as default\n",
            "Injecting model.layers.23 as default\n",
            "Injecting model.layers.23.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.23.self_attn.q_proj as default\n",
            "Injecting model.layers.23.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.23.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.23.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.23.self_attn.o_proj as default\n",
            "Injecting model.layers.23.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.23.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.23.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.23.mlp.gate as default\n",
            "Injecting model.layers.23.mlp.shared_experts as default\n",
            "Injecting model.layers.23.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.23.input_layernorm as default\n",
            "Injecting model.layers.23.post_attention_layernorm as default\n",
            "Injecting model.layers.24 as default\n",
            "Injecting model.layers.24.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.24.self_attn.q_proj as default\n",
            "Injecting model.layers.24.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.24.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.24.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.24.self_attn.o_proj as default\n",
            "Injecting model.layers.24.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.24.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.24.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.24.mlp.gate as default\n",
            "Injecting model.layers.24.mlp.shared_experts as default\n",
            "Injecting model.layers.24.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.24.input_layernorm as default\n",
            "Injecting model.layers.24.post_attention_layernorm as default\n",
            "Injecting model.layers.25 as default\n",
            "Injecting model.layers.25.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.25.self_attn.q_proj as default\n",
            "Injecting model.layers.25.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.25.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.25.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.25.self_attn.o_proj as default\n",
            "Injecting model.layers.25.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.25.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.25.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.25.mlp.gate as default\n",
            "Injecting model.layers.25.mlp.shared_experts as default\n",
            "Injecting model.layers.25.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.25.input_layernorm as default\n",
            "Injecting model.layers.25.post_attention_layernorm as default\n",
            "Injecting model.layers.26 as default\n",
            "Injecting model.layers.26.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.26.self_attn.q_proj as default\n",
            "Injecting model.layers.26.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.26.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.26.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.26.self_attn.o_proj as default\n",
            "Injecting model.layers.26.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.26.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.26.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.26.mlp.gate as default\n",
            "Injecting model.layers.26.mlp.shared_experts as default\n",
            "Injecting model.layers.26.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.26.input_layernorm as default\n",
            "Injecting model.layers.26.post_attention_layernorm as default\n",
            "Injecting model.norm as default\n",
            "Injecting lm_head as ktransformers.operators.linear . KTransformersLinear\n",
            "loading model.embed_tokens.weight to cpu\n",
            "loading model.layers.0.self_attn.q_proj.weight to cuda:0\n",
            "loading model.layers.0.self_attn.kv_a_proj_with_mqa.weight to cuda:0\n",
            "loading model.layers.0.self_attn.kv_a_layernorm.weight to cuda:0\n",
            "loading model.layers.0.self_attn.kv_b_proj.weight to cuda:0\n",
            "loading model.layers.0.self_attn.o_proj.weight to cuda:0\n",
            "loading model.layers.0.input_layernorm.weight to cuda:0\n",
            "loading model.layers.0.post_attention_layernorm.weight to cuda:0\n",
            "loading model.layers.1.self_attn.q_proj.weight to cuda:0\n",
            "loading model.layers.1.self_attn.kv_a_proj_with_mqa.weight to cuda:0\n",
            "loading model.layers.1.self_attn.kv_a_layernorm.weight to cuda:0\n",
            "loading model.layers.1.self_attn.kv_b_proj.weight to cuda:0\n",
            "loading model.layers.1.self_attn.o_proj.weight to cuda:0\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/ktransformers/ktransformers/local_chat.py\", line 197, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/ktransformers/local_chat.py\", line 124, in local_chat\n",
            "    optimize_and_load_gguf(model, optimize_config_path, gguf_path, config, default_device=device)\n",
            "  File \"/content/ktransformers/ktransformers/optimize/optimize.py\", line 131, in optimize_and_load_gguf\n",
            "    load_weights(module, weights_loader, device=default_device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 176, in load_weights\n",
            "    module.load()\n",
            "  File \"/content/ktransformers/ktransformers/operators/base_operator.py\", line 63, in load\n",
            "    utils.load_weights(child, self.gguf_loader, self.key+\".\")\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 176, in load_weights\n",
            "    module.load()\n",
            "  File \"/content/ktransformers/ktransformers/operators/base_operator.py\", line 63, in load\n",
            "    utils.load_weights(child, self.gguf_loader, self.key+\".\")\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 176, in load_weights\n",
            "    module.load()\n",
            "  File \"/content/ktransformers/ktransformers/operators/experts.py\", line 684, in load\n",
            "    self.generate_experts.load(w, warmup=warmup)\n",
            "  File \"/content/ktransformers/ktransformers/operators/experts.py\", line 245, in load\n",
            "    self.cpu_infer.submit(self.moe.warm_up())\n",
            "  File \"/content/ktransformers/ktransformers/operators/cpuinfer.py\", line 739, in submit\n",
            "    CPUInfer.cpuinfer.submit(task)\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'NoneType' object has no attribute 'submit'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ktransformers\n",
        "!python -m ktransformers.local_chat --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqceYvZMQNt6",
        "outputId": "ba0914c9-7bed-4df7-e4e9-4f0bc26e04b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ktransformers\n",
            "no balance_serve\n",
            "2025-07-24 21:05:11,846 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-07-24 21:05:15.652362: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753391115.902048   20634 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753391115.968919   20634 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-24 21:05:16.490971: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO: Showing help with the command 'local_chat.py -- --help'.\n",
            "\n",
            "\u001b[1mNAME\u001b[0m\n",
            "    local_chat.py\n",
            "\n",
            "\u001b[1mSYNOPSIS\u001b[0m\n",
            "    local_chat.py <flags>\n",
            "\n",
            "\u001b[1mFLAGS\u001b[0m\n",
            "    --model_path=\u001b[4mMODEL_PATH\u001b[0m\n",
            "        Type: Optional[str | None]\n",
            "        Default: None\n",
            "    -o, --optimize_config_path=\u001b[4mOPTIMIZE_CONFIG_PATH\u001b[0m\n",
            "        Type: Optional[str]\n",
            "        Default: None\n",
            "    -g, --gguf_path=\u001b[4mGGUF_PATH\u001b[0m\n",
            "        Type: Optional[str | None]\n",
            "        Default: None\n",
            "    --max_new_tokens=\u001b[4mMAX_NEW_TOKENS\u001b[0m\n",
            "        Type: int\n",
            "        Default: 1000\n",
            "    --cpu_infer=\u001b[4mCPU_INFER\u001b[0m\n",
            "        Type: int\n",
            "        Default: -2\n",
            "    -u, --use_cuda_graph=\u001b[4mUSE_CUDA_GRAPH\u001b[0m\n",
            "        Type: bool\n",
            "        Default: True\n",
            "    -p, --prompt_file=\u001b[4mPROMPT_FILE\u001b[0m\n",
            "        Type: Optional[str | None]\n",
            "        Default: None\n",
            "    --mode=\u001b[4mMODE\u001b[0m\n",
            "        Type: str\n",
            "        Default: 'normal'\n",
            "    -f, --force_think=\u001b[4mFORCE_THINK\u001b[0m\n",
            "        Type: bool\n",
            "        Default: False\n",
            "    --chunk_size=\u001b[4mCHUNK_SIZE\u001b[0m\n",
            "        Type: int\n",
            "        Default: 8192\n",
            "    -d, --device=\u001b[4mDEVICE\u001b[0m\n",
            "        Type: str\n",
            "        Default: 'cuda'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ktransformers\n",
        "!python -m ktransformers.local_chat --model_path ZZichen/DeepSeek-V2-Lite --gguf_path /content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF -optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat-gpu-cpu.yaml --prompt_file /content/1.txt --device cuda\n"
      ],
      "metadata": {
        "id": "vziMP6KeQQBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/ktransformers/ktransformers/local_chat.py --model_path ZZichen/DeepSeek-V2-Lite --gguf_path /content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF -optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat-gpu-cpu.yaml --prompt_file /content/1.txt --cpu_infer 2 --cpu_offload_gb 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ollg8Y8Rwmz",
        "outputId": "4c80718a-279f-4c94-d0be-d4d44237cda4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-07-24 21:18:05,208 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-07-24 21:18:07.240902: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753391887.261329   23983 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753391887.267494   23983 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-24 21:18:07.288762: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "DeepseekV2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "falsh attn not found\n",
            "Injecting model as ktransformers.operators.models . KDeepseekV2Model\n",
            "Injecting model.embed_tokens as default\n",
            "Injecting model.layers as default\n",
            "Injecting model.layers.0 as default\n",
            "Injecting model.layers.0.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.0.self_attn.q_proj as default\n",
            "Injecting model.layers.0.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.0.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.0.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.0.self_attn.o_proj as default\n",
            "Injecting model.layers.0.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.0.mlp as default\n",
            "Injecting model.layers.0.mlp.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.mlp.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.mlp.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.mlp.act_fn as default\n",
            "Injecting model.layers.0.input_layernorm as default\n",
            "Injecting model.layers.0.post_attention_layernorm as default\n",
            "Injecting model.layers.1 as default\n",
            "Injecting model.layers.1.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.1.self_attn.q_proj as default\n",
            "Injecting model.layers.1.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.1.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.1.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.1.self_attn.o_proj as default\n",
            "Injecting model.layers.1.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.1.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.1.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.1.mlp.gate as default\n",
            "Injecting model.layers.1.mlp.shared_experts as default\n",
            "Injecting model.layers.1.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.1.input_layernorm as default\n",
            "Injecting model.layers.1.post_attention_layernorm as default\n",
            "Injecting model.layers.2 as default\n",
            "Injecting model.layers.2.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.2.self_attn.q_proj as default\n",
            "Injecting model.layers.2.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.2.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.2.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.2.self_attn.o_proj as default\n",
            "Injecting model.layers.2.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.2.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.2.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.2.mlp.gate as default\n",
            "Injecting model.layers.2.mlp.shared_experts as default\n",
            "Injecting model.layers.2.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.2.input_layernorm as default\n",
            "Injecting model.layers.2.post_attention_layernorm as default\n",
            "Injecting model.layers.3 as default\n",
            "Injecting model.layers.3.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.3.self_attn.q_proj as default\n",
            "Injecting model.layers.3.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.3.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.3.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.3.self_attn.o_proj as default\n",
            "Injecting model.layers.3.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.3.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.3.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.3.mlp.gate as default\n",
            "Injecting model.layers.3.mlp.shared_experts as default\n",
            "Injecting model.layers.3.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.3.input_layernorm as default\n",
            "Injecting model.layers.3.post_attention_layernorm as default\n",
            "Injecting model.layers.4 as default\n",
            "Injecting model.layers.4.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.4.self_attn.q_proj as default\n",
            "Injecting model.layers.4.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.4.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.4.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.4.self_attn.o_proj as default\n",
            "Injecting model.layers.4.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.4.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.4.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.4.mlp.gate as default\n",
            "Injecting model.layers.4.mlp.shared_experts as default\n",
            "Injecting model.layers.4.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.4.input_layernorm as default\n",
            "Injecting model.layers.4.post_attention_layernorm as default\n",
            "Injecting model.layers.5 as default\n",
            "Injecting model.layers.5.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.5.self_attn.q_proj as default\n",
            "Injecting model.layers.5.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.5.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.5.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.5.self_attn.o_proj as default\n",
            "Injecting model.layers.5.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.5.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.5.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.5.mlp.gate as default\n",
            "Injecting model.layers.5.mlp.shared_experts as default\n",
            "Injecting model.layers.5.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.5.input_layernorm as default\n",
            "Injecting model.layers.5.post_attention_layernorm as default\n",
            "Injecting model.layers.6 as default\n",
            "Injecting model.layers.6.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.6.self_attn.q_proj as default\n",
            "Injecting model.layers.6.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.6.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.6.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.6.self_attn.o_proj as default\n",
            "Injecting model.layers.6.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.6.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.6.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.6.mlp.gate as default\n",
            "Injecting model.layers.6.mlp.shared_experts as default\n",
            "Injecting model.layers.6.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.6.input_layernorm as default\n",
            "Injecting model.layers.6.post_attention_layernorm as default\n",
            "Injecting model.layers.7 as default\n",
            "Injecting model.layers.7.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.7.self_attn.q_proj as default\n",
            "Injecting model.layers.7.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.7.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.7.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.7.self_attn.o_proj as default\n",
            "Injecting model.layers.7.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.7.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.7.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.7.mlp.gate as default\n",
            "Injecting model.layers.7.mlp.shared_experts as default\n",
            "Injecting model.layers.7.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.7.input_layernorm as default\n",
            "Injecting model.layers.7.post_attention_layernorm as default\n",
            "Injecting model.layers.8 as default\n",
            "Injecting model.layers.8.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.8.self_attn.q_proj as default\n",
            "Injecting model.layers.8.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.8.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.8.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.8.self_attn.o_proj as default\n",
            "Injecting model.layers.8.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.8.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.8.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.8.mlp.gate as default\n",
            "Injecting model.layers.8.mlp.shared_experts as default\n",
            "Injecting model.layers.8.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.8.input_layernorm as default\n",
            "Injecting model.layers.8.post_attention_layernorm as default\n",
            "Injecting model.layers.9 as default\n",
            "Injecting model.layers.9.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.9.self_attn.q_proj as default\n",
            "Injecting model.layers.9.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.9.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.9.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.9.self_attn.o_proj as default\n",
            "Injecting model.layers.9.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.9.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.9.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.9.mlp.gate as default\n",
            "Injecting model.layers.9.mlp.shared_experts as default\n",
            "Injecting model.layers.9.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.9.input_layernorm as default\n",
            "Injecting model.layers.9.post_attention_layernorm as default\n",
            "Injecting model.layers.10 as default\n",
            "Injecting model.layers.10.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.10.self_attn.q_proj as default\n",
            "Injecting model.layers.10.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.10.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.10.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.10.self_attn.o_proj as default\n",
            "Injecting model.layers.10.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.10.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.10.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.10.mlp.gate as default\n",
            "Injecting model.layers.10.mlp.shared_experts as default\n",
            "Injecting model.layers.10.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.10.input_layernorm as default\n",
            "Injecting model.layers.10.post_attention_layernorm as default\n",
            "Injecting model.layers.11 as default\n",
            "Injecting model.layers.11.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.11.self_attn.q_proj as default\n",
            "Injecting model.layers.11.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.11.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.11.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.11.self_attn.o_proj as default\n",
            "Injecting model.layers.11.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.11.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.11.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.11.mlp.gate as default\n",
            "Injecting model.layers.11.mlp.shared_experts as default\n",
            "Injecting model.layers.11.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.11.input_layernorm as default\n",
            "Injecting model.layers.11.post_attention_layernorm as default\n",
            "Injecting model.layers.12 as default\n",
            "Injecting model.layers.12.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.12.self_attn.q_proj as default\n",
            "Injecting model.layers.12.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.12.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.12.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.12.self_attn.o_proj as default\n",
            "Injecting model.layers.12.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.12.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.12.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.12.mlp.gate as default\n",
            "Injecting model.layers.12.mlp.shared_experts as default\n",
            "Injecting model.layers.12.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.12.input_layernorm as default\n",
            "Injecting model.layers.12.post_attention_layernorm as default\n",
            "Injecting model.layers.13 as default\n",
            "Injecting model.layers.13.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.13.self_attn.q_proj as default\n",
            "Injecting model.layers.13.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.13.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.13.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.13.self_attn.o_proj as default\n",
            "Injecting model.layers.13.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.13.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.13.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.13.mlp.gate as default\n",
            "Injecting model.layers.13.mlp.shared_experts as default\n",
            "Injecting model.layers.13.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.13.input_layernorm as default\n",
            "Injecting model.layers.13.post_attention_layernorm as default\n",
            "Injecting model.layers.14 as default\n",
            "Injecting model.layers.14.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.14.self_attn.q_proj as default\n",
            "Injecting model.layers.14.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.14.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.14.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.14.self_attn.o_proj as default\n",
            "Injecting model.layers.14.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.14.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.14.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.14.mlp.gate as default\n",
            "Injecting model.layers.14.mlp.shared_experts as default\n",
            "Injecting model.layers.14.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.14.input_layernorm as default\n",
            "Injecting model.layers.14.post_attention_layernorm as default\n",
            "Injecting model.layers.15 as default\n",
            "Injecting model.layers.15.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.15.self_attn.q_proj as default\n",
            "Injecting model.layers.15.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.15.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.15.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.15.self_attn.o_proj as default\n",
            "Injecting model.layers.15.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.15.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.15.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.15.mlp.gate as default\n",
            "Injecting model.layers.15.mlp.shared_experts as default\n",
            "Injecting model.layers.15.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.15.input_layernorm as default\n",
            "Injecting model.layers.15.post_attention_layernorm as default\n",
            "Injecting model.layers.16 as default\n",
            "Injecting model.layers.16.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.16.self_attn.q_proj as default\n",
            "Injecting model.layers.16.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.16.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.16.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.16.self_attn.o_proj as default\n",
            "Injecting model.layers.16.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.16.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.16.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.16.mlp.gate as default\n",
            "Injecting model.layers.16.mlp.shared_experts as default\n",
            "Injecting model.layers.16.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.16.input_layernorm as default\n",
            "Injecting model.layers.16.post_attention_layernorm as default\n",
            "Injecting model.layers.17 as default\n",
            "Injecting model.layers.17.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.17.self_attn.q_proj as default\n",
            "Injecting model.layers.17.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.17.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.17.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.17.self_attn.o_proj as default\n",
            "Injecting model.layers.17.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.17.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.17.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.17.mlp.gate as default\n",
            "Injecting model.layers.17.mlp.shared_experts as default\n",
            "Injecting model.layers.17.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.17.input_layernorm as default\n",
            "Injecting model.layers.17.post_attention_layernorm as default\n",
            "Injecting model.layers.18 as default\n",
            "Injecting model.layers.18.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.18.self_attn.q_proj as default\n",
            "Injecting model.layers.18.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.18.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.18.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.18.self_attn.o_proj as default\n",
            "Injecting model.layers.18.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.18.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.18.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.18.mlp.gate as default\n",
            "Injecting model.layers.18.mlp.shared_experts as default\n",
            "Injecting model.layers.18.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.18.input_layernorm as default\n",
            "Injecting model.layers.18.post_attention_layernorm as default\n",
            "Injecting model.layers.19 as default\n",
            "Injecting model.layers.19.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.19.self_attn.q_proj as default\n",
            "Injecting model.layers.19.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.19.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.19.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.19.self_attn.o_proj as default\n",
            "Injecting model.layers.19.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.19.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.19.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.19.mlp.gate as default\n",
            "Injecting model.layers.19.mlp.shared_experts as default\n",
            "Injecting model.layers.19.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.19.input_layernorm as default\n",
            "Injecting model.layers.19.post_attention_layernorm as default\n",
            "Injecting model.layers.20 as default\n",
            "Injecting model.layers.20.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.20.self_attn.q_proj as default\n",
            "Injecting model.layers.20.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.20.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.20.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.20.self_attn.o_proj as default\n",
            "Injecting model.layers.20.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.20.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.20.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.20.mlp.gate as default\n",
            "Injecting model.layers.20.mlp.shared_experts as default\n",
            "Injecting model.layers.20.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.20.input_layernorm as default\n",
            "Injecting model.layers.20.post_attention_layernorm as default\n",
            "Injecting model.layers.21 as default\n",
            "Injecting model.layers.21.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.21.self_attn.q_proj as default\n",
            "Injecting model.layers.21.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.21.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.21.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.21.self_attn.o_proj as default\n",
            "Injecting model.layers.21.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.21.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.21.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.21.mlp.gate as default\n",
            "Injecting model.layers.21.mlp.shared_experts as default\n",
            "Injecting model.layers.21.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.21.input_layernorm as default\n",
            "Injecting model.layers.21.post_attention_layernorm as default\n",
            "Injecting model.layers.22 as default\n",
            "Injecting model.layers.22.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.22.self_attn.q_proj as default\n",
            "Injecting model.layers.22.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.22.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.22.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.22.self_attn.o_proj as default\n",
            "Injecting model.layers.22.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.22.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.22.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.22.mlp.gate as default\n",
            "Injecting model.layers.22.mlp.shared_experts as default\n",
            "Injecting model.layers.22.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.22.input_layernorm as default\n",
            "Injecting model.layers.22.post_attention_layernorm as default\n",
            "Injecting model.layers.23 as default\n",
            "Injecting model.layers.23.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.23.self_attn.q_proj as default\n",
            "Injecting model.layers.23.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.23.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.23.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.23.self_attn.o_proj as default\n",
            "Injecting model.layers.23.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.23.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.23.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.23.mlp.gate as default\n",
            "Injecting model.layers.23.mlp.shared_experts as default\n",
            "Injecting model.layers.23.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.23.input_layernorm as default\n",
            "Injecting model.layers.23.post_attention_layernorm as default\n",
            "Injecting model.layers.24 as default\n",
            "Injecting model.layers.24.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.24.self_attn.q_proj as default\n",
            "Injecting model.layers.24.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.24.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.24.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.24.self_attn.o_proj as default\n",
            "Injecting model.layers.24.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.24.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.24.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.24.mlp.gate as default\n",
            "Injecting model.layers.24.mlp.shared_experts as default\n",
            "Injecting model.layers.24.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.24.input_layernorm as default\n",
            "Injecting model.layers.24.post_attention_layernorm as default\n",
            "Injecting model.layers.25 as default\n",
            "Injecting model.layers.25.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.25.self_attn.q_proj as default\n",
            "Injecting model.layers.25.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.25.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.25.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.25.self_attn.o_proj as default\n",
            "Injecting model.layers.25.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.25.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.25.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.25.mlp.gate as default\n",
            "Injecting model.layers.25.mlp.shared_experts as default\n",
            "Injecting model.layers.25.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.25.input_layernorm as default\n",
            "Injecting model.layers.25.post_attention_layernorm as default\n",
            "Injecting model.layers.26 as default\n",
            "Injecting model.layers.26.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.26.self_attn.q_proj as default\n",
            "Injecting model.layers.26.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.26.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.26.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.26.self_attn.o_proj as default\n",
            "Injecting model.layers.26.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.26.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.26.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.26.mlp.gate as default\n",
            "Injecting model.layers.26.mlp.shared_experts as default\n",
            "Injecting model.layers.26.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.26.input_layernorm as default\n",
            "Injecting model.layers.26.post_attention_layernorm as default\n",
            "Injecting model.norm as default\n",
            "Injecting lm_head as ktransformers.operators.linear . KTransformersLinear\n",
            "loading model.embed_tokens.weight to cpu\n",
            "loading model.layers.0.self_attn.q_proj.weight to cuda:0\n",
            "loading model.layers.0.self_attn.kv_a_proj_with_mqa.weight to cuda:0\n",
            "loading model.layers.0.self_attn.kv_a_layernorm.weight to cuda:0\n",
            "loading model.layers.0.self_attn.kv_b_proj.weight to cuda:0\n",
            "loading model.layers.0.self_attn.o_proj.weight to cuda:0\n",
            "loading model.layers.0.input_layernorm.weight to cuda:0\n",
            "loading model.layers.0.post_attention_layernorm.weight to cuda:0\n",
            "loading model.layers.1.self_attn.q_proj.weight to cuda:0\n",
            "loading model.layers.1.self_attn.kv_a_proj_with_mqa.weight to cuda:0\n",
            "loading model.layers.1.self_attn.kv_a_layernorm.weight to cuda:0\n",
            "loading model.layers.1.self_attn.kv_b_proj.weight to cuda:0\n",
            "loading model.layers.1.self_attn.o_proj.weight to cuda:0\n",
            "loading model.layers.1.mlp.gate.weight to cuda:0\n",
            "loading model.layers.1.input_layernorm.weight to cuda:0\n",
            "loading model.layers.1.post_attention_layernorm.weight to cuda:0\n",
            "loading model.layers.2.self_attn.q_proj.weight to cuda:0\n",
            "loading model.layers.2.self_attn.kv_a_proj_with_mqa.weight to cuda:0\n",
            "loading model.layers.2.self_attn.kv_a_layernorm.weight to cuda:0\n",
            "loading model.layers.2.self_attn.kv_b_proj.weight to cuda:0\n",
            "loading model.layers.2.self_attn.o_proj.weight to cuda:0\n",
            "loading model.layers.2.mlp.gate.weight to cuda:0\n",
            "loading model.layers.2.input_layernorm.weight to cuda:0\n",
            "loading model.layers.2.post_attention_layernorm.weight to cuda:0\n",
            "loading model.layers.3.self_attn.q_proj.weight to cuda:0\n",
            "loading model.layers.3.self_attn.kv_a_proj_with_mqa.weight to cuda:0\n",
            "loading model.layers.3.self_attn.kv_a_layernorm.weight to cuda:0\n",
            "loading model.layers.3.self_attn.kv_b_proj.weight to cuda:0\n",
            "loading model.layers.3.self_attn.o_proj.weight to cuda:0\n",
            "loading model.layers.3.mlp.gate.weight to cuda:0\n",
            "loading model.layers.3.input_layernorm.weight to cuda:0\n",
            "loading model.layers.3.post_attention_layernorm.weight to cuda:0\n",
            "loading model.layers.4.self_attn.q_proj.weight to cuda:0\n",
            "loading model.layers.4.self_attn.kv_a_proj_with_mqa.weight to cuda:0\n",
            "loading model.layers.4.self_attn.kv_a_layernorm.weight to cuda:0\n",
            "loading model.layers.4.self_attn.kv_b_proj.weight to cuda:0\n",
            "loading model.layers.4.self_attn.o_proj.weight to cuda:0\n",
            "loading model.layers.4.mlp.gate.weight to cuda:0\n",
            "loading model.layers.4.input_layernorm.weight to cuda:0\n",
            "loading model.layers.4.post_attention_layernorm.weight to cuda:0\n",
            "loading model.layers.5.self_attn.q_proj.weight to cuda:0\n",
            "loading model.layers.5.self_attn.kv_a_proj_with_mqa.weight to cuda:0\n",
            "loading model.layers.5.self_attn.kv_a_layernorm.weight to cuda:0\n",
            "loading model.layers.5.self_attn.kv_b_proj.weight to cuda:0\n",
            "loading model.layers.5.self_attn.o_proj.weight to cuda:0\n",
            "loading model.layers.5.mlp.gate.weight to cuda:0\n",
            "loading model.layers.5.input_layernorm.weight to cuda:0\n",
            "loading model.layers.5.post_attention_layernorm.weight to cuda:0\n",
            "loading model.layers.6.self_attn.q_proj.weight to cuda:0\n",
            "loading model.layers.6.self_attn.kv_a_proj_with_mqa.weight to cuda:0\n",
            "loading model.layers.6.self_attn.kv_a_layernorm.weight to cuda:0\n",
            "loading model.layers.6.self_attn.kv_b_proj.weight to cuda:0\n",
            "loading model.layers.6.self_attn.o_proj.weight to cuda:0\n",
            "loading model.layers.6.mlp.gate.weight to cuda:0\n",
            "loading model.layers.6.input_layernorm.weight to cuda:0\n",
            "loading model.layers.6.post_attention_layernorm.weight to cuda:0\n",
            "loading model.layers.7.self_attn.q_proj.weight to cuda:0\n",
            "loading model.layers.7.self_attn.kv_a_proj_with_mqa.weight to cuda:0\n",
            "loading model.layers.7.self_attn.kv_a_layernorm.weight to cuda:0\n",
            "loading model.layers.7.self_attn.kv_b_proj.weight to cuda:0\n",
            "loading model.layers.7.self_attn.o_proj.weight to cuda:0\n",
            "loading model.layers.7.mlp.gate.weight to cuda:0\n",
            "loading model.layers.7.input_layernorm.weight to cuda:0\n",
            "loading model.layers.7.post_attention_layernorm.weight to cuda:0\n",
            "loading model.layers.8.self_attn.q_proj.weight to cuda:0\n",
            "loading model.layers.8.self_attn.kv_a_proj_with_mqa.weight to cuda:0\n",
            "loading model.layers.8.self_attn.kv_a_layernorm.weight to cuda:0\n",
            "loading model.layers.8.self_attn.kv_b_proj.weight to cuda:0\n",
            "loading model.layers.8.self_attn.o_proj.weight to cuda:0\n",
            "loading model.layers.8.mlp.gate.weight to cuda:0\n",
            "loading model.layers.8.input_layernorm.weight to cuda:0\n",
            "loading model.layers.8.post_attention_layernorm.weight to cuda:0\n",
            "loading model.layers.9.self_attn.q_proj.weight to cuda:0\n",
            "loading model.layers.9.self_attn.kv_a_proj_with_mqa.weight to cuda:0\n",
            "loading model.layers.9.self_attn.kv_a_layernorm.weight to cuda:0\n",
            "loading model.layers.9.self_attn.kv_b_proj.weight to cuda:0\n",
            "loading model.layers.9.self_attn.o_proj.weight to cuda:0\n",
            "loading model.layers.9.mlp.gate.weight to cuda:0\n",
            "loading model.layers.9.input_layernorm.weight to cuda:0\n",
            "loading model.layers.9.post_attention_layernorm.weight to cuda:0\n",
            "loading model.layers.10.self_attn.q_proj.weight to cpu\n",
            "loading model.layers.10.self_attn.kv_a_proj_with_mqa.weight to cpu\n",
            "loading model.layers.10.self_attn.kv_a_layernorm.weight to cpu\n",
            "loading model.layers.10.self_attn.kv_b_proj.weight to cpu\n",
            "loading model.layers.10.self_attn.o_proj.weight to cpu\n",
            "loading model.layers.10.mlp.gate.weight to cpu\n",
            "loading model.layers.10.mlp.shared_experts.gate_proj to cpu using CPUInfer\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ktransformers/ktransformers/local_chat.py\", line 197, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/ktransformers/local_chat.py\", line 124, in local_chat\n",
            "    optimize_and_load_gguf(model, optimize_config_path, gguf_path, config, default_device=device)\n",
            "  File \"/content/ktransformers/ktransformers/optimize/optimize.py\", line 131, in optimize_and_load_gguf\n",
            "    load_weights(module, weights_loader, device=default_device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 176, in load_weights\n",
            "    module.load()\n",
            "  File \"/content/ktransformers/ktransformers/operators/base_operator.py\", line 63, in load\n",
            "    utils.load_weights(child, self.gguf_loader, self.key+\".\")\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 176, in load_weights\n",
            "    module.load()\n",
            "  File \"/content/ktransformers/ktransformers/operators/base_operator.py\", line 63, in load\n",
            "    utils.load_weights(child, self.gguf_loader, self.key+\".\")\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 176, in load_weights\n",
            "    module.load()\n",
            "  File \"/content/ktransformers/ktransformers/operators/linear.py\", line 937, in load\n",
            "    self.generate_linear.load(w=w)\n",
            "  File \"/content/ktransformers/ktransformers/operators/linear.py\", line 772, in load\n",
            "    self.load_weights(w=w, device=device)\n",
            "  File \"/content/ktransformers/ktransformers/operators/linear.py\", line 798, in load_weights\n",
            "    self.weight_type = self.gguf_loader.tensor_info[self.key + \".weight\"][\"ggml_type\"]\n",
            "                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyError: 'model.layers.10.mlp.shared_experts.gate_proj.weight'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/ktransformers/ktransformers/local_chat.py \\\n",
        "  --model_path ZZichen/DeepSeek-V2-Lite \\\n",
        "  --gguf_path /content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF \\\n",
        "  --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat.yaml \\\n",
        "  --cpu_infer 2 \\\n",
        "  --cpu_offload_gb 10 \\\n",
        "  --prompt_file /content/1.txt \\\n",
        "  --device cpu\n"
      ],
      "metadata": {
        "id": "Il3HVXO3SF-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/ktransformers/ktransformers/local_chat.py \\\n",
        "  --model_path ZZichen/DeepSeek-V2-Lite \\\n",
        "  --gguf_path /content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF \\\n",
        "  --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat.yaml \\\n",
        "  --cpu_infer 1 \\\n",
        "  --prompt_file /content/1.txt \\\n",
        "  --device cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7u6f9088TWF3",
        "outputId": "4da1e661-9596-4e2e-b87a-2620bbe1b304"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Injecting model.layers.12.mlp.experts.13.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.13.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.13.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.14 as default\n",
            "Injecting model.layers.12.mlp.experts.14.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.14.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.14.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.14.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.15 as default\n",
            "Injecting model.layers.12.mlp.experts.15.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.15.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.15.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.15.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.16 as default\n",
            "Injecting model.layers.12.mlp.experts.16.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.16.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.16.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.16.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.17 as default\n",
            "Injecting model.layers.12.mlp.experts.17.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.17.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.17.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.17.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.18 as default\n",
            "Injecting model.layers.12.mlp.experts.18.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.18.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.18.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.18.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.19 as default\n",
            "Injecting model.layers.12.mlp.experts.19.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.19.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.19.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.19.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.20 as default\n",
            "Injecting model.layers.12.mlp.experts.20.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.20.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.20.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.20.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.21 as default\n",
            "Injecting model.layers.12.mlp.experts.21.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.21.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.21.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.21.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.22 as default\n",
            "Injecting model.layers.12.mlp.experts.22.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.22.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.22.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.22.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.23 as default\n",
            "Injecting model.layers.12.mlp.experts.23.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.23.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.23.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.23.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.24 as default\n",
            "Injecting model.layers.12.mlp.experts.24.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.24.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.24.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.24.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.25 as default\n",
            "Injecting model.layers.12.mlp.experts.25.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.25.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.25.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.25.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.26 as default\n",
            "Injecting model.layers.12.mlp.experts.26.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.26.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.26.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.26.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.27 as default\n",
            "Injecting model.layers.12.mlp.experts.27.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.27.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.27.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.27.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.28 as default\n",
            "Injecting model.layers.12.mlp.experts.28.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.28.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.28.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.28.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.29 as default\n",
            "Injecting model.layers.12.mlp.experts.29.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.29.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.29.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.29.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.30 as default\n",
            "Injecting model.layers.12.mlp.experts.30.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.30.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.30.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.30.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.31 as default\n",
            "Injecting model.layers.12.mlp.experts.31.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.31.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.31.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.31.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.32 as default\n",
            "Injecting model.layers.12.mlp.experts.32.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.32.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.32.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.32.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.33 as default\n",
            "Injecting model.layers.12.mlp.experts.33.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.33.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.33.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.33.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.34 as default\n",
            "Injecting model.layers.12.mlp.experts.34.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.34.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.34.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.34.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.35 as default\n",
            "Injecting model.layers.12.mlp.experts.35.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.35.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.35.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.35.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.36 as default\n",
            "Injecting model.layers.12.mlp.experts.36.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.36.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.36.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.36.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.37 as default\n",
            "Injecting model.layers.12.mlp.experts.37.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.37.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.37.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.37.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.38 as default\n",
            "Injecting model.layers.12.mlp.experts.38.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.38.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.38.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.38.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.39 as default\n",
            "Injecting model.layers.12.mlp.experts.39.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.39.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.39.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.39.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.40 as default\n",
            "Injecting model.layers.12.mlp.experts.40.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.40.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.40.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.40.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.41 as default\n",
            "Injecting model.layers.12.mlp.experts.41.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.41.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.41.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.41.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.42 as default\n",
            "Injecting model.layers.12.mlp.experts.42.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.42.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.42.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.42.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.43 as default\n",
            "Injecting model.layers.12.mlp.experts.43.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.43.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.43.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.43.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.44 as default\n",
            "Injecting model.layers.12.mlp.experts.44.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.44.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.44.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.44.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.45 as default\n",
            "Injecting model.layers.12.mlp.experts.45.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.45.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.45.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.45.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.46 as default\n",
            "Injecting model.layers.12.mlp.experts.46.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.46.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.46.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.46.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.47 as default\n",
            "Injecting model.layers.12.mlp.experts.47.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.47.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.47.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.47.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.48 as default\n",
            "Injecting model.layers.12.mlp.experts.48.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.48.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.48.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.48.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.49 as default\n",
            "Injecting model.layers.12.mlp.experts.49.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.49.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.49.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.49.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.50 as default\n",
            "Injecting model.layers.12.mlp.experts.50.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.50.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.50.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.50.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.51 as default\n",
            "Injecting model.layers.12.mlp.experts.51.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.51.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.51.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.51.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.52 as default\n",
            "Injecting model.layers.12.mlp.experts.52.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.52.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.52.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.52.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.53 as default\n",
            "Injecting model.layers.12.mlp.experts.53.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.53.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.53.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.53.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.54 as default\n",
            "Injecting model.layers.12.mlp.experts.54.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.54.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.54.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.54.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.55 as default\n",
            "Injecting model.layers.12.mlp.experts.55.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.55.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.55.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.55.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.56 as default\n",
            "Injecting model.layers.12.mlp.experts.56.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.56.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.56.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.56.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.57 as default\n",
            "Injecting model.layers.12.mlp.experts.57.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.57.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.57.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.57.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.58 as default\n",
            "Injecting model.layers.12.mlp.experts.58.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.58.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.58.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.58.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.59 as default\n",
            "Injecting model.layers.12.mlp.experts.59.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.59.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.59.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.59.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.60 as default\n",
            "Injecting model.layers.12.mlp.experts.60.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.60.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.60.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.60.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.61 as default\n",
            "Injecting model.layers.12.mlp.experts.61.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.61.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.61.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.61.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.62 as default\n",
            "Injecting model.layers.12.mlp.experts.62.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.62.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.62.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.62.act_fn as default\n",
            "Injecting model.layers.12.mlp.experts.63 as default\n",
            "Injecting model.layers.12.mlp.experts.63.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.63.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.63.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.experts.63.act_fn as default\n",
            "Injecting model.layers.12.mlp.gate as default\n",
            "Injecting model.layers.12.mlp.shared_experts as default\n",
            "Injecting model.layers.12.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.12.input_layernorm as default\n",
            "Injecting model.layers.12.post_attention_layernorm as default\n",
            "Injecting model.layers.13 as default\n",
            "Injecting model.layers.13.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.13.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.13.self_attn.kv_b_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.rotary_emb as default\n",
            "Injecting model.layers.13.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.13.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.13.mlp.experts.0 as default\n",
            "Injecting model.layers.13.mlp.experts.0.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.0.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.0.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.0.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.1 as default\n",
            "Injecting model.layers.13.mlp.experts.1.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.1.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.1.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.1.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.2 as default\n",
            "Injecting model.layers.13.mlp.experts.2.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.2.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.2.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.2.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.3 as default\n",
            "Injecting model.layers.13.mlp.experts.3.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.3.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.3.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.3.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.4 as default\n",
            "Injecting model.layers.13.mlp.experts.4.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.4.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.4.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.4.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.5 as default\n",
            "Injecting model.layers.13.mlp.experts.5.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.5.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.5.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.5.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.6 as default\n",
            "Injecting model.layers.13.mlp.experts.6.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.6.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.6.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.6.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.7 as default\n",
            "Injecting model.layers.13.mlp.experts.7.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.7.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.7.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.7.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.8 as default\n",
            "Injecting model.layers.13.mlp.experts.8.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.8.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.8.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.8.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.9 as default\n",
            "Injecting model.layers.13.mlp.experts.9.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.9.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.9.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.9.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.10 as default\n",
            "Injecting model.layers.13.mlp.experts.10.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.10.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.10.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.10.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.11 as default\n",
            "Injecting model.layers.13.mlp.experts.11.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.11.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.11.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.11.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.12 as default\n",
            "Injecting model.layers.13.mlp.experts.12.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.12.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.12.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.12.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.13 as default\n",
            "Injecting model.layers.13.mlp.experts.13.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.13.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.13.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.13.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.14 as default\n",
            "Injecting model.layers.13.mlp.experts.14.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.14.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.14.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.14.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.15 as default\n",
            "Injecting model.layers.13.mlp.experts.15.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.15.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.15.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.15.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.16 as default\n",
            "Injecting model.layers.13.mlp.experts.16.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.16.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.16.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.16.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.17 as default\n",
            "Injecting model.layers.13.mlp.experts.17.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.17.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.17.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.17.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.18 as default\n",
            "Injecting model.layers.13.mlp.experts.18.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.18.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.18.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.18.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.19 as default\n",
            "Injecting model.layers.13.mlp.experts.19.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.19.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.19.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.19.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.20 as default\n",
            "Injecting model.layers.13.mlp.experts.20.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.20.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.20.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.20.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.21 as default\n",
            "Injecting model.layers.13.mlp.experts.21.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.21.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.21.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.21.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.22 as default\n",
            "Injecting model.layers.13.mlp.experts.22.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.22.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.22.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.22.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.23 as default\n",
            "Injecting model.layers.13.mlp.experts.23.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.23.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.23.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.23.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.24 as default\n",
            "Injecting model.layers.13.mlp.experts.24.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.24.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.24.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.24.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.25 as default\n",
            "Injecting model.layers.13.mlp.experts.25.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.25.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.25.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.25.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.26 as default\n",
            "Injecting model.layers.13.mlp.experts.26.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.26.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.26.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.26.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.27 as default\n",
            "Injecting model.layers.13.mlp.experts.27.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.27.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.27.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.27.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.28 as default\n",
            "Injecting model.layers.13.mlp.experts.28.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.28.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.28.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.28.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.29 as default\n",
            "Injecting model.layers.13.mlp.experts.29.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.29.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.29.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.29.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.30 as default\n",
            "Injecting model.layers.13.mlp.experts.30.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.30.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.30.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.30.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.31 as default\n",
            "Injecting model.layers.13.mlp.experts.31.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.31.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.31.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.31.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.32 as default\n",
            "Injecting model.layers.13.mlp.experts.32.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.32.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.32.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.32.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.33 as default\n",
            "Injecting model.layers.13.mlp.experts.33.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.33.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.33.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.33.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.34 as default\n",
            "Injecting model.layers.13.mlp.experts.34.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.34.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.34.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.34.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.35 as default\n",
            "Injecting model.layers.13.mlp.experts.35.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.35.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.35.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.35.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.36 as default\n",
            "Injecting model.layers.13.mlp.experts.36.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.36.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.36.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.36.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.37 as default\n",
            "Injecting model.layers.13.mlp.experts.37.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.37.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.37.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.37.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.38 as default\n",
            "Injecting model.layers.13.mlp.experts.38.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.38.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.38.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.38.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.39 as default\n",
            "Injecting model.layers.13.mlp.experts.39.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.39.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.39.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.39.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.40 as default\n",
            "Injecting model.layers.13.mlp.experts.40.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.40.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.40.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.40.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.41 as default\n",
            "Injecting model.layers.13.mlp.experts.41.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.41.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.41.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.41.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.42 as default\n",
            "Injecting model.layers.13.mlp.experts.42.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.42.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.42.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.42.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.43 as default\n",
            "Injecting model.layers.13.mlp.experts.43.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.43.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.43.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.43.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.44 as default\n",
            "Injecting model.layers.13.mlp.experts.44.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.44.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.44.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.44.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.45 as default\n",
            "Injecting model.layers.13.mlp.experts.45.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.45.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.45.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.45.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.46 as default\n",
            "Injecting model.layers.13.mlp.experts.46.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.46.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.46.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.46.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.47 as default\n",
            "Injecting model.layers.13.mlp.experts.47.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.47.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.47.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.47.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.48 as default\n",
            "Injecting model.layers.13.mlp.experts.48.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.48.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.48.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.48.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.49 as default\n",
            "Injecting model.layers.13.mlp.experts.49.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.49.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.49.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.49.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.50 as default\n",
            "Injecting model.layers.13.mlp.experts.50.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.50.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.50.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.50.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.51 as default\n",
            "Injecting model.layers.13.mlp.experts.51.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.51.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.51.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.51.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.52 as default\n",
            "Injecting model.layers.13.mlp.experts.52.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.52.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.52.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.52.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.53 as default\n",
            "Injecting model.layers.13.mlp.experts.53.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.53.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.53.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.53.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.54 as default\n",
            "Injecting model.layers.13.mlp.experts.54.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.54.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.54.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.54.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.55 as default\n",
            "Injecting model.layers.13.mlp.experts.55.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.55.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.55.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.55.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.56 as default\n",
            "Injecting model.layers.13.mlp.experts.56.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.56.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.56.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.56.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.57 as default\n",
            "Injecting model.layers.13.mlp.experts.57.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.57.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.57.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.57.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.58 as default\n",
            "Injecting model.layers.13.mlp.experts.58.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.58.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.58.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.58.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.59 as default\n",
            "Injecting model.layers.13.mlp.experts.59.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.59.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.59.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.59.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.60 as default\n",
            "Injecting model.layers.13.mlp.experts.60.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.60.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.60.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.60.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.61 as default\n",
            "Injecting model.layers.13.mlp.experts.61.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.61.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.61.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.61.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.62 as default\n",
            "Injecting model.layers.13.mlp.experts.62.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.62.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.62.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.62.act_fn as default\n",
            "Injecting model.layers.13.mlp.experts.63 as default\n",
            "Injecting model.layers.13.mlp.experts.63.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.63.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.63.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.experts.63.act_fn as default\n",
            "Injecting model.layers.13.mlp.gate as default\n",
            "Injecting model.layers.13.mlp.shared_experts as default\n",
            "Injecting model.layers.13.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.13.input_layernorm as default\n",
            "Injecting model.layers.13.post_attention_layernorm as default\n",
            "Injecting model.layers.14 as default\n",
            "Injecting model.layers.14.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.14.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.14.self_attn.kv_b_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.rotary_emb as default\n",
            "Injecting model.layers.14.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.14.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.14.mlp.experts.0 as default\n",
            "Injecting model.layers.14.mlp.experts.0.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.0.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.0.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.0.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.1 as default\n",
            "Injecting model.layers.14.mlp.experts.1.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.1.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.1.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.1.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.2 as default\n",
            "Injecting model.layers.14.mlp.experts.2.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.2.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.2.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.2.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.3 as default\n",
            "Injecting model.layers.14.mlp.experts.3.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.3.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.3.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.3.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.4 as default\n",
            "Injecting model.layers.14.mlp.experts.4.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.4.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.4.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.4.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.5 as default\n",
            "Injecting model.layers.14.mlp.experts.5.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.5.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.5.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.5.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.6 as default\n",
            "Injecting model.layers.14.mlp.experts.6.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.6.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.6.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.6.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.7 as default\n",
            "Injecting model.layers.14.mlp.experts.7.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.7.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.7.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.7.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.8 as default\n",
            "Injecting model.layers.14.mlp.experts.8.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.8.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.8.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.8.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.9 as default\n",
            "Injecting model.layers.14.mlp.experts.9.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.9.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.9.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.9.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.10 as default\n",
            "Injecting model.layers.14.mlp.experts.10.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.10.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.10.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.10.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.11 as default\n",
            "Injecting model.layers.14.mlp.experts.11.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.11.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.11.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.11.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.12 as default\n",
            "Injecting model.layers.14.mlp.experts.12.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.12.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.12.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.12.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.13 as default\n",
            "Injecting model.layers.14.mlp.experts.13.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.13.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.13.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.13.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.14 as default\n",
            "Injecting model.layers.14.mlp.experts.14.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.14.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.14.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.14.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.15 as default\n",
            "Injecting model.layers.14.mlp.experts.15.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.15.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.15.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.15.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.16 as default\n",
            "Injecting model.layers.14.mlp.experts.16.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.16.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.16.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.16.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.17 as default\n",
            "Injecting model.layers.14.mlp.experts.17.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.17.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.17.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.17.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.18 as default\n",
            "Injecting model.layers.14.mlp.experts.18.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.18.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.18.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.18.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.19 as default\n",
            "Injecting model.layers.14.mlp.experts.19.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.19.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.19.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.19.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.20 as default\n",
            "Injecting model.layers.14.mlp.experts.20.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.20.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.20.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.20.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.21 as default\n",
            "Injecting model.layers.14.mlp.experts.21.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.21.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.21.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.21.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.22 as default\n",
            "Injecting model.layers.14.mlp.experts.22.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.22.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.22.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.22.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.23 as default\n",
            "Injecting model.layers.14.mlp.experts.23.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.23.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.23.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.23.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.24 as default\n",
            "Injecting model.layers.14.mlp.experts.24.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.24.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.24.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.24.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.25 as default\n",
            "Injecting model.layers.14.mlp.experts.25.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.25.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.25.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.25.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.26 as default\n",
            "Injecting model.layers.14.mlp.experts.26.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.26.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.26.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.26.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.27 as default\n",
            "Injecting model.layers.14.mlp.experts.27.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.27.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.27.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.27.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.28 as default\n",
            "Injecting model.layers.14.mlp.experts.28.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.28.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.28.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.28.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.29 as default\n",
            "Injecting model.layers.14.mlp.experts.29.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.29.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.29.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.29.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.30 as default\n",
            "Injecting model.layers.14.mlp.experts.30.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.30.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.30.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.30.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.31 as default\n",
            "Injecting model.layers.14.mlp.experts.31.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.31.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.31.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.31.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.32 as default\n",
            "Injecting model.layers.14.mlp.experts.32.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.32.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.32.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.32.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.33 as default\n",
            "Injecting model.layers.14.mlp.experts.33.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.33.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.33.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.33.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.34 as default\n",
            "Injecting model.layers.14.mlp.experts.34.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.34.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.34.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.34.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.35 as default\n",
            "Injecting model.layers.14.mlp.experts.35.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.35.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.35.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.35.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.36 as default\n",
            "Injecting model.layers.14.mlp.experts.36.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.36.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.36.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.36.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.37 as default\n",
            "Injecting model.layers.14.mlp.experts.37.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.37.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.37.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.37.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.38 as default\n",
            "Injecting model.layers.14.mlp.experts.38.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.38.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.38.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.38.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.39 as default\n",
            "Injecting model.layers.14.mlp.experts.39.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.39.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.39.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.39.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.40 as default\n",
            "Injecting model.layers.14.mlp.experts.40.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.40.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.40.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.40.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.41 as default\n",
            "Injecting model.layers.14.mlp.experts.41.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.41.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.41.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.41.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.42 as default\n",
            "Injecting model.layers.14.mlp.experts.42.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.42.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.42.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.42.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.43 as default\n",
            "Injecting model.layers.14.mlp.experts.43.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.43.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.43.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.43.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.44 as default\n",
            "Injecting model.layers.14.mlp.experts.44.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.44.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.44.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.44.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.45 as default\n",
            "Injecting model.layers.14.mlp.experts.45.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.45.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.45.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.45.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.46 as default\n",
            "Injecting model.layers.14.mlp.experts.46.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.46.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.46.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.46.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.47 as default\n",
            "Injecting model.layers.14.mlp.experts.47.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.47.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.47.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.47.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.48 as default\n",
            "Injecting model.layers.14.mlp.experts.48.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.48.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.48.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.48.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.49 as default\n",
            "Injecting model.layers.14.mlp.experts.49.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.49.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.49.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.49.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.50 as default\n",
            "Injecting model.layers.14.mlp.experts.50.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.50.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.50.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.50.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.51 as default\n",
            "Injecting model.layers.14.mlp.experts.51.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.51.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.51.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.51.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.52 as default\n",
            "Injecting model.layers.14.mlp.experts.52.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.52.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.52.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.52.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.53 as default\n",
            "Injecting model.layers.14.mlp.experts.53.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.53.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.53.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.53.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.54 as default\n",
            "Injecting model.layers.14.mlp.experts.54.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.54.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.54.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.54.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.55 as default\n",
            "Injecting model.layers.14.mlp.experts.55.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.55.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.55.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.55.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.56 as default\n",
            "Injecting model.layers.14.mlp.experts.56.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.56.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.56.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.56.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.57 as default\n",
            "Injecting model.layers.14.mlp.experts.57.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.57.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.57.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.57.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.58 as default\n",
            "Injecting model.layers.14.mlp.experts.58.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.58.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.58.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.58.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.59 as default\n",
            "Injecting model.layers.14.mlp.experts.59.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.59.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.59.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.59.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.60 as default\n",
            "Injecting model.layers.14.mlp.experts.60.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.60.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.60.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.60.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.61 as default\n",
            "Injecting model.layers.14.mlp.experts.61.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.61.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.61.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.61.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.62 as default\n",
            "Injecting model.layers.14.mlp.experts.62.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.62.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.62.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.62.act_fn as default\n",
            "Injecting model.layers.14.mlp.experts.63 as default\n",
            "Injecting model.layers.14.mlp.experts.63.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.63.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.63.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.experts.63.act_fn as default\n",
            "Injecting model.layers.14.mlp.gate as default\n",
            "Injecting model.layers.14.mlp.shared_experts as default\n",
            "Injecting model.layers.14.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.14.input_layernorm as default\n",
            "Injecting model.layers.14.post_attention_layernorm as default\n",
            "Injecting model.layers.15 as default\n",
            "Injecting model.layers.15.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.15.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.15.self_attn.kv_b_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.rotary_emb as default\n",
            "Injecting model.layers.15.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.15.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.15.mlp.experts.0 as default\n",
            "Injecting model.layers.15.mlp.experts.0.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.0.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.0.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.0.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.1 as default\n",
            "Injecting model.layers.15.mlp.experts.1.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.1.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.1.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.1.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.2 as default\n",
            "Injecting model.layers.15.mlp.experts.2.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.2.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.2.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.2.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.3 as default\n",
            "Injecting model.layers.15.mlp.experts.3.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.3.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.3.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.3.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.4 as default\n",
            "Injecting model.layers.15.mlp.experts.4.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.4.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.4.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.4.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.5 as default\n",
            "Injecting model.layers.15.mlp.experts.5.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.5.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.5.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.5.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.6 as default\n",
            "Injecting model.layers.15.mlp.experts.6.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.6.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.6.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.6.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.7 as default\n",
            "Injecting model.layers.15.mlp.experts.7.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.7.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.7.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.7.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.8 as default\n",
            "Injecting model.layers.15.mlp.experts.8.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.8.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.8.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.8.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.9 as default\n",
            "Injecting model.layers.15.mlp.experts.9.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.9.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.9.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.9.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.10 as default\n",
            "Injecting model.layers.15.mlp.experts.10.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.10.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.10.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.10.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.11 as default\n",
            "Injecting model.layers.15.mlp.experts.11.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.11.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.11.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.11.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.12 as default\n",
            "Injecting model.layers.15.mlp.experts.12.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.12.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.12.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.12.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.13 as default\n",
            "Injecting model.layers.15.mlp.experts.13.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.13.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.13.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.13.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.14 as default\n",
            "Injecting model.layers.15.mlp.experts.14.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.14.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.14.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.14.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.15 as default\n",
            "Injecting model.layers.15.mlp.experts.15.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.15.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.15.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.15.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.16 as default\n",
            "Injecting model.layers.15.mlp.experts.16.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.16.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.16.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.16.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.17 as default\n",
            "Injecting model.layers.15.mlp.experts.17.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.17.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.17.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.17.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.18 as default\n",
            "Injecting model.layers.15.mlp.experts.18.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.18.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.18.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.18.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.19 as default\n",
            "Injecting model.layers.15.mlp.experts.19.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.19.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.19.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.19.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.20 as default\n",
            "Injecting model.layers.15.mlp.experts.20.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.20.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.20.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.20.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.21 as default\n",
            "Injecting model.layers.15.mlp.experts.21.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.21.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.21.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.21.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.22 as default\n",
            "Injecting model.layers.15.mlp.experts.22.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.22.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.22.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.22.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.23 as default\n",
            "Injecting model.layers.15.mlp.experts.23.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.23.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.23.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.23.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.24 as default\n",
            "Injecting model.layers.15.mlp.experts.24.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.24.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.24.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.24.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.25 as default\n",
            "Injecting model.layers.15.mlp.experts.25.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.25.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.25.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.25.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.26 as default\n",
            "Injecting model.layers.15.mlp.experts.26.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.26.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.26.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.26.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.27 as default\n",
            "Injecting model.layers.15.mlp.experts.27.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.27.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.27.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.27.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.28 as default\n",
            "Injecting model.layers.15.mlp.experts.28.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.28.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.28.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.28.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.29 as default\n",
            "Injecting model.layers.15.mlp.experts.29.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.29.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.29.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.29.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.30 as default\n",
            "Injecting model.layers.15.mlp.experts.30.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.30.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.30.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.30.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.31 as default\n",
            "Injecting model.layers.15.mlp.experts.31.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.31.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.31.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.31.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.32 as default\n",
            "Injecting model.layers.15.mlp.experts.32.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.32.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.32.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.32.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.33 as default\n",
            "Injecting model.layers.15.mlp.experts.33.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.33.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.33.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.33.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.34 as default\n",
            "Injecting model.layers.15.mlp.experts.34.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.34.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.34.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.34.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.35 as default\n",
            "Injecting model.layers.15.mlp.experts.35.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.35.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.35.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.35.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.36 as default\n",
            "Injecting model.layers.15.mlp.experts.36.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.36.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.36.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.36.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.37 as default\n",
            "Injecting model.layers.15.mlp.experts.37.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.37.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.37.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.37.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.38 as default\n",
            "Injecting model.layers.15.mlp.experts.38.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.38.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.38.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.38.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.39 as default\n",
            "Injecting model.layers.15.mlp.experts.39.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.39.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.39.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.39.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.40 as default\n",
            "Injecting model.layers.15.mlp.experts.40.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.40.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.40.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.40.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.41 as default\n",
            "Injecting model.layers.15.mlp.experts.41.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.41.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.41.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.41.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.42 as default\n",
            "Injecting model.layers.15.mlp.experts.42.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.42.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.42.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.42.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.43 as default\n",
            "Injecting model.layers.15.mlp.experts.43.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.43.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.43.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.43.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.44 as default\n",
            "Injecting model.layers.15.mlp.experts.44.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.44.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.44.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.44.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.45 as default\n",
            "Injecting model.layers.15.mlp.experts.45.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.45.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.45.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.45.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.46 as default\n",
            "Injecting model.layers.15.mlp.experts.46.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.46.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.46.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.46.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.47 as default\n",
            "Injecting model.layers.15.mlp.experts.47.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.47.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.47.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.47.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.48 as default\n",
            "Injecting model.layers.15.mlp.experts.48.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.48.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.48.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.48.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.49 as default\n",
            "Injecting model.layers.15.mlp.experts.49.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.49.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.49.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.49.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.50 as default\n",
            "Injecting model.layers.15.mlp.experts.50.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.50.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.50.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.50.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.51 as default\n",
            "Injecting model.layers.15.mlp.experts.51.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.51.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.51.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.51.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.52 as default\n",
            "Injecting model.layers.15.mlp.experts.52.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.52.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.52.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.52.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.53 as default\n",
            "Injecting model.layers.15.mlp.experts.53.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.53.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.53.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.53.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.54 as default\n",
            "Injecting model.layers.15.mlp.experts.54.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.54.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.54.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.54.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.55 as default\n",
            "Injecting model.layers.15.mlp.experts.55.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.55.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.55.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.55.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.56 as default\n",
            "Injecting model.layers.15.mlp.experts.56.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.56.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.56.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.56.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.57 as default\n",
            "Injecting model.layers.15.mlp.experts.57.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.57.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.57.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.57.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.58 as default\n",
            "Injecting model.layers.15.mlp.experts.58.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.58.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.58.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.58.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.59 as default\n",
            "Injecting model.layers.15.mlp.experts.59.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.59.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.59.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.59.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.60 as default\n",
            "Injecting model.layers.15.mlp.experts.60.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.60.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.60.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.60.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.61 as default\n",
            "Injecting model.layers.15.mlp.experts.61.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.61.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.61.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.61.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.62 as default\n",
            "Injecting model.layers.15.mlp.experts.62.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.62.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.62.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.62.act_fn as default\n",
            "Injecting model.layers.15.mlp.experts.63 as default\n",
            "Injecting model.layers.15.mlp.experts.63.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.63.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.63.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.experts.63.act_fn as default\n",
            "Injecting model.layers.15.mlp.gate as default\n",
            "Injecting model.layers.15.mlp.shared_experts as default\n",
            "Injecting model.layers.15.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.15.input_layernorm as default\n",
            "Injecting model.layers.15.post_attention_layernorm as default\n",
            "Injecting model.layers.16 as default\n",
            "Injecting model.layers.16.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.16.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.16.self_attn.kv_b_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.rotary_emb as default\n",
            "Injecting model.layers.16.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.16.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.16.mlp.experts.0 as default\n",
            "Injecting model.layers.16.mlp.experts.0.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.0.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.0.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.0.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.1 as default\n",
            "Injecting model.layers.16.mlp.experts.1.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.1.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.1.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.1.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.2 as default\n",
            "Injecting model.layers.16.mlp.experts.2.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.2.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.2.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.2.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.3 as default\n",
            "Injecting model.layers.16.mlp.experts.3.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.3.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.3.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.3.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.4 as default\n",
            "Injecting model.layers.16.mlp.experts.4.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.4.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.4.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.4.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.5 as default\n",
            "Injecting model.layers.16.mlp.experts.5.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.5.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.5.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.5.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.6 as default\n",
            "Injecting model.layers.16.mlp.experts.6.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.6.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.6.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.6.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.7 as default\n",
            "Injecting model.layers.16.mlp.experts.7.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.7.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.7.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.7.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.8 as default\n",
            "Injecting model.layers.16.mlp.experts.8.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.8.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.8.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.8.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.9 as default\n",
            "Injecting model.layers.16.mlp.experts.9.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.9.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.9.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.9.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.10 as default\n",
            "Injecting model.layers.16.mlp.experts.10.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.10.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.10.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.10.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.11 as default\n",
            "Injecting model.layers.16.mlp.experts.11.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.11.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.11.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.11.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.12 as default\n",
            "Injecting model.layers.16.mlp.experts.12.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.12.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.12.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.12.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.13 as default\n",
            "Injecting model.layers.16.mlp.experts.13.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.13.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.13.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.13.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.14 as default\n",
            "Injecting model.layers.16.mlp.experts.14.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.14.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.14.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.14.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.15 as default\n",
            "Injecting model.layers.16.mlp.experts.15.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.15.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.15.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.15.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.16 as default\n",
            "Injecting model.layers.16.mlp.experts.16.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.16.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.16.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.16.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.17 as default\n",
            "Injecting model.layers.16.mlp.experts.17.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.17.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.17.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.17.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.18 as default\n",
            "Injecting model.layers.16.mlp.experts.18.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.18.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.18.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.18.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.19 as default\n",
            "Injecting model.layers.16.mlp.experts.19.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.19.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.19.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.19.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.20 as default\n",
            "Injecting model.layers.16.mlp.experts.20.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.20.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.20.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.20.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.21 as default\n",
            "Injecting model.layers.16.mlp.experts.21.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.21.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.21.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.21.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.22 as default\n",
            "Injecting model.layers.16.mlp.experts.22.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.22.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.22.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.22.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.23 as default\n",
            "Injecting model.layers.16.mlp.experts.23.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.23.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.23.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.23.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.24 as default\n",
            "Injecting model.layers.16.mlp.experts.24.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.24.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.24.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.24.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.25 as default\n",
            "Injecting model.layers.16.mlp.experts.25.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.25.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.25.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.25.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.26 as default\n",
            "Injecting model.layers.16.mlp.experts.26.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.26.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.26.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.26.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.27 as default\n",
            "Injecting model.layers.16.mlp.experts.27.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.27.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.27.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.27.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.28 as default\n",
            "Injecting model.layers.16.mlp.experts.28.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.28.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.28.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.28.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.29 as default\n",
            "Injecting model.layers.16.mlp.experts.29.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.29.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.29.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.29.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.30 as default\n",
            "Injecting model.layers.16.mlp.experts.30.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.30.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.30.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.30.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.31 as default\n",
            "Injecting model.layers.16.mlp.experts.31.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.31.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.31.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.31.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.32 as default\n",
            "Injecting model.layers.16.mlp.experts.32.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.32.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.32.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.32.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.33 as default\n",
            "Injecting model.layers.16.mlp.experts.33.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.33.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.33.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.33.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.34 as default\n",
            "Injecting model.layers.16.mlp.experts.34.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.34.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.34.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.34.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.35 as default\n",
            "Injecting model.layers.16.mlp.experts.35.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.35.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.35.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.35.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.36 as default\n",
            "Injecting model.layers.16.mlp.experts.36.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.36.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.36.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.36.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.37 as default\n",
            "Injecting model.layers.16.mlp.experts.37.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.37.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.37.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.37.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.38 as default\n",
            "Injecting model.layers.16.mlp.experts.38.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.38.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.38.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.38.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.39 as default\n",
            "Injecting model.layers.16.mlp.experts.39.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.39.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.39.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.39.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.40 as default\n",
            "Injecting model.layers.16.mlp.experts.40.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.40.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.40.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.40.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.41 as default\n",
            "Injecting model.layers.16.mlp.experts.41.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.41.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.41.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.41.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.42 as default\n",
            "Injecting model.layers.16.mlp.experts.42.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.42.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.42.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.42.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.43 as default\n",
            "Injecting model.layers.16.mlp.experts.43.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.43.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.43.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.43.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.44 as default\n",
            "Injecting model.layers.16.mlp.experts.44.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.44.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.44.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.44.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.45 as default\n",
            "Injecting model.layers.16.mlp.experts.45.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.45.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.45.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.45.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.46 as default\n",
            "Injecting model.layers.16.mlp.experts.46.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.46.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.46.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.46.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.47 as default\n",
            "Injecting model.layers.16.mlp.experts.47.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.47.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.47.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.47.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.48 as default\n",
            "Injecting model.layers.16.mlp.experts.48.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.48.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.48.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.48.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.49 as default\n",
            "Injecting model.layers.16.mlp.experts.49.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.49.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.49.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.49.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.50 as default\n",
            "Injecting model.layers.16.mlp.experts.50.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.50.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.50.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.50.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.51 as default\n",
            "Injecting model.layers.16.mlp.experts.51.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.51.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.51.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.51.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.52 as default\n",
            "Injecting model.layers.16.mlp.experts.52.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.52.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.52.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.52.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.53 as default\n",
            "Injecting model.layers.16.mlp.experts.53.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.53.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.53.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.53.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.54 as default\n",
            "Injecting model.layers.16.mlp.experts.54.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.54.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.54.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.54.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.55 as default\n",
            "Injecting model.layers.16.mlp.experts.55.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.55.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.55.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.55.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.56 as default\n",
            "Injecting model.layers.16.mlp.experts.56.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.56.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.56.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.56.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.57 as default\n",
            "Injecting model.layers.16.mlp.experts.57.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.57.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.57.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.57.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.58 as default\n",
            "Injecting model.layers.16.mlp.experts.58.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.58.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.58.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.58.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.59 as default\n",
            "Injecting model.layers.16.mlp.experts.59.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.59.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.59.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.59.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.60 as default\n",
            "Injecting model.layers.16.mlp.experts.60.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.60.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.60.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.60.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.61 as default\n",
            "Injecting model.layers.16.mlp.experts.61.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.61.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.61.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.61.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.62 as default\n",
            "Injecting model.layers.16.mlp.experts.62.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.62.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.62.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.62.act_fn as default\n",
            "Injecting model.layers.16.mlp.experts.63 as default\n",
            "Injecting model.layers.16.mlp.experts.63.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.63.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.63.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.experts.63.act_fn as default\n",
            "Injecting model.layers.16.mlp.gate as default\n",
            "Injecting model.layers.16.mlp.shared_experts as default\n",
            "Injecting model.layers.16.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.16.input_layernorm as default\n",
            "Injecting model.layers.16.post_attention_layernorm as default\n",
            "Injecting model.layers.17 as default\n",
            "Injecting model.layers.17.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.17.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.17.self_attn.kv_b_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.rotary_emb as default\n",
            "Injecting model.layers.17.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.17.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.17.mlp.experts.0 as default\n",
            "Injecting model.layers.17.mlp.experts.0.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.0.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.0.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.0.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.1 as default\n",
            "Injecting model.layers.17.mlp.experts.1.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.1.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.1.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.1.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.2 as default\n",
            "Injecting model.layers.17.mlp.experts.2.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.2.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.2.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.2.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.3 as default\n",
            "Injecting model.layers.17.mlp.experts.3.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.3.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.3.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.3.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.4 as default\n",
            "Injecting model.layers.17.mlp.experts.4.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.4.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.4.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.4.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.5 as default\n",
            "Injecting model.layers.17.mlp.experts.5.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.5.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.5.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.5.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.6 as default\n",
            "Injecting model.layers.17.mlp.experts.6.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.6.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.6.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.6.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.7 as default\n",
            "Injecting model.layers.17.mlp.experts.7.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.7.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.7.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.7.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.8 as default\n",
            "Injecting model.layers.17.mlp.experts.8.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.8.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.8.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.8.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.9 as default\n",
            "Injecting model.layers.17.mlp.experts.9.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.9.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.9.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.9.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.10 as default\n",
            "Injecting model.layers.17.mlp.experts.10.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.10.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.10.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.10.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.11 as default\n",
            "Injecting model.layers.17.mlp.experts.11.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.11.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.11.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.11.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.12 as default\n",
            "Injecting model.layers.17.mlp.experts.12.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.12.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.12.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.12.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.13 as default\n",
            "Injecting model.layers.17.mlp.experts.13.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.13.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.13.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.13.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.14 as default\n",
            "Injecting model.layers.17.mlp.experts.14.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.14.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.14.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.14.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.15 as default\n",
            "Injecting model.layers.17.mlp.experts.15.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.15.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.15.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.15.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.16 as default\n",
            "Injecting model.layers.17.mlp.experts.16.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.16.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.16.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.16.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.17 as default\n",
            "Injecting model.layers.17.mlp.experts.17.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.17.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.17.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.17.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.18 as default\n",
            "Injecting model.layers.17.mlp.experts.18.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.18.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.18.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.18.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.19 as default\n",
            "Injecting model.layers.17.mlp.experts.19.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.19.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.19.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.19.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.20 as default\n",
            "Injecting model.layers.17.mlp.experts.20.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.20.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.20.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.20.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.21 as default\n",
            "Injecting model.layers.17.mlp.experts.21.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.21.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.21.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.21.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.22 as default\n",
            "Injecting model.layers.17.mlp.experts.22.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.22.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.22.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.22.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.23 as default\n",
            "Injecting model.layers.17.mlp.experts.23.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.23.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.23.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.23.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.24 as default\n",
            "Injecting model.layers.17.mlp.experts.24.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.24.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.24.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.24.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.25 as default\n",
            "Injecting model.layers.17.mlp.experts.25.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.25.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.25.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.25.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.26 as default\n",
            "Injecting model.layers.17.mlp.experts.26.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.26.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.26.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.26.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.27 as default\n",
            "Injecting model.layers.17.mlp.experts.27.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.27.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.27.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.27.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.28 as default\n",
            "Injecting model.layers.17.mlp.experts.28.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.28.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.28.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.28.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.29 as default\n",
            "Injecting model.layers.17.mlp.experts.29.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.29.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.29.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.29.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.30 as default\n",
            "Injecting model.layers.17.mlp.experts.30.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.30.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.30.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.30.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.31 as default\n",
            "Injecting model.layers.17.mlp.experts.31.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.31.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.31.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.31.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.32 as default\n",
            "Injecting model.layers.17.mlp.experts.32.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.32.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.32.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.32.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.33 as default\n",
            "Injecting model.layers.17.mlp.experts.33.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.33.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.33.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.33.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.34 as default\n",
            "Injecting model.layers.17.mlp.experts.34.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.34.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.34.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.34.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.35 as default\n",
            "Injecting model.layers.17.mlp.experts.35.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.35.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.35.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.35.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.36 as default\n",
            "Injecting model.layers.17.mlp.experts.36.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.36.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.36.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.36.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.37 as default\n",
            "Injecting model.layers.17.mlp.experts.37.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.37.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.37.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.37.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.38 as default\n",
            "Injecting model.layers.17.mlp.experts.38.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.38.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.38.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.38.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.39 as default\n",
            "Injecting model.layers.17.mlp.experts.39.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.39.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.39.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.39.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.40 as default\n",
            "Injecting model.layers.17.mlp.experts.40.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.40.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.40.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.40.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.41 as default\n",
            "Injecting model.layers.17.mlp.experts.41.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.41.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.41.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.41.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.42 as default\n",
            "Injecting model.layers.17.mlp.experts.42.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.42.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.42.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.42.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.43 as default\n",
            "Injecting model.layers.17.mlp.experts.43.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.43.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.43.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.43.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.44 as default\n",
            "Injecting model.layers.17.mlp.experts.44.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.44.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.44.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.44.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.45 as default\n",
            "Injecting model.layers.17.mlp.experts.45.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.45.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.45.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.45.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.46 as default\n",
            "Injecting model.layers.17.mlp.experts.46.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.46.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.46.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.46.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.47 as default\n",
            "Injecting model.layers.17.mlp.experts.47.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.47.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.47.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.47.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.48 as default\n",
            "Injecting model.layers.17.mlp.experts.48.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.48.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.48.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.48.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.49 as default\n",
            "Injecting model.layers.17.mlp.experts.49.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.49.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.49.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.49.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.50 as default\n",
            "Injecting model.layers.17.mlp.experts.50.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.50.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.50.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.50.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.51 as default\n",
            "Injecting model.layers.17.mlp.experts.51.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.51.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.51.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.51.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.52 as default\n",
            "Injecting model.layers.17.mlp.experts.52.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.52.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.52.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.52.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.53 as default\n",
            "Injecting model.layers.17.mlp.experts.53.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.53.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.53.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.53.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.54 as default\n",
            "Injecting model.layers.17.mlp.experts.54.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.54.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.54.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.54.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.55 as default\n",
            "Injecting model.layers.17.mlp.experts.55.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.55.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.55.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.55.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.56 as default\n",
            "Injecting model.layers.17.mlp.experts.56.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.56.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.56.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.56.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.57 as default\n",
            "Injecting model.layers.17.mlp.experts.57.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.57.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.57.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.57.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.58 as default\n",
            "Injecting model.layers.17.mlp.experts.58.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.58.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.58.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.58.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.59 as default\n",
            "Injecting model.layers.17.mlp.experts.59.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.59.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.59.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.59.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.60 as default\n",
            "Injecting model.layers.17.mlp.experts.60.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.60.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.60.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.60.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.61 as default\n",
            "Injecting model.layers.17.mlp.experts.61.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.61.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.61.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.61.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.62 as default\n",
            "Injecting model.layers.17.mlp.experts.62.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.62.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.62.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.62.act_fn as default\n",
            "Injecting model.layers.17.mlp.experts.63 as default\n",
            "Injecting model.layers.17.mlp.experts.63.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.63.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.63.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.experts.63.act_fn as default\n",
            "Injecting model.layers.17.mlp.gate as default\n",
            "Injecting model.layers.17.mlp.shared_experts as default\n",
            "Injecting model.layers.17.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.17.input_layernorm as default\n",
            "Injecting model.layers.17.post_attention_layernorm as default\n",
            "Injecting model.layers.18 as default\n",
            "Injecting model.layers.18.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.18.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.18.self_attn.kv_b_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.rotary_emb as default\n",
            "Injecting model.layers.18.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.18.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.18.mlp.experts.0 as default\n",
            "Injecting model.layers.18.mlp.experts.0.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.0.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.0.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.0.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.1 as default\n",
            "Injecting model.layers.18.mlp.experts.1.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.1.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.1.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.1.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.2 as default\n",
            "Injecting model.layers.18.mlp.experts.2.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.2.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.2.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.2.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.3 as default\n",
            "Injecting model.layers.18.mlp.experts.3.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.3.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.3.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.3.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.4 as default\n",
            "Injecting model.layers.18.mlp.experts.4.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.4.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.4.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.4.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.5 as default\n",
            "Injecting model.layers.18.mlp.experts.5.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.5.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.5.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.5.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.6 as default\n",
            "Injecting model.layers.18.mlp.experts.6.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.6.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.6.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.6.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.7 as default\n",
            "Injecting model.layers.18.mlp.experts.7.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.7.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.7.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.7.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.8 as default\n",
            "Injecting model.layers.18.mlp.experts.8.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.8.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.8.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.8.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.9 as default\n",
            "Injecting model.layers.18.mlp.experts.9.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.9.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.9.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.9.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.10 as default\n",
            "Injecting model.layers.18.mlp.experts.10.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.10.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.10.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.10.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.11 as default\n",
            "Injecting model.layers.18.mlp.experts.11.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.11.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.11.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.11.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.12 as default\n",
            "Injecting model.layers.18.mlp.experts.12.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.12.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.12.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.12.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.13 as default\n",
            "Injecting model.layers.18.mlp.experts.13.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.13.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.13.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.13.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.14 as default\n",
            "Injecting model.layers.18.mlp.experts.14.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.14.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.14.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.14.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.15 as default\n",
            "Injecting model.layers.18.mlp.experts.15.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.15.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.15.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.15.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.16 as default\n",
            "Injecting model.layers.18.mlp.experts.16.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.16.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.16.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.16.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.17 as default\n",
            "Injecting model.layers.18.mlp.experts.17.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.17.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.17.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.17.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.18 as default\n",
            "Injecting model.layers.18.mlp.experts.18.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.18.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.18.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.18.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.19 as default\n",
            "Injecting model.layers.18.mlp.experts.19.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.19.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.19.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.19.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.20 as default\n",
            "Injecting model.layers.18.mlp.experts.20.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.20.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.20.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.20.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.21 as default\n",
            "Injecting model.layers.18.mlp.experts.21.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.21.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.21.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.21.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.22 as default\n",
            "Injecting model.layers.18.mlp.experts.22.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.22.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.22.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.22.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.23 as default\n",
            "Injecting model.layers.18.mlp.experts.23.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.23.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.23.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.23.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.24 as default\n",
            "Injecting model.layers.18.mlp.experts.24.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.24.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.24.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.24.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.25 as default\n",
            "Injecting model.layers.18.mlp.experts.25.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.25.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.25.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.25.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.26 as default\n",
            "Injecting model.layers.18.mlp.experts.26.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.26.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.26.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.26.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.27 as default\n",
            "Injecting model.layers.18.mlp.experts.27.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.27.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.27.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.27.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.28 as default\n",
            "Injecting model.layers.18.mlp.experts.28.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.28.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.28.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.28.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.29 as default\n",
            "Injecting model.layers.18.mlp.experts.29.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.29.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.29.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.29.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.30 as default\n",
            "Injecting model.layers.18.mlp.experts.30.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.30.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.30.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.30.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.31 as default\n",
            "Injecting model.layers.18.mlp.experts.31.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.31.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.31.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.31.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.32 as default\n",
            "Injecting model.layers.18.mlp.experts.32.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.32.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.32.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.32.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.33 as default\n",
            "Injecting model.layers.18.mlp.experts.33.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.33.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.33.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.33.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.34 as default\n",
            "Injecting model.layers.18.mlp.experts.34.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.34.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.34.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.34.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.35 as default\n",
            "Injecting model.layers.18.mlp.experts.35.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.35.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.35.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.35.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.36 as default\n",
            "Injecting model.layers.18.mlp.experts.36.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.36.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.36.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.36.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.37 as default\n",
            "Injecting model.layers.18.mlp.experts.37.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.37.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.37.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.37.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.38 as default\n",
            "Injecting model.layers.18.mlp.experts.38.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.38.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.38.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.38.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.39 as default\n",
            "Injecting model.layers.18.mlp.experts.39.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.39.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.39.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.39.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.40 as default\n",
            "Injecting model.layers.18.mlp.experts.40.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.40.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.40.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.40.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.41 as default\n",
            "Injecting model.layers.18.mlp.experts.41.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.41.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.41.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.41.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.42 as default\n",
            "Injecting model.layers.18.mlp.experts.42.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.42.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.42.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.42.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.43 as default\n",
            "Injecting model.layers.18.mlp.experts.43.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.43.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.43.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.43.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.44 as default\n",
            "Injecting model.layers.18.mlp.experts.44.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.44.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.44.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.44.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.45 as default\n",
            "Injecting model.layers.18.mlp.experts.45.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.45.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.45.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.45.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.46 as default\n",
            "Injecting model.layers.18.mlp.experts.46.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.46.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.46.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.46.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.47 as default\n",
            "Injecting model.layers.18.mlp.experts.47.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.47.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.47.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.47.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.48 as default\n",
            "Injecting model.layers.18.mlp.experts.48.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.48.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.48.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.48.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.49 as default\n",
            "Injecting model.layers.18.mlp.experts.49.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.49.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.49.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.49.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.50 as default\n",
            "Injecting model.layers.18.mlp.experts.50.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.50.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.50.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.50.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.51 as default\n",
            "Injecting model.layers.18.mlp.experts.51.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.51.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.51.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.51.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.52 as default\n",
            "Injecting model.layers.18.mlp.experts.52.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.52.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.52.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.52.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.53 as default\n",
            "Injecting model.layers.18.mlp.experts.53.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.53.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.53.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.53.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.54 as default\n",
            "Injecting model.layers.18.mlp.experts.54.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.54.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.54.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.54.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.55 as default\n",
            "Injecting model.layers.18.mlp.experts.55.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.55.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.55.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.55.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.56 as default\n",
            "Injecting model.layers.18.mlp.experts.56.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.56.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.56.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.56.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.57 as default\n",
            "Injecting model.layers.18.mlp.experts.57.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.57.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.57.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.57.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.58 as default\n",
            "Injecting model.layers.18.mlp.experts.58.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.58.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.58.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.58.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.59 as default\n",
            "Injecting model.layers.18.mlp.experts.59.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.59.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.59.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.59.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.60 as default\n",
            "Injecting model.layers.18.mlp.experts.60.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.60.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.60.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.60.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.61 as default\n",
            "Injecting model.layers.18.mlp.experts.61.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.61.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.61.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.61.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.62 as default\n",
            "Injecting model.layers.18.mlp.experts.62.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.62.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.62.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.62.act_fn as default\n",
            "Injecting model.layers.18.mlp.experts.63 as default\n",
            "Injecting model.layers.18.mlp.experts.63.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.63.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.63.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.experts.63.act_fn as default\n",
            "Injecting model.layers.18.mlp.gate as default\n",
            "Injecting model.layers.18.mlp.shared_experts as default\n",
            "Injecting model.layers.18.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.18.input_layernorm as default\n",
            "Injecting model.layers.18.post_attention_layernorm as default\n",
            "Injecting model.layers.19 as default\n",
            "Injecting model.layers.19.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.19.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.19.self_attn.kv_b_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.rotary_emb as default\n",
            "Injecting model.layers.19.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.19.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.19.mlp.experts.0 as default\n",
            "Injecting model.layers.19.mlp.experts.0.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.0.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.0.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.0.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.1 as default\n",
            "Injecting model.layers.19.mlp.experts.1.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.1.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.1.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.1.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.2 as default\n",
            "Injecting model.layers.19.mlp.experts.2.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.2.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.2.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.2.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.3 as default\n",
            "Injecting model.layers.19.mlp.experts.3.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.3.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.3.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.3.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.4 as default\n",
            "Injecting model.layers.19.mlp.experts.4.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.4.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.4.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.4.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.5 as default\n",
            "Injecting model.layers.19.mlp.experts.5.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.5.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.5.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.5.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.6 as default\n",
            "Injecting model.layers.19.mlp.experts.6.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.6.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.6.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.6.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.7 as default\n",
            "Injecting model.layers.19.mlp.experts.7.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.7.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.7.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.7.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.8 as default\n",
            "Injecting model.layers.19.mlp.experts.8.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.8.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.8.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.8.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.9 as default\n",
            "Injecting model.layers.19.mlp.experts.9.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.9.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.9.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.9.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.10 as default\n",
            "Injecting model.layers.19.mlp.experts.10.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.10.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.10.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.10.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.11 as default\n",
            "Injecting model.layers.19.mlp.experts.11.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.11.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.11.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.11.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.12 as default\n",
            "Injecting model.layers.19.mlp.experts.12.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.12.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.12.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.12.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.13 as default\n",
            "Injecting model.layers.19.mlp.experts.13.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.13.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.13.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.13.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.14 as default\n",
            "Injecting model.layers.19.mlp.experts.14.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.14.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.14.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.14.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.15 as default\n",
            "Injecting model.layers.19.mlp.experts.15.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.15.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.15.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.15.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.16 as default\n",
            "Injecting model.layers.19.mlp.experts.16.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.16.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.16.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.16.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.17 as default\n",
            "Injecting model.layers.19.mlp.experts.17.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.17.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.17.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.17.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.18 as default\n",
            "Injecting model.layers.19.mlp.experts.18.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.18.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.18.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.18.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.19 as default\n",
            "Injecting model.layers.19.mlp.experts.19.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.19.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.19.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.19.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.20 as default\n",
            "Injecting model.layers.19.mlp.experts.20.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.20.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.20.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.20.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.21 as default\n",
            "Injecting model.layers.19.mlp.experts.21.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.21.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.21.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.21.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.22 as default\n",
            "Injecting model.layers.19.mlp.experts.22.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.22.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.22.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.22.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.23 as default\n",
            "Injecting model.layers.19.mlp.experts.23.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.23.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.23.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.23.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.24 as default\n",
            "Injecting model.layers.19.mlp.experts.24.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.24.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.24.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.24.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.25 as default\n",
            "Injecting model.layers.19.mlp.experts.25.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.25.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.25.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.25.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.26 as default\n",
            "Injecting model.layers.19.mlp.experts.26.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.26.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.26.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.26.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.27 as default\n",
            "Injecting model.layers.19.mlp.experts.27.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.27.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.27.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.27.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.28 as default\n",
            "Injecting model.layers.19.mlp.experts.28.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.28.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.28.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.28.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.29 as default\n",
            "Injecting model.layers.19.mlp.experts.29.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.29.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.29.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.29.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.30 as default\n",
            "Injecting model.layers.19.mlp.experts.30.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.30.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.30.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.30.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.31 as default\n",
            "Injecting model.layers.19.mlp.experts.31.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.31.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.31.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.31.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.32 as default\n",
            "Injecting model.layers.19.mlp.experts.32.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.32.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.32.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.32.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.33 as default\n",
            "Injecting model.layers.19.mlp.experts.33.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.33.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.33.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.33.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.34 as default\n",
            "Injecting model.layers.19.mlp.experts.34.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.34.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.34.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.34.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.35 as default\n",
            "Injecting model.layers.19.mlp.experts.35.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.35.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.35.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.35.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.36 as default\n",
            "Injecting model.layers.19.mlp.experts.36.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.36.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.36.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.36.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.37 as default\n",
            "Injecting model.layers.19.mlp.experts.37.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.37.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.37.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.37.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.38 as default\n",
            "Injecting model.layers.19.mlp.experts.38.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.38.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.38.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.38.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.39 as default\n",
            "Injecting model.layers.19.mlp.experts.39.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.39.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.39.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.39.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.40 as default\n",
            "Injecting model.layers.19.mlp.experts.40.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.40.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.40.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.40.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.41 as default\n",
            "Injecting model.layers.19.mlp.experts.41.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.41.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.41.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.41.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.42 as default\n",
            "Injecting model.layers.19.mlp.experts.42.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.42.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.42.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.42.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.43 as default\n",
            "Injecting model.layers.19.mlp.experts.43.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.43.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.43.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.43.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.44 as default\n",
            "Injecting model.layers.19.mlp.experts.44.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.44.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.44.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.44.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.45 as default\n",
            "Injecting model.layers.19.mlp.experts.45.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.45.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.45.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.45.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.46 as default\n",
            "Injecting model.layers.19.mlp.experts.46.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.46.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.46.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.46.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.47 as default\n",
            "Injecting model.layers.19.mlp.experts.47.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.47.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.47.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.47.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.48 as default\n",
            "Injecting model.layers.19.mlp.experts.48.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.48.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.48.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.48.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.49 as default\n",
            "Injecting model.layers.19.mlp.experts.49.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.49.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.49.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.49.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.50 as default\n",
            "Injecting model.layers.19.mlp.experts.50.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.50.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.50.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.50.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.51 as default\n",
            "Injecting model.layers.19.mlp.experts.51.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.51.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.51.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.51.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.52 as default\n",
            "Injecting model.layers.19.mlp.experts.52.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.52.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.52.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.52.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.53 as default\n",
            "Injecting model.layers.19.mlp.experts.53.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.53.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.53.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.53.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.54 as default\n",
            "Injecting model.layers.19.mlp.experts.54.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.54.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.54.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.54.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.55 as default\n",
            "Injecting model.layers.19.mlp.experts.55.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.55.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.55.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.55.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.56 as default\n",
            "Injecting model.layers.19.mlp.experts.56.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.56.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.56.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.56.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.57 as default\n",
            "Injecting model.layers.19.mlp.experts.57.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.57.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.57.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.57.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.58 as default\n",
            "Injecting model.layers.19.mlp.experts.58.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.58.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.58.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.58.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.59 as default\n",
            "Injecting model.layers.19.mlp.experts.59.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.59.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.59.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.59.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.60 as default\n",
            "Injecting model.layers.19.mlp.experts.60.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.60.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.60.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.60.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.61 as default\n",
            "Injecting model.layers.19.mlp.experts.61.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.61.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.61.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.61.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.62 as default\n",
            "Injecting model.layers.19.mlp.experts.62.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.62.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.62.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.62.act_fn as default\n",
            "Injecting model.layers.19.mlp.experts.63 as default\n",
            "Injecting model.layers.19.mlp.experts.63.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.63.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.63.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.experts.63.act_fn as default\n",
            "Injecting model.layers.19.mlp.gate as default\n",
            "Injecting model.layers.19.mlp.shared_experts as default\n",
            "Injecting model.layers.19.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.19.input_layernorm as default\n",
            "Injecting model.layers.19.post_attention_layernorm as default\n",
            "Injecting model.layers.20 as default\n",
            "Injecting model.layers.20.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.20.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.20.self_attn.kv_b_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.rotary_emb as default\n",
            "Injecting model.layers.20.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.20.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.20.mlp.experts.0 as default\n",
            "Injecting model.layers.20.mlp.experts.0.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.0.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.0.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.0.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.1 as default\n",
            "Injecting model.layers.20.mlp.experts.1.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.1.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.1.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.1.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.2 as default\n",
            "Injecting model.layers.20.mlp.experts.2.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.2.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.2.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.2.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.3 as default\n",
            "Injecting model.layers.20.mlp.experts.3.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.3.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.3.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.3.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.4 as default\n",
            "Injecting model.layers.20.mlp.experts.4.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.4.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.4.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.4.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.5 as default\n",
            "Injecting model.layers.20.mlp.experts.5.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.5.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.5.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.5.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.6 as default\n",
            "Injecting model.layers.20.mlp.experts.6.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.6.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.6.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.6.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.7 as default\n",
            "Injecting model.layers.20.mlp.experts.7.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.7.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.7.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.7.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.8 as default\n",
            "Injecting model.layers.20.mlp.experts.8.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.8.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.8.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.8.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.9 as default\n",
            "Injecting model.layers.20.mlp.experts.9.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.9.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.9.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.9.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.10 as default\n",
            "Injecting model.layers.20.mlp.experts.10.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.10.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.10.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.10.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.11 as default\n",
            "Injecting model.layers.20.mlp.experts.11.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.11.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.11.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.11.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.12 as default\n",
            "Injecting model.layers.20.mlp.experts.12.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.12.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.12.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.12.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.13 as default\n",
            "Injecting model.layers.20.mlp.experts.13.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.13.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.13.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.13.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.14 as default\n",
            "Injecting model.layers.20.mlp.experts.14.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.14.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.14.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.14.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.15 as default\n",
            "Injecting model.layers.20.mlp.experts.15.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.15.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.15.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.15.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.16 as default\n",
            "Injecting model.layers.20.mlp.experts.16.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.16.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.16.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.16.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.17 as default\n",
            "Injecting model.layers.20.mlp.experts.17.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.17.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.17.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.17.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.18 as default\n",
            "Injecting model.layers.20.mlp.experts.18.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.18.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.18.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.18.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.19 as default\n",
            "Injecting model.layers.20.mlp.experts.19.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.19.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.19.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.19.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.20 as default\n",
            "Injecting model.layers.20.mlp.experts.20.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.20.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.20.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.20.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.21 as default\n",
            "Injecting model.layers.20.mlp.experts.21.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.21.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.21.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.21.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.22 as default\n",
            "Injecting model.layers.20.mlp.experts.22.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.22.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.22.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.22.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.23 as default\n",
            "Injecting model.layers.20.mlp.experts.23.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.23.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.23.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.23.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.24 as default\n",
            "Injecting model.layers.20.mlp.experts.24.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.24.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.24.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.24.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.25 as default\n",
            "Injecting model.layers.20.mlp.experts.25.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.25.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.25.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.25.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.26 as default\n",
            "Injecting model.layers.20.mlp.experts.26.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.26.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.26.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.26.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.27 as default\n",
            "Injecting model.layers.20.mlp.experts.27.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.27.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.27.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.27.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.28 as default\n",
            "Injecting model.layers.20.mlp.experts.28.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.28.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.28.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.28.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.29 as default\n",
            "Injecting model.layers.20.mlp.experts.29.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.29.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.29.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.29.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.30 as default\n",
            "Injecting model.layers.20.mlp.experts.30.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.30.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.30.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.30.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.31 as default\n",
            "Injecting model.layers.20.mlp.experts.31.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.31.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.31.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.31.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.32 as default\n",
            "Injecting model.layers.20.mlp.experts.32.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.32.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.32.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.32.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.33 as default\n",
            "Injecting model.layers.20.mlp.experts.33.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.33.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.33.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.33.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.34 as default\n",
            "Injecting model.layers.20.mlp.experts.34.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.34.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.34.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.34.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.35 as default\n",
            "Injecting model.layers.20.mlp.experts.35.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.35.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.35.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.35.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.36 as default\n",
            "Injecting model.layers.20.mlp.experts.36.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.36.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.36.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.36.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.37 as default\n",
            "Injecting model.layers.20.mlp.experts.37.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.37.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.37.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.37.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.38 as default\n",
            "Injecting model.layers.20.mlp.experts.38.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.38.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.38.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.38.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.39 as default\n",
            "Injecting model.layers.20.mlp.experts.39.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.39.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.39.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.39.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.40 as default\n",
            "Injecting model.layers.20.mlp.experts.40.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.40.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.40.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.40.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.41 as default\n",
            "Injecting model.layers.20.mlp.experts.41.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.41.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.41.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.41.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.42 as default\n",
            "Injecting model.layers.20.mlp.experts.42.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.42.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.42.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.42.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.43 as default\n",
            "Injecting model.layers.20.mlp.experts.43.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.43.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.43.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.43.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.44 as default\n",
            "Injecting model.layers.20.mlp.experts.44.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.44.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.44.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.44.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.45 as default\n",
            "Injecting model.layers.20.mlp.experts.45.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.45.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.45.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.45.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.46 as default\n",
            "Injecting model.layers.20.mlp.experts.46.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.46.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.46.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.46.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.47 as default\n",
            "Injecting model.layers.20.mlp.experts.47.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.47.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.47.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.47.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.48 as default\n",
            "Injecting model.layers.20.mlp.experts.48.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.48.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.48.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.48.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.49 as default\n",
            "Injecting model.layers.20.mlp.experts.49.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.49.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.49.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.49.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.50 as default\n",
            "Injecting model.layers.20.mlp.experts.50.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.50.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.50.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.50.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.51 as default\n",
            "Injecting model.layers.20.mlp.experts.51.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.51.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.51.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.51.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.52 as default\n",
            "Injecting model.layers.20.mlp.experts.52.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.52.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.52.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.52.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.53 as default\n",
            "Injecting model.layers.20.mlp.experts.53.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.53.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.53.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.53.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.54 as default\n",
            "Injecting model.layers.20.mlp.experts.54.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.54.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.54.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.54.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.55 as default\n",
            "Injecting model.layers.20.mlp.experts.55.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.55.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.55.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.55.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.56 as default\n",
            "Injecting model.layers.20.mlp.experts.56.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.56.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.56.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.56.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.57 as default\n",
            "Injecting model.layers.20.mlp.experts.57.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.57.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.57.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.57.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.58 as default\n",
            "Injecting model.layers.20.mlp.experts.58.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.58.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.58.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.58.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.59 as default\n",
            "Injecting model.layers.20.mlp.experts.59.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.59.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.59.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.59.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.60 as default\n",
            "Injecting model.layers.20.mlp.experts.60.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.60.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.60.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.60.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.61 as default\n",
            "Injecting model.layers.20.mlp.experts.61.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.61.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.61.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.61.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.62 as default\n",
            "Injecting model.layers.20.mlp.experts.62.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.62.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.62.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.62.act_fn as default\n",
            "Injecting model.layers.20.mlp.experts.63 as default\n",
            "Injecting model.layers.20.mlp.experts.63.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.63.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.63.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.experts.63.act_fn as default\n",
            "Injecting model.layers.20.mlp.gate as default\n",
            "Injecting model.layers.20.mlp.shared_experts as default\n",
            "Injecting model.layers.20.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.20.input_layernorm as default\n",
            "Injecting model.layers.20.post_attention_layernorm as default\n",
            "Injecting model.layers.21 as default\n",
            "Injecting model.layers.21.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.21.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.21.self_attn.kv_b_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.rotary_emb as default\n",
            "Injecting model.layers.21.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.21.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.21.mlp.experts.0 as default\n",
            "Injecting model.layers.21.mlp.experts.0.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.0.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.0.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.0.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.1 as default\n",
            "Injecting model.layers.21.mlp.experts.1.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.1.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.1.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.1.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.2 as default\n",
            "Injecting model.layers.21.mlp.experts.2.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.2.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.2.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.2.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.3 as default\n",
            "Injecting model.layers.21.mlp.experts.3.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.3.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.3.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.3.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.4 as default\n",
            "Injecting model.layers.21.mlp.experts.4.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.4.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.4.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.4.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.5 as default\n",
            "Injecting model.layers.21.mlp.experts.5.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.5.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.5.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.5.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.6 as default\n",
            "Injecting model.layers.21.mlp.experts.6.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.6.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.6.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.6.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.7 as default\n",
            "Injecting model.layers.21.mlp.experts.7.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.7.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.7.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.7.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.8 as default\n",
            "Injecting model.layers.21.mlp.experts.8.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.8.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.8.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.8.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.9 as default\n",
            "Injecting model.layers.21.mlp.experts.9.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.9.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.9.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.9.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.10 as default\n",
            "Injecting model.layers.21.mlp.experts.10.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.10.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.10.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.10.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.11 as default\n",
            "Injecting model.layers.21.mlp.experts.11.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.11.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.11.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.11.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.12 as default\n",
            "Injecting model.layers.21.mlp.experts.12.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.12.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.12.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.12.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.13 as default\n",
            "Injecting model.layers.21.mlp.experts.13.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.13.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.13.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.13.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.14 as default\n",
            "Injecting model.layers.21.mlp.experts.14.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.14.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.14.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.14.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.15 as default\n",
            "Injecting model.layers.21.mlp.experts.15.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.15.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.15.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.15.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.16 as default\n",
            "Injecting model.layers.21.mlp.experts.16.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.16.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.16.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.16.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.17 as default\n",
            "Injecting model.layers.21.mlp.experts.17.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.17.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.17.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.17.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.18 as default\n",
            "Injecting model.layers.21.mlp.experts.18.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.18.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.18.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.18.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.19 as default\n",
            "Injecting model.layers.21.mlp.experts.19.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.19.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.19.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.19.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.20 as default\n",
            "Injecting model.layers.21.mlp.experts.20.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.20.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.20.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.20.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.21 as default\n",
            "Injecting model.layers.21.mlp.experts.21.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.21.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.21.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.21.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.22 as default\n",
            "Injecting model.layers.21.mlp.experts.22.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.22.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.22.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.22.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.23 as default\n",
            "Injecting model.layers.21.mlp.experts.23.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.23.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.23.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.23.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.24 as default\n",
            "Injecting model.layers.21.mlp.experts.24.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.24.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.24.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.24.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.25 as default\n",
            "Injecting model.layers.21.mlp.experts.25.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.25.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.25.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.25.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.26 as default\n",
            "Injecting model.layers.21.mlp.experts.26.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.26.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.26.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.26.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.27 as default\n",
            "Injecting model.layers.21.mlp.experts.27.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.27.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.27.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.27.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.28 as default\n",
            "Injecting model.layers.21.mlp.experts.28.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.28.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.28.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.28.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.29 as default\n",
            "Injecting model.layers.21.mlp.experts.29.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.29.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.29.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.29.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.30 as default\n",
            "Injecting model.layers.21.mlp.experts.30.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.30.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.30.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.30.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.31 as default\n",
            "Injecting model.layers.21.mlp.experts.31.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.31.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.31.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.31.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.32 as default\n",
            "Injecting model.layers.21.mlp.experts.32.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.32.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.32.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.32.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.33 as default\n",
            "Injecting model.layers.21.mlp.experts.33.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.33.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.33.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.33.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.34 as default\n",
            "Injecting model.layers.21.mlp.experts.34.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.34.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.34.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.34.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.35 as default\n",
            "Injecting model.layers.21.mlp.experts.35.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.35.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.35.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.35.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.36 as default\n",
            "Injecting model.layers.21.mlp.experts.36.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.36.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.36.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.36.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.37 as default\n",
            "Injecting model.layers.21.mlp.experts.37.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.37.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.37.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.37.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.38 as default\n",
            "Injecting model.layers.21.mlp.experts.38.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.38.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.38.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.38.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.39 as default\n",
            "Injecting model.layers.21.mlp.experts.39.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.39.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.39.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.39.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.40 as default\n",
            "Injecting model.layers.21.mlp.experts.40.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.40.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.40.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.40.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.41 as default\n",
            "Injecting model.layers.21.mlp.experts.41.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.41.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.41.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.41.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.42 as default\n",
            "Injecting model.layers.21.mlp.experts.42.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.42.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.42.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.42.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.43 as default\n",
            "Injecting model.layers.21.mlp.experts.43.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.43.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.43.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.43.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.44 as default\n",
            "Injecting model.layers.21.mlp.experts.44.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.44.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.44.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.44.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.45 as default\n",
            "Injecting model.layers.21.mlp.experts.45.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.45.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.45.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.45.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.46 as default\n",
            "Injecting model.layers.21.mlp.experts.46.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.46.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.46.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.46.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.47 as default\n",
            "Injecting model.layers.21.mlp.experts.47.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.47.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.47.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.47.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.48 as default\n",
            "Injecting model.layers.21.mlp.experts.48.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.48.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.48.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.48.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.49 as default\n",
            "Injecting model.layers.21.mlp.experts.49.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.49.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.49.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.49.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.50 as default\n",
            "Injecting model.layers.21.mlp.experts.50.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.50.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.50.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.50.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.51 as default\n",
            "Injecting model.layers.21.mlp.experts.51.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.51.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.51.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.51.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.52 as default\n",
            "Injecting model.layers.21.mlp.experts.52.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.52.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.52.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.52.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.53 as default\n",
            "Injecting model.layers.21.mlp.experts.53.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.53.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.53.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.53.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.54 as default\n",
            "Injecting model.layers.21.mlp.experts.54.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.54.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.54.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.54.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.55 as default\n",
            "Injecting model.layers.21.mlp.experts.55.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.55.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.55.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.55.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.56 as default\n",
            "Injecting model.layers.21.mlp.experts.56.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.56.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.56.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.56.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.57 as default\n",
            "Injecting model.layers.21.mlp.experts.57.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.57.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.57.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.57.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.58 as default\n",
            "Injecting model.layers.21.mlp.experts.58.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.58.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.58.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.58.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.59 as default\n",
            "Injecting model.layers.21.mlp.experts.59.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.59.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.59.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.59.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.60 as default\n",
            "Injecting model.layers.21.mlp.experts.60.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.60.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.60.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.60.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.61 as default\n",
            "Injecting model.layers.21.mlp.experts.61.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.61.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.61.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.61.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.62 as default\n",
            "Injecting model.layers.21.mlp.experts.62.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.62.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.62.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.62.act_fn as default\n",
            "Injecting model.layers.21.mlp.experts.63 as default\n",
            "Injecting model.layers.21.mlp.experts.63.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.63.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.63.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.experts.63.act_fn as default\n",
            "Injecting model.layers.21.mlp.gate as default\n",
            "Injecting model.layers.21.mlp.shared_experts as default\n",
            "Injecting model.layers.21.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.21.input_layernorm as default\n",
            "Injecting model.layers.21.post_attention_layernorm as default\n",
            "Injecting model.layers.22 as default\n",
            "Injecting model.layers.22.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.22.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.22.self_attn.kv_b_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.rotary_emb as default\n",
            "Injecting model.layers.22.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.22.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.22.mlp.experts.0 as default\n",
            "Injecting model.layers.22.mlp.experts.0.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.0.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.0.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.0.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.1 as default\n",
            "Injecting model.layers.22.mlp.experts.1.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.1.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.1.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.1.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.2 as default\n",
            "Injecting model.layers.22.mlp.experts.2.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.2.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.2.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.2.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.3 as default\n",
            "Injecting model.layers.22.mlp.experts.3.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.3.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.3.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.3.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.4 as default\n",
            "Injecting model.layers.22.mlp.experts.4.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.4.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.4.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.4.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.5 as default\n",
            "Injecting model.layers.22.mlp.experts.5.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.5.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.5.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.5.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.6 as default\n",
            "Injecting model.layers.22.mlp.experts.6.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.6.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.6.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.6.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.7 as default\n",
            "Injecting model.layers.22.mlp.experts.7.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.7.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.7.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.7.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.8 as default\n",
            "Injecting model.layers.22.mlp.experts.8.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.8.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.8.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.8.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.9 as default\n",
            "Injecting model.layers.22.mlp.experts.9.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.9.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.9.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.9.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.10 as default\n",
            "Injecting model.layers.22.mlp.experts.10.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.10.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.10.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.10.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.11 as default\n",
            "Injecting model.layers.22.mlp.experts.11.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.11.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.11.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.11.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.12 as default\n",
            "Injecting model.layers.22.mlp.experts.12.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.12.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.12.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.12.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.13 as default\n",
            "Injecting model.layers.22.mlp.experts.13.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.13.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.13.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.13.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.14 as default\n",
            "Injecting model.layers.22.mlp.experts.14.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.14.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.14.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.14.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.15 as default\n",
            "Injecting model.layers.22.mlp.experts.15.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.15.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.15.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.15.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.16 as default\n",
            "Injecting model.layers.22.mlp.experts.16.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.16.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.16.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.16.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.17 as default\n",
            "Injecting model.layers.22.mlp.experts.17.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.17.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.17.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.17.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.18 as default\n",
            "Injecting model.layers.22.mlp.experts.18.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.18.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.18.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.18.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.19 as default\n",
            "Injecting model.layers.22.mlp.experts.19.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.19.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.19.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.19.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.20 as default\n",
            "Injecting model.layers.22.mlp.experts.20.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.20.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.20.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.20.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.21 as default\n",
            "Injecting model.layers.22.mlp.experts.21.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.21.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.21.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.21.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.22 as default\n",
            "Injecting model.layers.22.mlp.experts.22.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.22.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.22.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.22.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.23 as default\n",
            "Injecting model.layers.22.mlp.experts.23.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.23.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.23.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.23.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.24 as default\n",
            "Injecting model.layers.22.mlp.experts.24.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.24.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.24.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.24.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.25 as default\n",
            "Injecting model.layers.22.mlp.experts.25.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.25.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.25.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.25.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.26 as default\n",
            "Injecting model.layers.22.mlp.experts.26.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.26.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.26.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.26.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.27 as default\n",
            "Injecting model.layers.22.mlp.experts.27.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.27.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.27.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.27.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.28 as default\n",
            "Injecting model.layers.22.mlp.experts.28.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.28.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.28.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.28.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.29 as default\n",
            "Injecting model.layers.22.mlp.experts.29.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.29.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.29.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.29.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.30 as default\n",
            "Injecting model.layers.22.mlp.experts.30.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.30.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.30.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.30.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.31 as default\n",
            "Injecting model.layers.22.mlp.experts.31.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.31.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.31.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.31.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.32 as default\n",
            "Injecting model.layers.22.mlp.experts.32.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.32.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.32.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.32.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.33 as default\n",
            "Injecting model.layers.22.mlp.experts.33.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.33.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.33.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.33.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.34 as default\n",
            "Injecting model.layers.22.mlp.experts.34.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.34.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.34.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.34.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.35 as default\n",
            "Injecting model.layers.22.mlp.experts.35.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.35.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.35.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.35.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.36 as default\n",
            "Injecting model.layers.22.mlp.experts.36.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.36.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.36.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.36.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.37 as default\n",
            "Injecting model.layers.22.mlp.experts.37.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.37.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.37.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.37.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.38 as default\n",
            "Injecting model.layers.22.mlp.experts.38.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.38.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.38.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.38.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.39 as default\n",
            "Injecting model.layers.22.mlp.experts.39.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.39.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.39.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.39.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.40 as default\n",
            "Injecting model.layers.22.mlp.experts.40.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.40.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.40.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.40.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.41 as default\n",
            "Injecting model.layers.22.mlp.experts.41.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.41.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.41.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.41.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.42 as default\n",
            "Injecting model.layers.22.mlp.experts.42.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.42.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.42.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.42.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.43 as default\n",
            "Injecting model.layers.22.mlp.experts.43.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.43.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.43.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.43.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.44 as default\n",
            "Injecting model.layers.22.mlp.experts.44.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.44.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.44.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.44.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.45 as default\n",
            "Injecting model.layers.22.mlp.experts.45.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.45.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.45.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.45.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.46 as default\n",
            "Injecting model.layers.22.mlp.experts.46.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.46.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.46.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.46.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.47 as default\n",
            "Injecting model.layers.22.mlp.experts.47.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.47.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.47.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.47.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.48 as default\n",
            "Injecting model.layers.22.mlp.experts.48.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.48.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.48.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.48.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.49 as default\n",
            "Injecting model.layers.22.mlp.experts.49.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.49.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.49.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.49.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.50 as default\n",
            "Injecting model.layers.22.mlp.experts.50.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.50.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.50.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.50.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.51 as default\n",
            "Injecting model.layers.22.mlp.experts.51.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.51.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.51.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.51.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.52 as default\n",
            "Injecting model.layers.22.mlp.experts.52.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.52.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.52.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.52.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.53 as default\n",
            "Injecting model.layers.22.mlp.experts.53.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.53.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.53.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.53.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.54 as default\n",
            "Injecting model.layers.22.mlp.experts.54.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.54.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.54.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.54.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.55 as default\n",
            "Injecting model.layers.22.mlp.experts.55.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.55.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.55.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.55.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.56 as default\n",
            "Injecting model.layers.22.mlp.experts.56.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.56.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.56.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.56.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.57 as default\n",
            "Injecting model.layers.22.mlp.experts.57.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.57.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.57.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.57.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.58 as default\n",
            "Injecting model.layers.22.mlp.experts.58.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.58.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.58.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.58.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.59 as default\n",
            "Injecting model.layers.22.mlp.experts.59.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.59.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.59.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.59.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.60 as default\n",
            "Injecting model.layers.22.mlp.experts.60.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.60.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.60.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.60.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.61 as default\n",
            "Injecting model.layers.22.mlp.experts.61.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.61.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.61.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.61.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.62 as default\n",
            "Injecting model.layers.22.mlp.experts.62.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.62.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.62.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.62.act_fn as default\n",
            "Injecting model.layers.22.mlp.experts.63 as default\n",
            "Injecting model.layers.22.mlp.experts.63.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.63.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.63.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.experts.63.act_fn as default\n",
            "Injecting model.layers.22.mlp.gate as default\n",
            "Injecting model.layers.22.mlp.shared_experts as default\n",
            "Injecting model.layers.22.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.22.input_layernorm as default\n",
            "Injecting model.layers.22.post_attention_layernorm as default\n",
            "Injecting model.layers.23 as default\n",
            "Injecting model.layers.23.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.23.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.23.self_attn.kv_b_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.rotary_emb as default\n",
            "Injecting model.layers.23.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.23.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.23.mlp.experts.0 as default\n",
            "Injecting model.layers.23.mlp.experts.0.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.0.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.0.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.0.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.1 as default\n",
            "Injecting model.layers.23.mlp.experts.1.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.1.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.1.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.1.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.2 as default\n",
            "Injecting model.layers.23.mlp.experts.2.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.2.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.2.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.2.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.3 as default\n",
            "Injecting model.layers.23.mlp.experts.3.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.3.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.3.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.3.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.4 as default\n",
            "Injecting model.layers.23.mlp.experts.4.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.4.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.4.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.4.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.5 as default\n",
            "Injecting model.layers.23.mlp.experts.5.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.5.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.5.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.5.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.6 as default\n",
            "Injecting model.layers.23.mlp.experts.6.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.6.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.6.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.6.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.7 as default\n",
            "Injecting model.layers.23.mlp.experts.7.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.7.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.7.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.7.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.8 as default\n",
            "Injecting model.layers.23.mlp.experts.8.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.8.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.8.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.8.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.9 as default\n",
            "Injecting model.layers.23.mlp.experts.9.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.9.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.9.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.9.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.10 as default\n",
            "Injecting model.layers.23.mlp.experts.10.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.10.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.10.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.10.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.11 as default\n",
            "Injecting model.layers.23.mlp.experts.11.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.11.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.11.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.11.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.12 as default\n",
            "Injecting model.layers.23.mlp.experts.12.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.12.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.12.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.12.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.13 as default\n",
            "Injecting model.layers.23.mlp.experts.13.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.13.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.13.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.13.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.14 as default\n",
            "Injecting model.layers.23.mlp.experts.14.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.14.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.14.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.14.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.15 as default\n",
            "Injecting model.layers.23.mlp.experts.15.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.15.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.15.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.15.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.16 as default\n",
            "Injecting model.layers.23.mlp.experts.16.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.16.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.16.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.16.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.17 as default\n",
            "Injecting model.layers.23.mlp.experts.17.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.17.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.17.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.17.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.18 as default\n",
            "Injecting model.layers.23.mlp.experts.18.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.18.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.18.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.18.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.19 as default\n",
            "Injecting model.layers.23.mlp.experts.19.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.19.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.19.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.19.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.20 as default\n",
            "Injecting model.layers.23.mlp.experts.20.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.20.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.20.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.20.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.21 as default\n",
            "Injecting model.layers.23.mlp.experts.21.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.21.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.21.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.21.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.22 as default\n",
            "Injecting model.layers.23.mlp.experts.22.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.22.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.22.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.22.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.23 as default\n",
            "Injecting model.layers.23.mlp.experts.23.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.23.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.23.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.23.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.24 as default\n",
            "Injecting model.layers.23.mlp.experts.24.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.24.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.24.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.24.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.25 as default\n",
            "Injecting model.layers.23.mlp.experts.25.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.25.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.25.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.25.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.26 as default\n",
            "Injecting model.layers.23.mlp.experts.26.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.26.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.26.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.26.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.27 as default\n",
            "Injecting model.layers.23.mlp.experts.27.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.27.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.27.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.27.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.28 as default\n",
            "Injecting model.layers.23.mlp.experts.28.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.28.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.28.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.28.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.29 as default\n",
            "Injecting model.layers.23.mlp.experts.29.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.29.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.29.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.29.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.30 as default\n",
            "Injecting model.layers.23.mlp.experts.30.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.30.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.30.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.30.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.31 as default\n",
            "Injecting model.layers.23.mlp.experts.31.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.31.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.31.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.31.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.32 as default\n",
            "Injecting model.layers.23.mlp.experts.32.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.32.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.32.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.32.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.33 as default\n",
            "Injecting model.layers.23.mlp.experts.33.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.33.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.33.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.33.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.34 as default\n",
            "Injecting model.layers.23.mlp.experts.34.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.34.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.34.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.34.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.35 as default\n",
            "Injecting model.layers.23.mlp.experts.35.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.35.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.35.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.35.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.36 as default\n",
            "Injecting model.layers.23.mlp.experts.36.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.36.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.36.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.36.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.37 as default\n",
            "Injecting model.layers.23.mlp.experts.37.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.37.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.37.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.37.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.38 as default\n",
            "Injecting model.layers.23.mlp.experts.38.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.38.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.38.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.38.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.39 as default\n",
            "Injecting model.layers.23.mlp.experts.39.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.39.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.39.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.39.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.40 as default\n",
            "Injecting model.layers.23.mlp.experts.40.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.40.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.40.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.40.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.41 as default\n",
            "Injecting model.layers.23.mlp.experts.41.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.41.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.41.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.41.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.42 as default\n",
            "Injecting model.layers.23.mlp.experts.42.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.42.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.42.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.42.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.43 as default\n",
            "Injecting model.layers.23.mlp.experts.43.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.43.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.43.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.43.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.44 as default\n",
            "Injecting model.layers.23.mlp.experts.44.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.44.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.44.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.44.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.45 as default\n",
            "Injecting model.layers.23.mlp.experts.45.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.45.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.45.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.45.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.46 as default\n",
            "Injecting model.layers.23.mlp.experts.46.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.46.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.46.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.46.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.47 as default\n",
            "Injecting model.layers.23.mlp.experts.47.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.47.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.47.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.47.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.48 as default\n",
            "Injecting model.layers.23.mlp.experts.48.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.48.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.48.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.48.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.49 as default\n",
            "Injecting model.layers.23.mlp.experts.49.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.49.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.49.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.49.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.50 as default\n",
            "Injecting model.layers.23.mlp.experts.50.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.50.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.50.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.50.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.51 as default\n",
            "Injecting model.layers.23.mlp.experts.51.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.51.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.51.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.51.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.52 as default\n",
            "Injecting model.layers.23.mlp.experts.52.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.52.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.52.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.52.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.53 as default\n",
            "Injecting model.layers.23.mlp.experts.53.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.53.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.53.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.53.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.54 as default\n",
            "Injecting model.layers.23.mlp.experts.54.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.54.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.54.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.54.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.55 as default\n",
            "Injecting model.layers.23.mlp.experts.55.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.55.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.55.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.55.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.56 as default\n",
            "Injecting model.layers.23.mlp.experts.56.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.56.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.56.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.56.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.57 as default\n",
            "Injecting model.layers.23.mlp.experts.57.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.57.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.57.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.57.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.58 as default\n",
            "Injecting model.layers.23.mlp.experts.58.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.58.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.58.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.58.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.59 as default\n",
            "Injecting model.layers.23.mlp.experts.59.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.59.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.59.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.59.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.60 as default\n",
            "Injecting model.layers.23.mlp.experts.60.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.60.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.60.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.60.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.61 as default\n",
            "Injecting model.layers.23.mlp.experts.61.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.61.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.61.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.61.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.62 as default\n",
            "Injecting model.layers.23.mlp.experts.62.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.62.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.62.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.62.act_fn as default\n",
            "Injecting model.layers.23.mlp.experts.63 as default\n",
            "Injecting model.layers.23.mlp.experts.63.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.63.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.63.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.experts.63.act_fn as default\n",
            "Injecting model.layers.23.mlp.gate as default\n",
            "Injecting model.layers.23.mlp.shared_experts as default\n",
            "Injecting model.layers.23.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.23.input_layernorm as default\n",
            "Injecting model.layers.23.post_attention_layernorm as default\n",
            "Injecting model.layers.24 as default\n",
            "Injecting model.layers.24.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.24.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.24.self_attn.kv_b_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.rotary_emb as default\n",
            "Injecting model.layers.24.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.24.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.24.mlp.experts.0 as default\n",
            "Injecting model.layers.24.mlp.experts.0.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.0.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.0.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.0.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.1 as default\n",
            "Injecting model.layers.24.mlp.experts.1.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.1.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.1.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.1.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.2 as default\n",
            "Injecting model.layers.24.mlp.experts.2.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.2.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.2.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.2.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.3 as default\n",
            "Injecting model.layers.24.mlp.experts.3.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.3.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.3.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.3.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.4 as default\n",
            "Injecting model.layers.24.mlp.experts.4.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.4.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.4.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.4.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.5 as default\n",
            "Injecting model.layers.24.mlp.experts.5.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.5.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.5.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.5.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.6 as default\n",
            "Injecting model.layers.24.mlp.experts.6.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.6.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.6.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.6.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.7 as default\n",
            "Injecting model.layers.24.mlp.experts.7.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.7.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.7.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.7.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.8 as default\n",
            "Injecting model.layers.24.mlp.experts.8.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.8.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.8.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.8.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.9 as default\n",
            "Injecting model.layers.24.mlp.experts.9.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.9.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.9.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.9.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.10 as default\n",
            "Injecting model.layers.24.mlp.experts.10.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.10.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.10.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.10.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.11 as default\n",
            "Injecting model.layers.24.mlp.experts.11.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.11.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.11.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.11.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.12 as default\n",
            "Injecting model.layers.24.mlp.experts.12.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.12.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.12.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.12.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.13 as default\n",
            "Injecting model.layers.24.mlp.experts.13.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.13.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.13.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.13.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.14 as default\n",
            "Injecting model.layers.24.mlp.experts.14.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.14.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.14.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.14.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.15 as default\n",
            "Injecting model.layers.24.mlp.experts.15.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.15.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.15.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.15.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.16 as default\n",
            "Injecting model.layers.24.mlp.experts.16.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.16.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.16.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.16.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.17 as default\n",
            "Injecting model.layers.24.mlp.experts.17.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.17.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.17.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.17.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.18 as default\n",
            "Injecting model.layers.24.mlp.experts.18.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.18.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.18.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.18.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.19 as default\n",
            "Injecting model.layers.24.mlp.experts.19.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.19.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.19.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.19.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.20 as default\n",
            "Injecting model.layers.24.mlp.experts.20.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.20.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.20.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.20.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.21 as default\n",
            "Injecting model.layers.24.mlp.experts.21.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.21.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.21.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.21.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.22 as default\n",
            "Injecting model.layers.24.mlp.experts.22.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.22.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.22.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.22.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.23 as default\n",
            "Injecting model.layers.24.mlp.experts.23.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.23.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.23.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.23.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.24 as default\n",
            "Injecting model.layers.24.mlp.experts.24.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.24.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.24.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.24.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.25 as default\n",
            "Injecting model.layers.24.mlp.experts.25.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.25.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.25.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.25.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.26 as default\n",
            "Injecting model.layers.24.mlp.experts.26.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.26.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.26.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.26.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.27 as default\n",
            "Injecting model.layers.24.mlp.experts.27.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.27.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.27.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.27.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.28 as default\n",
            "Injecting model.layers.24.mlp.experts.28.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.28.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.28.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.28.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.29 as default\n",
            "Injecting model.layers.24.mlp.experts.29.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.29.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.29.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.29.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.30 as default\n",
            "Injecting model.layers.24.mlp.experts.30.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.30.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.30.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.30.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.31 as default\n",
            "Injecting model.layers.24.mlp.experts.31.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.31.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.31.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.31.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.32 as default\n",
            "Injecting model.layers.24.mlp.experts.32.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.32.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.32.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.32.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.33 as default\n",
            "Injecting model.layers.24.mlp.experts.33.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.33.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.33.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.33.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.34 as default\n",
            "Injecting model.layers.24.mlp.experts.34.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.34.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.34.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.34.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.35 as default\n",
            "Injecting model.layers.24.mlp.experts.35.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.35.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.35.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.35.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.36 as default\n",
            "Injecting model.layers.24.mlp.experts.36.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.36.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.36.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.36.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.37 as default\n",
            "Injecting model.layers.24.mlp.experts.37.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.37.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.37.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.37.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.38 as default\n",
            "Injecting model.layers.24.mlp.experts.38.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.38.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.38.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.38.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.39 as default\n",
            "Injecting model.layers.24.mlp.experts.39.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.39.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.39.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.39.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.40 as default\n",
            "Injecting model.layers.24.mlp.experts.40.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.40.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.40.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.40.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.41 as default\n",
            "Injecting model.layers.24.mlp.experts.41.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.41.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.41.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.41.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.42 as default\n",
            "Injecting model.layers.24.mlp.experts.42.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.42.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.42.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.42.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.43 as default\n",
            "Injecting model.layers.24.mlp.experts.43.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.43.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.43.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.43.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.44 as default\n",
            "Injecting model.layers.24.mlp.experts.44.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.44.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.44.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.44.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.45 as default\n",
            "Injecting model.layers.24.mlp.experts.45.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.45.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.45.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.45.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.46 as default\n",
            "Injecting model.layers.24.mlp.experts.46.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.46.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.46.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.46.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.47 as default\n",
            "Injecting model.layers.24.mlp.experts.47.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.47.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.47.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.47.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.48 as default\n",
            "Injecting model.layers.24.mlp.experts.48.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.48.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.48.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.48.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.49 as default\n",
            "Injecting model.layers.24.mlp.experts.49.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.49.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.49.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.49.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.50 as default\n",
            "Injecting model.layers.24.mlp.experts.50.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.50.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.50.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.50.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.51 as default\n",
            "Injecting model.layers.24.mlp.experts.51.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.51.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.51.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.51.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.52 as default\n",
            "Injecting model.layers.24.mlp.experts.52.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.52.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.52.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.52.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.53 as default\n",
            "Injecting model.layers.24.mlp.experts.53.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.53.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.53.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.53.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.54 as default\n",
            "Injecting model.layers.24.mlp.experts.54.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.54.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.54.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.54.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.55 as default\n",
            "Injecting model.layers.24.mlp.experts.55.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.55.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.55.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.55.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.56 as default\n",
            "Injecting model.layers.24.mlp.experts.56.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.56.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.56.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.56.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.57 as default\n",
            "Injecting model.layers.24.mlp.experts.57.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.57.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.57.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.57.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.58 as default\n",
            "Injecting model.layers.24.mlp.experts.58.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.58.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.58.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.58.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.59 as default\n",
            "Injecting model.layers.24.mlp.experts.59.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.59.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.59.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.59.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.60 as default\n",
            "Injecting model.layers.24.mlp.experts.60.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.60.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.60.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.60.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.61 as default\n",
            "Injecting model.layers.24.mlp.experts.61.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.61.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.61.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.61.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.62 as default\n",
            "Injecting model.layers.24.mlp.experts.62.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.62.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.62.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.62.act_fn as default\n",
            "Injecting model.layers.24.mlp.experts.63 as default\n",
            "Injecting model.layers.24.mlp.experts.63.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.63.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.63.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.experts.63.act_fn as default\n",
            "Injecting model.layers.24.mlp.gate as default\n",
            "Injecting model.layers.24.mlp.shared_experts as default\n",
            "Injecting model.layers.24.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.24.input_layernorm as default\n",
            "Injecting model.layers.24.post_attention_layernorm as default\n",
            "Injecting model.layers.25 as default\n",
            "Injecting model.layers.25.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.25.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.25.self_attn.kv_b_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.rotary_emb as default\n",
            "Injecting model.layers.25.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.25.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.25.mlp.experts.0 as default\n",
            "Injecting model.layers.25.mlp.experts.0.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.0.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.0.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.0.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.1 as default\n",
            "Injecting model.layers.25.mlp.experts.1.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.1.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.1.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.1.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.2 as default\n",
            "Injecting model.layers.25.mlp.experts.2.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.2.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.2.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.2.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.3 as default\n",
            "Injecting model.layers.25.mlp.experts.3.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.3.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.3.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.3.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.4 as default\n",
            "Injecting model.layers.25.mlp.experts.4.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.4.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.4.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.4.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.5 as default\n",
            "Injecting model.layers.25.mlp.experts.5.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.5.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.5.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.5.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.6 as default\n",
            "Injecting model.layers.25.mlp.experts.6.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.6.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.6.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.6.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.7 as default\n",
            "Injecting model.layers.25.mlp.experts.7.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.7.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.7.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.7.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.8 as default\n",
            "Injecting model.layers.25.mlp.experts.8.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.8.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.8.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.8.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.9 as default\n",
            "Injecting model.layers.25.mlp.experts.9.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.9.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.9.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.9.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.10 as default\n",
            "Injecting model.layers.25.mlp.experts.10.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.10.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.10.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.10.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.11 as default\n",
            "Injecting model.layers.25.mlp.experts.11.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.11.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.11.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.11.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.12 as default\n",
            "Injecting model.layers.25.mlp.experts.12.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.12.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.12.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.12.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.13 as default\n",
            "Injecting model.layers.25.mlp.experts.13.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.13.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.13.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.13.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.14 as default\n",
            "Injecting model.layers.25.mlp.experts.14.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.14.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.14.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.14.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.15 as default\n",
            "Injecting model.layers.25.mlp.experts.15.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.15.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.15.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.15.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.16 as default\n",
            "Injecting model.layers.25.mlp.experts.16.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.16.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.16.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.16.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.17 as default\n",
            "Injecting model.layers.25.mlp.experts.17.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.17.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.17.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.17.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.18 as default\n",
            "Injecting model.layers.25.mlp.experts.18.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.18.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.18.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.18.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.19 as default\n",
            "Injecting model.layers.25.mlp.experts.19.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.19.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.19.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.19.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.20 as default\n",
            "Injecting model.layers.25.mlp.experts.20.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.20.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.20.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.20.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.21 as default\n",
            "Injecting model.layers.25.mlp.experts.21.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.21.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.21.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.21.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.22 as default\n",
            "Injecting model.layers.25.mlp.experts.22.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.22.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.22.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.22.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.23 as default\n",
            "Injecting model.layers.25.mlp.experts.23.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.23.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.23.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.23.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.24 as default\n",
            "Injecting model.layers.25.mlp.experts.24.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.24.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.24.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.24.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.25 as default\n",
            "Injecting model.layers.25.mlp.experts.25.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.25.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.25.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.25.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.26 as default\n",
            "Injecting model.layers.25.mlp.experts.26.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.26.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.26.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.26.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.27 as default\n",
            "Injecting model.layers.25.mlp.experts.27.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.27.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.27.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.27.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.28 as default\n",
            "Injecting model.layers.25.mlp.experts.28.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.28.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.28.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.28.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.29 as default\n",
            "Injecting model.layers.25.mlp.experts.29.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.29.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.29.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.29.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.30 as default\n",
            "Injecting model.layers.25.mlp.experts.30.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.30.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.30.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.30.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.31 as default\n",
            "Injecting model.layers.25.mlp.experts.31.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.31.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.31.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.31.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.32 as default\n",
            "Injecting model.layers.25.mlp.experts.32.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.32.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.32.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.32.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.33 as default\n",
            "Injecting model.layers.25.mlp.experts.33.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.33.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.33.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.33.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.34 as default\n",
            "Injecting model.layers.25.mlp.experts.34.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.34.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.34.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.34.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.35 as default\n",
            "Injecting model.layers.25.mlp.experts.35.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.35.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.35.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.35.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.36 as default\n",
            "Injecting model.layers.25.mlp.experts.36.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.36.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.36.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.36.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.37 as default\n",
            "Injecting model.layers.25.mlp.experts.37.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.37.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.37.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.37.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.38 as default\n",
            "Injecting model.layers.25.mlp.experts.38.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.38.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.38.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.38.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.39 as default\n",
            "Injecting model.layers.25.mlp.experts.39.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.39.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.39.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.39.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.40 as default\n",
            "Injecting model.layers.25.mlp.experts.40.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.40.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.40.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.40.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.41 as default\n",
            "Injecting model.layers.25.mlp.experts.41.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.41.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.41.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.41.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.42 as default\n",
            "Injecting model.layers.25.mlp.experts.42.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.42.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.42.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.42.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.43 as default\n",
            "Injecting model.layers.25.mlp.experts.43.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.43.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.43.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.43.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.44 as default\n",
            "Injecting model.layers.25.mlp.experts.44.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.44.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.44.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.44.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.45 as default\n",
            "Injecting model.layers.25.mlp.experts.45.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.45.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.45.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.45.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.46 as default\n",
            "Injecting model.layers.25.mlp.experts.46.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.46.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.46.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.46.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.47 as default\n",
            "Injecting model.layers.25.mlp.experts.47.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.47.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.47.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.47.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.48 as default\n",
            "Injecting model.layers.25.mlp.experts.48.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.48.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.48.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.48.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.49 as default\n",
            "Injecting model.layers.25.mlp.experts.49.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.49.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.49.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.49.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.50 as default\n",
            "Injecting model.layers.25.mlp.experts.50.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.50.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.50.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.50.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.51 as default\n",
            "Injecting model.layers.25.mlp.experts.51.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.51.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.51.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.51.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.52 as default\n",
            "Injecting model.layers.25.mlp.experts.52.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.52.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.52.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.52.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.53 as default\n",
            "Injecting model.layers.25.mlp.experts.53.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.53.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.53.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.53.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.54 as default\n",
            "Injecting model.layers.25.mlp.experts.54.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.54.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.54.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.54.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.55 as default\n",
            "Injecting model.layers.25.mlp.experts.55.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.55.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.55.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.55.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.56 as default\n",
            "Injecting model.layers.25.mlp.experts.56.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.56.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.56.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.56.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.57 as default\n",
            "Injecting model.layers.25.mlp.experts.57.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.57.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.57.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.57.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.58 as default\n",
            "Injecting model.layers.25.mlp.experts.58.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.58.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.58.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.58.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.59 as default\n",
            "Injecting model.layers.25.mlp.experts.59.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.59.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.59.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.59.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.60 as default\n",
            "Injecting model.layers.25.mlp.experts.60.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.60.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.60.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.60.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.61 as default\n",
            "Injecting model.layers.25.mlp.experts.61.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.61.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.61.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.61.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.62 as default\n",
            "Injecting model.layers.25.mlp.experts.62.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.62.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.62.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.62.act_fn as default\n",
            "Injecting model.layers.25.mlp.experts.63 as default\n",
            "Injecting model.layers.25.mlp.experts.63.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.63.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.63.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.experts.63.act_fn as default\n",
            "Injecting model.layers.25.mlp.gate as default\n",
            "Injecting model.layers.25.mlp.shared_experts as default\n",
            "Injecting model.layers.25.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.25.input_layernorm as default\n",
            "Injecting model.layers.25.post_attention_layernorm as default\n",
            "Injecting model.layers.26 as default\n",
            "Injecting model.layers.26.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.26.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.26.self_attn.kv_b_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.rotary_emb as default\n",
            "Injecting model.layers.26.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.26.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.26.mlp.experts.0 as default\n",
            "Injecting model.layers.26.mlp.experts.0.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.0.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.0.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.0.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.1 as default\n",
            "Injecting model.layers.26.mlp.experts.1.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.1.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.1.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.1.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.2 as default\n",
            "Injecting model.layers.26.mlp.experts.2.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.2.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.2.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.2.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.3 as default\n",
            "Injecting model.layers.26.mlp.experts.3.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.3.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.3.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.3.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.4 as default\n",
            "Injecting model.layers.26.mlp.experts.4.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.4.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.4.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.4.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.5 as default\n",
            "Injecting model.layers.26.mlp.experts.5.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.5.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.5.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.5.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.6 as default\n",
            "Injecting model.layers.26.mlp.experts.6.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.6.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.6.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.6.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.7 as default\n",
            "Injecting model.layers.26.mlp.experts.7.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.7.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.7.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.7.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.8 as default\n",
            "Injecting model.layers.26.mlp.experts.8.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.8.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.8.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.8.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.9 as default\n",
            "Injecting model.layers.26.mlp.experts.9.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.9.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.9.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.9.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.10 as default\n",
            "Injecting model.layers.26.mlp.experts.10.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.10.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.10.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.10.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.11 as default\n",
            "Injecting model.layers.26.mlp.experts.11.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.11.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.11.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.11.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.12 as default\n",
            "Injecting model.layers.26.mlp.experts.12.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.12.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.12.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.12.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.13 as default\n",
            "Injecting model.layers.26.mlp.experts.13.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.13.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.13.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.13.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.14 as default\n",
            "Injecting model.layers.26.mlp.experts.14.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.14.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.14.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.14.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.15 as default\n",
            "Injecting model.layers.26.mlp.experts.15.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.15.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.15.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.15.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.16 as default\n",
            "Injecting model.layers.26.mlp.experts.16.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.16.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.16.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.16.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.17 as default\n",
            "Injecting model.layers.26.mlp.experts.17.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.17.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.17.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.17.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.18 as default\n",
            "Injecting model.layers.26.mlp.experts.18.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.18.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.18.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.18.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.19 as default\n",
            "Injecting model.layers.26.mlp.experts.19.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.19.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.19.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.19.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.20 as default\n",
            "Injecting model.layers.26.mlp.experts.20.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.20.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.20.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.20.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.21 as default\n",
            "Injecting model.layers.26.mlp.experts.21.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.21.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.21.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.21.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.22 as default\n",
            "Injecting model.layers.26.mlp.experts.22.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.22.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.22.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.22.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.23 as default\n",
            "Injecting model.layers.26.mlp.experts.23.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.23.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.23.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.23.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.24 as default\n",
            "Injecting model.layers.26.mlp.experts.24.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.24.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.24.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.24.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.25 as default\n",
            "Injecting model.layers.26.mlp.experts.25.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.25.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.25.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.25.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.26 as default\n",
            "Injecting model.layers.26.mlp.experts.26.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.26.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.26.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.26.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.27 as default\n",
            "Injecting model.layers.26.mlp.experts.27.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.27.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.27.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.27.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.28 as default\n",
            "Injecting model.layers.26.mlp.experts.28.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.28.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.28.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.28.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.29 as default\n",
            "Injecting model.layers.26.mlp.experts.29.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.29.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.29.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.29.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.30 as default\n",
            "Injecting model.layers.26.mlp.experts.30.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.30.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.30.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.30.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.31 as default\n",
            "Injecting model.layers.26.mlp.experts.31.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.31.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.31.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.31.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.32 as default\n",
            "Injecting model.layers.26.mlp.experts.32.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.32.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.32.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.32.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.33 as default\n",
            "Injecting model.layers.26.mlp.experts.33.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.33.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.33.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.33.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.34 as default\n",
            "Injecting model.layers.26.mlp.experts.34.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.34.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.34.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.34.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.35 as default\n",
            "Injecting model.layers.26.mlp.experts.35.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.35.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.35.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.35.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.36 as default\n",
            "Injecting model.layers.26.mlp.experts.36.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.36.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.36.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.36.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.37 as default\n",
            "Injecting model.layers.26.mlp.experts.37.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.37.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.37.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.37.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.38 as default\n",
            "Injecting model.layers.26.mlp.experts.38.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.38.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.38.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.38.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.39 as default\n",
            "Injecting model.layers.26.mlp.experts.39.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.39.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.39.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.39.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.40 as default\n",
            "Injecting model.layers.26.mlp.experts.40.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.40.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.40.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.40.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.41 as default\n",
            "Injecting model.layers.26.mlp.experts.41.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.41.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.41.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.41.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.42 as default\n",
            "Injecting model.layers.26.mlp.experts.42.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.42.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.42.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.42.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.43 as default\n",
            "Injecting model.layers.26.mlp.experts.43.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.43.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.43.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.43.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.44 as default\n",
            "Injecting model.layers.26.mlp.experts.44.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.44.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.44.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.44.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.45 as default\n",
            "Injecting model.layers.26.mlp.experts.45.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.45.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.45.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.45.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.46 as default\n",
            "Injecting model.layers.26.mlp.experts.46.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.46.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.46.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.46.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.47 as default\n",
            "Injecting model.layers.26.mlp.experts.47.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.47.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.47.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.47.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.48 as default\n",
            "Injecting model.layers.26.mlp.experts.48.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.48.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.48.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.48.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.49 as default\n",
            "Injecting model.layers.26.mlp.experts.49.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.49.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.49.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.49.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.50 as default\n",
            "Injecting model.layers.26.mlp.experts.50.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.50.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.50.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.50.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.51 as default\n",
            "Injecting model.layers.26.mlp.experts.51.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.51.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.51.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.51.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.52 as default\n",
            "Injecting model.layers.26.mlp.experts.52.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.52.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.52.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.52.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.53 as default\n",
            "Injecting model.layers.26.mlp.experts.53.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.53.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.53.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.53.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.54 as default\n",
            "Injecting model.layers.26.mlp.experts.54.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.54.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.54.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.54.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.55 as default\n",
            "Injecting model.layers.26.mlp.experts.55.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.55.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.55.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.55.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.56 as default\n",
            "Injecting model.layers.26.mlp.experts.56.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.56.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.56.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.56.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.57 as default\n",
            "Injecting model.layers.26.mlp.experts.57.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.57.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.57.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.57.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.58 as default\n",
            "Injecting model.layers.26.mlp.experts.58.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.58.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.58.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.58.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.59 as default\n",
            "Injecting model.layers.26.mlp.experts.59.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.59.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.59.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.59.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.60 as default\n",
            "Injecting model.layers.26.mlp.experts.60.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.60.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.60.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.60.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.61 as default\n",
            "Injecting model.layers.26.mlp.experts.61.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.61.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.61.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.61.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.62 as default\n",
            "Injecting model.layers.26.mlp.experts.62.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.62.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.62.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.62.act_fn as default\n",
            "Injecting model.layers.26.mlp.experts.63 as default\n",
            "Injecting model.layers.26.mlp.experts.63.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.63.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.63.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.experts.63.act_fn as default\n",
            "Injecting model.layers.26.mlp.gate as default\n",
            "Injecting model.layers.26.mlp.shared_experts as default\n",
            "Injecting model.layers.26.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.26.input_layernorm as default\n",
            "Injecting model.layers.26.post_attention_layernorm as default\n",
            "Injecting model.norm as default\n",
            "Injecting lm_head as ktransformers.operators.linear . KTransformersLinear\n",
            "loading model.embed_tokens.weight to cpu\n",
            "loading model.layers.0.self_attn.kv_a_layernorm.weight to cpu\n",
            "loading model.layers.0.input_layernorm.weight to cpu\n",
            "loading model.layers.0.post_attention_layernorm.weight to cpu\n",
            "loading model.layers.1.self_attn.kv_a_layernorm.weight to cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ktransformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo7NA9ABXoZd",
        "outputId": "8581b0ba-ec7c-45b5-fc7a-881c335e03de"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ktransformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/ktransformers/ktransformers/local_chat.py \\\n",
        "  --model_path ZZichen/DeepSeek-V2-Lite \\\n",
        "  --gguf_path /content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF/DeepSeek-V2-Lite-Chat.Q4_K_M.gguf \\\n",
        "  --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat-gpu-cpu.yaml \\\n",
        "  --cpu_infer 2 \\\n",
        "  --device cpu \\\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVZWqGGcW376",
        "outputId": "f6534e00-c047-4b37-e2dc-e182cbb115bb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-07-24 21:43:35,764 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-07-24 21:43:37.569446: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753393417.589124   30585 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753393417.595207   30585 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-24 21:43:37.615267: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "DeepseekV2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "falsh attn not found\n",
            "Injecting model as ktransformers.operators.models . KDeepseekV2Model\n",
            "Injecting model.embed_tokens as default\n",
            "Injecting model.layers as default\n",
            "Injecting model.layers.0 as default\n",
            "Injecting model.layers.0.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.0.self_attn.q_proj as default\n",
            "Injecting model.layers.0.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.0.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.0.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.0.self_attn.o_proj as default\n",
            "Injecting model.layers.0.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.0.mlp as default\n",
            "Injecting model.layers.0.mlp.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.mlp.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.mlp.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.mlp.act_fn as default\n",
            "Injecting model.layers.0.input_layernorm as default\n",
            "Injecting model.layers.0.post_attention_layernorm as default\n",
            "Injecting model.layers.1 as default\n",
            "Injecting model.layers.1.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.1.self_attn.q_proj as default\n",
            "Injecting model.layers.1.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.1.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.1.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.1.self_attn.o_proj as default\n",
            "Injecting model.layers.1.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.1.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.1.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.1.mlp.gate as default\n",
            "Injecting model.layers.1.mlp.shared_experts as default\n",
            "Injecting model.layers.1.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.1.input_layernorm as default\n",
            "Injecting model.layers.1.post_attention_layernorm as default\n",
            "Injecting model.layers.2 as default\n",
            "Injecting model.layers.2.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.2.self_attn.q_proj as default\n",
            "Injecting model.layers.2.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.2.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.2.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.2.self_attn.o_proj as default\n",
            "Injecting model.layers.2.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.2.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.2.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.2.mlp.gate as default\n",
            "Injecting model.layers.2.mlp.shared_experts as default\n",
            "Injecting model.layers.2.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.2.input_layernorm as default\n",
            "Injecting model.layers.2.post_attention_layernorm as default\n",
            "Injecting model.layers.3 as default\n",
            "Injecting model.layers.3.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.3.self_attn.q_proj as default\n",
            "Injecting model.layers.3.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.3.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.3.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.3.self_attn.o_proj as default\n",
            "Injecting model.layers.3.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.3.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.3.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.3.mlp.gate as default\n",
            "Injecting model.layers.3.mlp.shared_experts as default\n",
            "Injecting model.layers.3.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.3.input_layernorm as default\n",
            "Injecting model.layers.3.post_attention_layernorm as default\n",
            "Injecting model.layers.4 as default\n",
            "Injecting model.layers.4.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.4.self_attn.q_proj as default\n",
            "Injecting model.layers.4.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.4.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.4.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.4.self_attn.o_proj as default\n",
            "Injecting model.layers.4.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.4.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.4.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.4.mlp.gate as default\n",
            "Injecting model.layers.4.mlp.shared_experts as default\n",
            "Injecting model.layers.4.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.4.input_layernorm as default\n",
            "Injecting model.layers.4.post_attention_layernorm as default\n",
            "Injecting model.layers.5 as default\n",
            "Injecting model.layers.5.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.5.self_attn.q_proj as default\n",
            "Injecting model.layers.5.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.5.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.5.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.5.self_attn.o_proj as default\n",
            "Injecting model.layers.5.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.5.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.5.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.5.mlp.gate as default\n",
            "Injecting model.layers.5.mlp.shared_experts as default\n",
            "Injecting model.layers.5.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.5.input_layernorm as default\n",
            "Injecting model.layers.5.post_attention_layernorm as default\n",
            "Injecting model.layers.6 as default\n",
            "Injecting model.layers.6.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.6.self_attn.q_proj as default\n",
            "Injecting model.layers.6.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.6.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.6.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.6.self_attn.o_proj as default\n",
            "Injecting model.layers.6.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.6.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.6.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.6.mlp.gate as default\n",
            "Injecting model.layers.6.mlp.shared_experts as default\n",
            "Injecting model.layers.6.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.6.input_layernorm as default\n",
            "Injecting model.layers.6.post_attention_layernorm as default\n",
            "Injecting model.layers.7 as default\n",
            "Injecting model.layers.7.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.7.self_attn.q_proj as default\n",
            "Injecting model.layers.7.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.7.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.7.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.7.self_attn.o_proj as default\n",
            "Injecting model.layers.7.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.7.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.7.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.7.mlp.gate as default\n",
            "Injecting model.layers.7.mlp.shared_experts as default\n",
            "Injecting model.layers.7.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.7.input_layernorm as default\n",
            "Injecting model.layers.7.post_attention_layernorm as default\n",
            "Injecting model.layers.8 as default\n",
            "Injecting model.layers.8.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.8.self_attn.q_proj as default\n",
            "Injecting model.layers.8.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.8.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.8.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.8.self_attn.o_proj as default\n",
            "Injecting model.layers.8.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.8.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.8.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.8.mlp.gate as default\n",
            "Injecting model.layers.8.mlp.shared_experts as default\n",
            "Injecting model.layers.8.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.8.input_layernorm as default\n",
            "Injecting model.layers.8.post_attention_layernorm as default\n",
            "Injecting model.layers.9 as default\n",
            "Injecting model.layers.9.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.9.self_attn.q_proj as default\n",
            "Injecting model.layers.9.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.9.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.9.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.9.self_attn.o_proj as default\n",
            "Injecting model.layers.9.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.9.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.9.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.9.mlp.gate as default\n",
            "Injecting model.layers.9.mlp.shared_experts as default\n",
            "Injecting model.layers.9.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.9.input_layernorm as default\n",
            "Injecting model.layers.9.post_attention_layernorm as default\n",
            "Injecting model.layers.10 as default\n",
            "Injecting model.layers.10.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.10.self_attn.q_proj as default\n",
            "Injecting model.layers.10.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.10.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.10.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.10.self_attn.o_proj as default\n",
            "Injecting model.layers.10.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.10.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.10.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.10.mlp.gate as default\n",
            "Injecting model.layers.10.mlp.shared_experts as default\n",
            "Injecting model.layers.10.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.10.input_layernorm as default\n",
            "Injecting model.layers.10.post_attention_layernorm as default\n",
            "Injecting model.layers.11 as default\n",
            "Injecting model.layers.11.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.11.self_attn.q_proj as default\n",
            "Injecting model.layers.11.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.11.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.11.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.11.self_attn.o_proj as default\n",
            "Injecting model.layers.11.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.11.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.11.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.11.mlp.gate as default\n",
            "Injecting model.layers.11.mlp.shared_experts as default\n",
            "Injecting model.layers.11.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.11.input_layernorm as default\n",
            "Injecting model.layers.11.post_attention_layernorm as default\n",
            "Injecting model.layers.12 as default\n",
            "Injecting model.layers.12.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.12.self_attn.q_proj as default\n",
            "Injecting model.layers.12.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.12.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.12.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.12.self_attn.o_proj as default\n",
            "Injecting model.layers.12.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.12.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.12.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.12.mlp.gate as default\n",
            "Injecting model.layers.12.mlp.shared_experts as default\n",
            "Injecting model.layers.12.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.12.input_layernorm as default\n",
            "Injecting model.layers.12.post_attention_layernorm as default\n",
            "Injecting model.layers.13 as default\n",
            "Injecting model.layers.13.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.13.self_attn.q_proj as default\n",
            "Injecting model.layers.13.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.13.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.13.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.13.self_attn.o_proj as default\n",
            "Injecting model.layers.13.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.13.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.13.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.13.mlp.gate as default\n",
            "Injecting model.layers.13.mlp.shared_experts as default\n",
            "Injecting model.layers.13.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.13.input_layernorm as default\n",
            "Injecting model.layers.13.post_attention_layernorm as default\n",
            "Injecting model.layers.14 as default\n",
            "Injecting model.layers.14.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.14.self_attn.q_proj as default\n",
            "Injecting model.layers.14.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.14.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.14.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.14.self_attn.o_proj as default\n",
            "Injecting model.layers.14.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.14.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.14.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.14.mlp.gate as default\n",
            "Injecting model.layers.14.mlp.shared_experts as default\n",
            "Injecting model.layers.14.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.14.input_layernorm as default\n",
            "Injecting model.layers.14.post_attention_layernorm as default\n",
            "Injecting model.layers.15 as default\n",
            "Injecting model.layers.15.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.15.self_attn.q_proj as default\n",
            "Injecting model.layers.15.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.15.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.15.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.15.self_attn.o_proj as default\n",
            "Injecting model.layers.15.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.15.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.15.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.15.mlp.gate as default\n",
            "Injecting model.layers.15.mlp.shared_experts as default\n",
            "Injecting model.layers.15.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.15.input_layernorm as default\n",
            "Injecting model.layers.15.post_attention_layernorm as default\n",
            "Injecting model.layers.16 as default\n",
            "Injecting model.layers.16.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.16.self_attn.q_proj as default\n",
            "Injecting model.layers.16.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.16.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.16.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.16.self_attn.o_proj as default\n",
            "Injecting model.layers.16.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.16.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.16.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.16.mlp.gate as default\n",
            "Injecting model.layers.16.mlp.shared_experts as default\n",
            "Injecting model.layers.16.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.16.input_layernorm as default\n",
            "Injecting model.layers.16.post_attention_layernorm as default\n",
            "Injecting model.layers.17 as default\n",
            "Injecting model.layers.17.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.17.self_attn.q_proj as default\n",
            "Injecting model.layers.17.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.17.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.17.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.17.self_attn.o_proj as default\n",
            "Injecting model.layers.17.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.17.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.17.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.17.mlp.gate as default\n",
            "Injecting model.layers.17.mlp.shared_experts as default\n",
            "Injecting model.layers.17.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.17.input_layernorm as default\n",
            "Injecting model.layers.17.post_attention_layernorm as default\n",
            "Injecting model.layers.18 as default\n",
            "Injecting model.layers.18.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.18.self_attn.q_proj as default\n",
            "Injecting model.layers.18.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.18.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.18.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.18.self_attn.o_proj as default\n",
            "Injecting model.layers.18.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.18.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.18.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.18.mlp.gate as default\n",
            "Injecting model.layers.18.mlp.shared_experts as default\n",
            "Injecting model.layers.18.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.18.input_layernorm as default\n",
            "Injecting model.layers.18.post_attention_layernorm as default\n",
            "Injecting model.layers.19 as default\n",
            "Injecting model.layers.19.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.19.self_attn.q_proj as default\n",
            "Injecting model.layers.19.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.19.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.19.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.19.self_attn.o_proj as default\n",
            "Injecting model.layers.19.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.19.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.19.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.19.mlp.gate as default\n",
            "Injecting model.layers.19.mlp.shared_experts as default\n",
            "Injecting model.layers.19.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.19.input_layernorm as default\n",
            "Injecting model.layers.19.post_attention_layernorm as default\n",
            "Injecting model.layers.20 as default\n",
            "Injecting model.layers.20.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.20.self_attn.q_proj as default\n",
            "Injecting model.layers.20.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.20.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.20.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.20.self_attn.o_proj as default\n",
            "Injecting model.layers.20.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.20.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.20.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.20.mlp.gate as default\n",
            "Injecting model.layers.20.mlp.shared_experts as default\n",
            "Injecting model.layers.20.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.20.input_layernorm as default\n",
            "Injecting model.layers.20.post_attention_layernorm as default\n",
            "Injecting model.layers.21 as default\n",
            "Injecting model.layers.21.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.21.self_attn.q_proj as default\n",
            "Injecting model.layers.21.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.21.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.21.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.21.self_attn.o_proj as default\n",
            "Injecting model.layers.21.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.21.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.21.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.21.mlp.gate as default\n",
            "Injecting model.layers.21.mlp.shared_experts as default\n",
            "Injecting model.layers.21.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.21.input_layernorm as default\n",
            "Injecting model.layers.21.post_attention_layernorm as default\n",
            "Injecting model.layers.22 as default\n",
            "Injecting model.layers.22.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.22.self_attn.q_proj as default\n",
            "Injecting model.layers.22.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.22.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.22.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.22.self_attn.o_proj as default\n",
            "Injecting model.layers.22.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.22.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.22.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.22.mlp.gate as default\n",
            "Injecting model.layers.22.mlp.shared_experts as default\n",
            "Injecting model.layers.22.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.22.input_layernorm as default\n",
            "Injecting model.layers.22.post_attention_layernorm as default\n",
            "Injecting model.layers.23 as default\n",
            "Injecting model.layers.23.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.23.self_attn.q_proj as default\n",
            "Injecting model.layers.23.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.23.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.23.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.23.self_attn.o_proj as default\n",
            "Injecting model.layers.23.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.23.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.23.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.23.mlp.gate as default\n",
            "Injecting model.layers.23.mlp.shared_experts as default\n",
            "Injecting model.layers.23.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.23.input_layernorm as default\n",
            "Injecting model.layers.23.post_attention_layernorm as default\n",
            "Injecting model.layers.24 as default\n",
            "Injecting model.layers.24.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.24.self_attn.q_proj as default\n",
            "Injecting model.layers.24.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.24.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.24.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.24.self_attn.o_proj as default\n",
            "Injecting model.layers.24.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.24.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.24.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.24.mlp.gate as default\n",
            "Injecting model.layers.24.mlp.shared_experts as default\n",
            "Injecting model.layers.24.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.24.input_layernorm as default\n",
            "Injecting model.layers.24.post_attention_layernorm as default\n",
            "Injecting model.layers.25 as default\n",
            "Injecting model.layers.25.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.25.self_attn.q_proj as default\n",
            "Injecting model.layers.25.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.25.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.25.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.25.self_attn.o_proj as default\n",
            "Injecting model.layers.25.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.25.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.25.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.25.mlp.gate as default\n",
            "Injecting model.layers.25.mlp.shared_experts as default\n",
            "Injecting model.layers.25.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.25.input_layernorm as default\n",
            "Injecting model.layers.25.post_attention_layernorm as default\n",
            "Injecting model.layers.26 as default\n",
            "Injecting model.layers.26.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
            "Injecting model.layers.26.self_attn.q_proj as default\n",
            "Injecting model.layers.26.self_attn.kv_a_proj_with_mqa as default\n",
            "Injecting model.layers.26.self_attn.kv_a_layernorm as default\n",
            "Injecting model.layers.26.self_attn.kv_b_proj as default\n",
            "Injecting model.layers.26.self_attn.o_proj as default\n",
            "Injecting model.layers.26.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
            "Injecting model.layers.26.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
            "Injecting model.layers.26.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.26.mlp.gate as default\n",
            "Injecting model.layers.26.mlp.shared_experts as default\n",
            "Injecting model.layers.26.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.mlp.shared_experts.act_fn as default\n",
            "Injecting model.layers.26.input_layernorm as default\n",
            "Injecting model.layers.26.post_attention_layernorm as default\n",
            "Injecting model.norm as default\n",
            "Injecting lm_head as ktransformers.operators.linear . KTransformersLinear\n",
            "loading lm_head to cpu using CPUInfer\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ktransformers/ktransformers/local_chat.py\", line 197, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/ktransformers/local_chat.py\", line 124, in local_chat\n",
            "    optimize_and_load_gguf(model, optimize_config_path, gguf_path, config, default_device=device)\n",
            "  File \"/content/ktransformers/ktransformers/optimize/optimize.py\", line 130, in optimize_and_load_gguf\n",
            "    load_weights(module.lm_head, weights_loader, \"lm_head.\", device=default_device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 176, in load_weights\n",
            "    module.load()\n",
            "  File \"/content/ktransformers/ktransformers/operators/linear.py\", line 937, in load\n",
            "    self.generate_linear.load(w=w)\n",
            "  File \"/content/ktransformers/ktransformers/operators/linear.py\", line 772, in load\n",
            "    self.load_weights(w=w, device=device)\n",
            "  File \"/content/ktransformers/ktransformers/operators/linear.py\", line 798, in load_weights\n",
            "    self.weight_type = self.gguf_loader.tensor_info[self.key + \".weight\"][\"ggml_type\"]\n",
            "                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyError: 'lm_head.weight'\n"
          ]
        }
      ]
    }
  ]
}