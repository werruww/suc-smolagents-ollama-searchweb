{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqDVYmwKqLjl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cbsoPp-OqWtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers/main/en/serving"
      ],
      "metadata": {
        "id": "Z5N0bCgdzWfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'transformers[torch]'\n",
        "#uv pip install 'transformers[torch]'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3UTSsAUqWw2",
        "outputId": "2b2aa437-8c48-4a33-b7f8-c828f9ae8aa5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.11/dist-packages (4.55.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (4.67.1)\n",
            "Requirement already satisfied: torch>=2.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.6.0+cu124)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (1.9.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (1.1.7)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.1->transformers[torch])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.1->transformers[torch])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.1->transformers[torch])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1->transformers[torch])\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1->transformers[torch])\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1->transformers[torch])\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1->transformers[torch])\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1->transformers[torch])\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1->transformers[torch])\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=2.1->transformers[torch])\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1->transformers[torch])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (2025.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1->transformers[torch]) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('hugging face is the best'))\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSI4LJo_qaC4",
        "outputId": "3a5ffe4f-1928-4771-cf39-2a245fdb8203"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-10 20:28:40.190603: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754857720.451572     851 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754857720.526471     851 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754857721.063064     851 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754857721.063114     851 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754857721.063119     851 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754857721.063123     851 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-10 20:28:41.111329: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "config.json: 100% 629/629 [00:00<00:00, 2.85MB/s]\n",
            "model.safetensors: 100% 268M/268M [00:03<00:00, 78.3MB/s]\n",
            "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 299kB/s]\n",
            "vocab.txt: 232kB [00:00, 28.4MB/s]\n",
            "Device set to use cpu\n",
            "[{'label': 'POSITIVE', 'score': 0.999839186668396}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[{'label': 'POSITIVE', 'score': 0.9998704791069031}]"
      ],
      "metadata": {
        "id": "5Z3C8lqBqw2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "metadata": {
        "id": "saTA9viMrIln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!transformers serve"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6NrRhKsyjTA",
        "outputId": "c961bc21-7c09-48d4-d8b7-f56b73647a9a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-10 21:02:02.904050: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754859722.932557    8885 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754859722.940735    8885 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754859722.980738    8885 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754859722.980813    8885 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754859722.980818    8885 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754859722.980822    8885 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-10 21:02:02.989026: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m8885\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://localhost:8000\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     Shutting down\n",
            "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m8885\u001b[0m]\n",
            "\u001b[31mERROR\u001b[0m:    Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/asyncio/runners.py\", line 190, in run\n",
            "    return runner.run(main)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/asyncio/runners.py\", line 118, in run\n",
            "    return self._loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 641, in run_until_complete\n",
            "    self.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 70, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 331, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "  File \"/usr/lib/python3.11/asyncio/runners.py\", line 157, in _on_sigint\n",
            "    raise KeyboardInterrupt()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 701, in lifespan\n",
            "    await receive()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/lifespan/on.py\", line 137, in receive\n",
            "    return await self.receive_queue.get()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/asyncio/queues.py\", line 158, in get\n",
            "    await getter\n",
            "asyncio.exceptions.CancelledError\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!transformers serve -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZp1rdEUyj2S",
        "outputId": "cd273992-89d4-48df-c08e-8536882d6455"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-10 21:02:42.612945: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754859762.658698    9055 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754859762.672760    9055 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754859762.703339    9055 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754859762.703390    9055 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754859762.703400    9055 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754859762.703408    9055 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-10 21:02:42.711667: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "usage: transformers <command> [<args>] serve [-h] [--device DEVICE]\n",
            "                                             [--torch_dtype {auto,bfloat16,float16,float32}]\n",
            "                                             [--trust_remote_code [TRUST_REMOTE_CODE]]\n",
            "                                             [--attn_implementation ATTN_IMPLEMENTATION]\n",
            "                                             [--load_in_8bit [LOAD_IN_8BIT]]\n",
            "                                             [--load_in_4bit [LOAD_IN_4BIT]]\n",
            "                                             [--bnb_4bit_quant_type {fp4,nf4}]\n",
            "                                             [--use_bnb_nested_quant [USE_BNB_NESTED_QUANT]]\n",
            "                                             [--host HOST] [--port PORT]\n",
            "                                             [--model_timeout MODEL_TIMEOUT]\n",
            "                                             [--log_level LOG_LEVEL]\n",
            "                                             [--default_seed DEFAULT_SEED]\n",
            "                                             [--enable_cors [ENABLE_CORS]]\n",
            "                                             [--input_validation [INPUT_VALIDATION]]\n",
            "                                             [--force_model FORCE_MODEL]\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --device DEVICE       Device to use for inference; will default to `auto`\n",
            "                        andplace the model on an accelerator if available.\n",
            "                        (default: auto)\n",
            "  --torch_dtype {auto,bfloat16,float16,float32}, --torch-dtype {auto,bfloat16,float16,float32}\n",
            "                        Override the default `torch.dtype` and load the model\n",
            "                        under this dtype. If `'auto'` is passed, the dtype\n",
            "                        will be automatically derived from the model's\n",
            "                        weights. (default: auto)\n",
            "  --trust_remote_code [TRUST_REMOTE_CODE], --trust-remote-code [TRUST_REMOTE_CODE]\n",
            "                        Whether to trust remote code when loading a model.\n",
            "                        (default: False)\n",
            "  --attn_implementation ATTN_IMPLEMENTATION, --attn-implementation ATTN_IMPLEMENTATION\n",
            "                        Which attention implementation to use; you can run\n",
            "                        --attn_implementation=flash_attention_2, in which case\n",
            "                        you must install this manually by running `pip install\n",
            "                        flash-attn --no-build-isolation`. (default: None)\n",
            "  --load_in_8bit [LOAD_IN_8BIT], --load-in-8bit [LOAD_IN_8BIT]\n",
            "                        Whether to use 8 bit precision for the base model -\n",
            "                        works only with LoRA. (default: False)\n",
            "  --load_in_4bit [LOAD_IN_4BIT], --load-in-4bit [LOAD_IN_4BIT]\n",
            "                        Whether to use 4 bit precision for the base model -\n",
            "                        works only with LoRA. (default: False)\n",
            "  --bnb_4bit_quant_type {fp4,nf4}, --bnb-4bit-quant-type {fp4,nf4}\n",
            "                        Quantization type. (default: nf4)\n",
            "  --use_bnb_nested_quant [USE_BNB_NESTED_QUANT], --use-bnb-nested-quant [USE_BNB_NESTED_QUANT]\n",
            "                        Whether to use nested quantization. (default: False)\n",
            "  --host HOST           Interface the server will listen to. (default:\n",
            "                        localhost)\n",
            "  --port PORT           Port the server will listen to. (default: 8000)\n",
            "  --model_timeout MODEL_TIMEOUT, --model-timeout MODEL_TIMEOUT\n",
            "                        Time in seconds after which a model will be removed\n",
            "                        from memory. (default: 300)\n",
            "  --log_level LOG_LEVEL, --log-level LOG_LEVEL\n",
            "                        Logging level as a string. Example: 'info' or\n",
            "                        'warning'. (default: info)\n",
            "  --default_seed DEFAULT_SEED, --default-seed DEFAULT_SEED\n",
            "                        The default seed for torch, should be an integer.\n",
            "                        (default: None)\n",
            "  --enable_cors [ENABLE_CORS], --enable-cors [ENABLE_CORS]\n",
            "                        Whether to enable CORS. Some apps that make requests\n",
            "                        from external domains (e.g. Cursor) require CORS to be\n",
            "                        enabled. (default: False)\n",
            "  --input_validation [INPUT_VALIDATION], --input-validation [INPUT_VALIDATION]\n",
            "                        Whether to turn on strict input validation. (default:\n",
            "                        False)\n",
            "  --force_model FORCE_MODEL, --force-model FORCE_MODEL\n",
            "                        Name of the model to be forced on all requests. This\n",
            "                        is useful for testing Apps that don't allow changing\n",
            "                        models in the request. (default: None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!transformers serve --force_model facebook/opt-125m"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDwA3n80yto_",
        "outputId": "77836893-1dc8-4b02-977f-e256a8cf9a4f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-10 21:04:17.938376: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754859857.963019    9443 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754859857.970483    9443 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754859857.989465    9443 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754859857.989516    9443 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754859857.989520    9443 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754859857.989524    9443 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-10 21:04:17.995058: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m9443\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://localhost:8000\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     Shutting down\n",
            "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m9443\u001b[0m]\n",
            "\u001b[31mERROR\u001b[0m:    Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/asyncio/runners.py\", line 190, in run\n",
            "    return runner.run(main)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/asyncio/runners.py\", line 118, in run\n",
            "    return self._loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 641, in run_until_complete\n",
            "    self.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 70, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 331, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "  File \"/usr/lib/python3.11/asyncio/runners.py\", line 157, in _on_sigint\n",
            "    raise KeyboardInterrupt()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 701, in lifespan\n",
            "    await receive()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/lifespan/on.py\", line 137, in receive\n",
            "    return await self.receive_queue.get()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/asyncio/queues.py\", line 158, in get\n",
            "    await getter\n",
            "asyncio.exceptions.CancelledError\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PrTTt7O1zEzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6350a5b",
        "outputId": "d8719250-8f58-4014-ad65-e6f7d1267fca"
      },
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# The URL of the local server\n",
        "url = \"http://localhost:8000/generate\" # Or other endpoint depending on the task\n",
        "\n",
        "# The data to send in the request (e.g., a prompt for text generation)\n",
        "# This will vary depending on the specific model and task\n",
        "data = {\n",
        "    \"inputs\": \"Once upon a time,\",\n",
        "    \"parameters\": {\n",
        "        \"max_new_tokens\": 50\n",
        "    }\n",
        "}\n",
        "\n",
        "try:\n",
        "    # Send a POST request to the server\n",
        "    response = requests.post(url, json=data)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Print the response from the server\n",
        "        print(response.json())\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "        print(response.text)\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error sending request: {e}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 404\n",
            "{\"detail\":\"Not Found\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[serving]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne6hkmb70MLA",
        "outputId": "5933fe24-ebbd-4d85-a2d0-eb124c974146"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[serving] in /usr/local/lib/python3.11/dist-packages (4.55.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers[serving]) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers[serving]) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers[serving]) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers[serving]) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers[serving]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[serving]) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers[serving]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[serving]) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[serving]) (0.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers[serving]) (4.67.1)\n",
            "Requirement already satisfied: openai>=1.98.0 in /usr/local/lib/python3.11/dist-packages (from transformers[serving]) (1.99.1)\n",
            "Requirement already satisfied: pydantic>=2 in /usr/local/lib/python3.11/dist-packages (from transformers[serving]) (2.11.7)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (from transformers[serving]) (0.35.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from transformers[serving]) (0.116.1)\n",
            "Requirement already satisfied: starlette in /usr/local/lib/python3.11/dist-packages (from transformers[serving]) (0.47.2)\n",
            "Requirement already satisfied: torch>=2.1 in /usr/local/lib/python3.11/dist-packages (from transformers[serving]) (2.6.0+cu124)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[serving]) (1.9.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[serving]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[serving]) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[serving]) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[serving]) (1.1.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.98.0->transformers[serving]) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.98.0->transformers[serving]) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.98.0->transformers[serving]) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.98.0->transformers[serving]) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.98.0->transformers[serving]) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->transformers[serving]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->transformers[serving]) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->transformers[serving]) (0.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[serving]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[serving]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[serving]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[serving]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[serving]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[serving]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[serving]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[serving]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[serving]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[serving]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[serving]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[serving]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[serving]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[serving]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[serving]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[serving]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[serving]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1->transformers[serving]) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[serving]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[serving]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[serving]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[serving]) (2025.8.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->transformers[serving]) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->transformers[serving]) (0.16.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.98.0->transformers[serving]) (1.0.9)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1->transformers[serving]) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a34Qcqp30Uqg",
        "outputId": "634be2b1-2f1a-4b0d-c981-3665a708e707"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-efxwsd9f\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-efxwsd9f\n",
            "  Resolved https://github.com/huggingface/transformers to commit 99c747539e07b2e141cb5f13961b72108e9dc864\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0.dev0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0.dev0) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0.dev0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0.dev0) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0.dev0) (0.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.0.dev0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.0.dev0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.0.dev0) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.56.0.dev0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.56.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.56.0.dev0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.56.0.dev0) (2025.8.3)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.56.0.dev0-py3-none-any.whl size=12261147 sha256=364ca6ae301d7b29cedd3b6f044aaa0a8f3ffd3f506d81f7fb01b0c6be941b0a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jr0_43oz/wheels/04/a3/f1/b88775f8e1665827525b19ac7590250f1038d947067beba9fb\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.55.0\n",
            "    Uninstalling transformers-4.55.0:\n",
            "      Successfully uninstalled transformers-4.55.0\n",
            "Successfully installed transformers-4.56.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!transformers-cli convert --model facebook/opt-125m --framework pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9vbTGr50sbw",
        "outputId": "426b6dfd-697b-4a4c-c29e-dd966347bc58"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-10 21:11:44.746049: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754860304.772613   11293 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754860304.780526   11293 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754860304.800234   11293 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754860304.800278   11293 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754860304.800283   11293 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754860304.800287   11293 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-10 21:11:44.805880: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "usage: transformers <command> [<args>] convert [-h] --model_type MODEL_TYPE\n",
            "                                               --tf_checkpoint TF_CHECKPOINT\n",
            "                                               --pytorch_dump_output\n",
            "                                               PYTORCH_DUMP_OUTPUT\n",
            "                                               [--config CONFIG]\n",
            "                                               [--finetuning_task_name FINETUNING_TASK_NAME]\n",
            "transformers <command> [<args>] convert: error: the following arguments are required: --tf_checkpoint, --pytorch_dump_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!transformers chat Qwen/Qwen3-0.6B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pqbfWW_01Ew",
        "outputId": "a2df006e-60f9-4166-a16c-016e46f5cae3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-10 21:14:36.222966: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754860476.247101   12054 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754860476.254426   12054 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754860476.272644   12054 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754860476.272703   12054 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754860476.272708   12054 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754860476.272712   12054 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-10 21:14:36.278110: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[31mERROR\u001b[0m:    [Errno 98] error while attempting to bind on address ('127.0.0.1', 8000): address already in use\n",
            "\u001b[2J\u001b[H\u001b[1mTRANSFORMERS CHAT INTERFACE\u001b[0m                                                     \n",
            "\n",
            "Chat interface to try out a model. Besides chatting with the model, here are    \n",
            "some basic commands:                                                            \n",
            "\n",
            "\u001b[1;33m • \u001b[0m\u001b[1m!help\u001b[0m: shows all available commands (set generation settings, save chat,     \n",
            "\u001b[1;33m   \u001b[0metc.)                                                                        \n",
            "\u001b[1;33m • \u001b[0m\u001b[1m!status\u001b[0m: shows the current status of the model and generation settings       \n",
            "\u001b[1;33m • \u001b[0m\u001b[1m!clear\u001b[0m: clears the current conversation and starts a new one                 \n",
            "\u001b[1;33m • \u001b[0m\u001b[1m!exit\u001b[0m: closes the interface                                                  \n",
            "\n",
            "\u001b[1;31m<\u001b[0m\u001b[1;31mroot\u001b[0m\u001b[1;31m>\u001b[0m\u001b[1;31m:\u001b[0m\n",
            "hi\n",
            "\n",
            "\u001b[1;34m<\u001b[0m\u001b[1;34mQwen\u001b[0m\u001b[1;34m/Qwen3-\u001b[0m\u001b[1;34m0.\u001b[0m\u001b[1;34m6B\u001b[0m\u001b[1;34m>\u001b[0m\u001b[1;34m:\u001b[0m\n",
            "\u001b[?25l\n",
            "\u001b[?25hTraceback (most recent call last):\n",
            "  File \"/usr/local/bin/transformers\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/commands/transformers_cli.py\", line 59, in main\n",
            "    service.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/commands/chat.py\", line 643, in run\n",
            "    asyncio.run(self._inner_run())\n",
            "  File \"/usr/lib/python3.11/asyncio/runners.py\", line 190, in run\n",
            "    return runner.run(main)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/asyncio/runners.py\", line 118, in run\n",
            "    return self._loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\n",
            "    return future.result()\n",
            "           ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/commands/chat.py\", line 728, in _inner_run\n",
            "    model_output, request_id = await interface.stream_output(stream)\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/commands/chat.py\", line 129, in stream_output\n",
            "    async for token in await stream:\n",
            "                       ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\", line 962, in chat_completion\n",
            "    data = await self._inner_post(request_parameters, stream=stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\", line 288, in _inner_post\n",
            "    raise error\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\", line 274, in _inner_post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/aiohttp/client_reqrep.py\", line 629, in raise_for_status\n",
            "    raise ClientResponseError(\n",
            "aiohttp.client_exceptions.ClientResponseError: 500, message='Internal Server Error', url='http://localhost:8000/v1/chat/completions'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52d4e2c4"
      },
      "source": [
        "import subprocess\n",
        "import shlex\n",
        "\n",
        "command = \"transformers serve --force_model Qwen/Qwen3-0.6B\"\n",
        "process = subprocess.Popen(shlex.split(command))\n",
        "\n",
        "# You can add code here to interact with the server\n",
        "# For example, using the 'requests' library after the server has started\n",
        "\n",
        "# To stop the server, you can use process.terminate() or process.kill()\n",
        "# import time\n",
        "# time.sleep(10) # Example: let the server run for 10 seconds\n",
        "# process.terminate()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -X POST http://localhost:8000/v1/chat/completions -H \"Content-Type: application/json\" -d '{\"messages\": [{\"role\": \"system\", \"content\": \"hello\"}], \"temperature\": 0.9, \"max_tokens\": 10, \"stream\": true, \"model\": \"Qwen/Qwen3-0.6B\"}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6kNLdQD2CJ5",
        "outputId": "bf4b8b75-11ff-45e8-ab78-cd249c3ac437"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Internal Server Error"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!transformers chat localhost:8000 --model-name-or-path Qwen/Qwen3-0.6B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwCwL25D3I-R",
        "outputId": "24c000c5-19f6-43b9-f5fb-9411c0681e8a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_prims/__init__.py\", line 524, in <module>\n",
            "    abs = _make_elementwise_unary_prim(\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_prims/__init__.py\", line 492, in _make_elementwise_unary_prim\n",
            "    return _make_prim(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_prims/__init__.py\", line 320, in _make_prim\n",
            "    prim_def = torch.library.custom_op(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_library/custom_ops.py\", line 145, in custom_op\n",
            "    return inner(fn)\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_library/custom_ops.py\", line 126, in inner\n",
            "    result = CustomOpDef(namespace, opname, schema_str, fn)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_library/custom_ops.py\", line 174, in __init__\n",
            "    self._register_to_dispatcher()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_library/custom_ops.py\", line 594, in _register_to_dispatcher\n",
            "    lib._register_fake(self._name, fake_impl, _stacklevel=4)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/library.py\", line 177, in _register_fake\n",
            "    source = torch._library.utils.get_source(_stacklevel + 1)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_library/utils.py\", line 53, in get_source\n",
            "    frame = inspect.getframeinfo(sys._getframe(stacklevel))\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1688, in getframeinfo\n",
            "    lines, lnum = findsource(frame)\n",
            "                  ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1071, in findsource\n",
            "    module = getmodule(object, file)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 997, in getmodule\n",
            "    os.path.realpath(f)] = module.__name__\n",
            "    ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen posixpath>\", line 416, in realpath\n",
            "  File \"<frozen posixpath>\", line 445, in _joinrealpath\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/transformers\", line 4, in <module>\n",
            "    from transformers.commands.transformers_cli import main\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/__init__.py\", line 27, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/__init__.py\", line 24, in <module>\n",
            "    from .auto_docstring import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/auto_docstring.py\", line 30, in <module>\n",
            "    from .generic import ModelOutput\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\", line 53, in <module>\n",
            "    import torch  # noqa: F401\n",
            "    ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 2604, in <module>\n",
            "    from torch import _meta_registrations\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_meta_registrations.py\", line 11, in <module>\n",
            "    from torch._decomp import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_decomp/__init__.py\", line 284, in <module>\n",
            "    import torch._decomp.decompositions\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_decomp/decompositions.py\", line 15, in <module>\n",
            "    import torch._prims as prims\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1150, in _find_and_load_unlocked\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -X POST http://localhost:8000/v1/chat/completions -H \"Content-Type: application/json\" -d '{\"messages\": [{\"role\": \"system\", \"content\": \"hello\"}], \"temperature\": 0.9, \"max_tokens\": 10, \"stream\": true, \"model\": \"Qwen/Qwen3-0.6B\"}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLy_hfXi3Vgw",
        "outputId": "6c6f2161-381d-40b2-81d4-063ff62dd971"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "curl: (7) Failed to connect to localhost port 8000 after 0 ms: Connection refused\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!transformers chat --model-name-or-path Qwen/Qwen3-0.6B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyJRbIM831qJ",
        "outputId": "815df452-a1d8-4827-e6ba-0aec07383ad9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-10 21:25:58.321510: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754861158.346405   14859 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754861158.354360   14859 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754861158.378921   14859 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754861158.378982   14859 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754861158.378993   14859 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754861158.379001   14859 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-10 21:25:58.387377: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "usage: transformers <command> [<args>] chat [-h]\n",
            "                                            [--model_name_or_path MODEL_NAME_OR_PATH]\n",
            "                                            [--user USER]\n",
            "                                            [--system_prompt SYSTEM_PROMPT]\n",
            "                                            [--save_folder SAVE_FOLDER]\n",
            "                                            [--examples_path EXAMPLES_PATH]\n",
            "                                            [--verbose [VERBOSE]]\n",
            "                                            [--generation_config GENERATION_CONFIG]\n",
            "                                            [--model_revision MODEL_REVISION]\n",
            "                                            [--device DEVICE]\n",
            "                                            [--torch_dtype {auto,bfloat16,float16,float32}]\n",
            "                                            [--trust_remote_code [TRUST_REMOTE_CODE]]\n",
            "                                            [--attn_implementation ATTN_IMPLEMENTATION]\n",
            "                                            [--load_in_8bit [LOAD_IN_8BIT]]\n",
            "                                            [--load_in_4bit [LOAD_IN_4BIT]]\n",
            "                                            [--bnb_4bit_quant_type {fp4,nf4}]\n",
            "                                            [--use_bnb_nested_quant [USE_BNB_NESTED_QUANT]]\n",
            "                                            [--host HOST] [--port PORT]\n",
            "                                            model_name_or_path_or_address\n",
            "                                            [generate_flags ...]\n",
            "transformers <command> [<args>] chat: error: the following arguments are required: model_name_or_path_or_address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O8CHiSOi4vbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wjfK5lET4vZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IGN4XaUX4vW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vAe6W7sZ4vTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O2nOqgoY4vQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### %%%%%%%%%%%%%%%%%%%%%%شغال"
      ],
      "metadata": {
        "id": "ItmpJXQx41up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ف الترمنال\n",
        "transformers serve"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "6oC_RmQY4vMz",
        "outputId": "5d8cb012-cff9-4f1a-fe0d-f3cc83e4dcb6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-119145343.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-119145343.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    transformers serve\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"<random_string>\")\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is the Transformers library known for?\"\n",
        "        }\n",
        "    ],\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in completion:\n",
        "    token = chunk.choices[0].delta.content\n",
        "    if token:\n",
        "        print(token, end='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_Z8KrjR36Qp",
        "outputId": "57af5312-d2bd-4cf3-f369-867b2e85fa01"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Transformers library is known for several reasons:\n",
            "\n",
            "1. Extensive documentation: The library comes with extensive documentation that provides comprehensive information on its features and usage.\n",
            "\n",
            "2. Robustness and stability: It has been actively maintained over time, which ensures it remains stable and reliable as new versions are released.\n",
            "\n",
            "3. Large community support: There's a large and active community of developers who contribute to the library, helping to improve it through pull requests and discussions.\n",
            "\n",
            "4. Cross-platform compatibility: Many applications can run on different platforms (Windows, macOS, Linux), making it easier for users to use the library across various devices and operating systems.\n",
            "\n",
            "5. Modernized API: The library includes modernized APIs that follow best practices in web development, ensuring that the codebase remains up-to-date and maintainable.\n",
            "\n",
            "6. Community contributions: Regularly updated and reviewed by the community, it maintains high standards of quality and reliability.\n",
            "\n",
            "7. Security enhancements: The library includes security improvements such as input validation and error handling, enhancing the overall user experience.\n",
            "\n",
            "8. Scalability: Designed with scalability in mind, allowing developers to easily scale their applications without requiring significant changes to the underlying technology.\n",
            "\n",
            "9. Performance optimization: The library often includes performance optimizations, making it faster than alternative libraries when dealing with complex datasets or large-scale applications.\n",
            "\n",
            "10. Continuous integration and deployment: The library supports continuous integration and deployment, allowing developers to quickly test and deploy updates without having to rebuild from scratch each time.\n",
            "\n",
            "Overall, the Transformers library is known for being robust, well-documented, and constantly evolving to meet the needs of modern software development."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "%%%%%%%%%%%%%%%"
      ],
      "metadata": {
        "id": "6HDS8EDC43_5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XcIW1qhB45qp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}