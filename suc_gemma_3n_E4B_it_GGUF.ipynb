{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rc6ukCIDr1Uj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RmOUFJV_wiIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ESun8gAjwiLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_K_M.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0js0OmrlwiNZ",
        "outputId": "eafabc3d-47bf-4516-d572-c6a392f19c41"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-02 01:31:07--  https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.168.73.111, 3.168.73.106, 3.168.73.38, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.168.73.111|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/685d3c0352e700d127394d36/362b2c2e867b05e30a9315c640fd66ae953005ea1d2c40b8ce37b0066603a8f2?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250702%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250702T013107Z&X-Amz-Expires=3600&X-Amz-Signature=1cb4b82df9df6c55413ca1693d8be4f6f8cc87e52e04a48c627434c511af9a6c&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27gemma-3n-E4B-it-Q4_K_M.gguf%3B+filename%3D%22gemma-3n-E4B-it-Q4_K_M.gguf%22%3B&x-id=GetObject&Expires=1751423467&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTQyMzQ2N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODVkM2MwMzUyZTcwMGQxMjczOTRkMzYvMzYyYjJjMmU4NjdiMDVlMzBhOTMxNWM2NDBmZDY2YWU5NTMwMDVlYTFkMmM0MGI4Y2UzN2IwMDY2NjAzYThmMioifV19&Signature=V3UpWhl%7EWEP1rg-XMznxVq3BA4hqANSZW0giOkut30BvxveErMPp-XenJkl8m9bvC6j0FvQf-3flt2uET0Ds%7EZfbJZi87CMYWEt%7EvJqT0ricwBz1kfJY0u16SaUakKr5PemdUleqeUz6EgYEEedlpO3Cdx7HWVaIBemOTsRg97bnNP%7ELV7%7Ebcf7EXHoVhS-Q%7Ed%7EDHSimt1wO9pdWORdfix3R8DHLJsCL5GLMhWggz72SNnGguCWNasMBTmH6%7EKobrooEwCOxbxdQL3C%7EHdZ5VT7G-hHBoF1tbNajCkKXfp1FH3jh-KeZwi%7EbOM3rxbJmx%7E6JY1gw9VdFi%7EZdQZWt0g__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-07-02 01:31:07--  https://cas-bridge.xethub.hf.co/xet-bridge-us/685d3c0352e700d127394d36/362b2c2e867b05e30a9315c640fd66ae953005ea1d2c40b8ce37b0066603a8f2?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250702%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250702T013107Z&X-Amz-Expires=3600&X-Amz-Signature=1cb4b82df9df6c55413ca1693d8be4f6f8cc87e52e04a48c627434c511af9a6c&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27gemma-3n-E4B-it-Q4_K_M.gguf%3B+filename%3D%22gemma-3n-E4B-it-Q4_K_M.gguf%22%3B&x-id=GetObject&Expires=1751423467&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTQyMzQ2N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODVkM2MwMzUyZTcwMGQxMjczOTRkMzYvMzYyYjJjMmU4NjdiMDVlMzBhOTMxNWM2NDBmZDY2YWU5NTMwMDVlYTFkMmM0MGI4Y2UzN2IwMDY2NjAzYThmMioifV19&Signature=V3UpWhl%7EWEP1rg-XMznxVq3BA4hqANSZW0giOkut30BvxveErMPp-XenJkl8m9bvC6j0FvQf-3flt2uET0Ds%7EZfbJZi87CMYWEt%7EvJqT0ricwBz1kfJY0u16SaUakKr5PemdUleqeUz6EgYEEedlpO3Cdx7HWVaIBemOTsRg97bnNP%7ELV7%7Ebcf7EXHoVhS-Q%7Ed%7EDHSimt1wO9pdWORdfix3R8DHLJsCL5GLMhWggz72SNnGguCWNasMBTmH6%7EKobrooEwCOxbxdQL3C%7EHdZ5VT7G-hHBoF1tbNajCkKXfp1FH3jh-KeZwi%7EbOM3rxbJmx%7E6JY1gw9VdFi%7EZdQZWt0g__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 13.33.252.122, 13.33.252.50, 13.33.252.17, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|13.33.252.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4539054208 (4.2G)\n",
            "Saving to: ‘gemma-3n-E4B-it-Q4_K_M.gguf’\n",
            "\n",
            "gemma-3n-E4B-it-Q4_ 100%[===================>]   4.23G   235MB/s    in 22s     \n",
            "\n",
            "2025-07-02 01:31:29 (193 MB/s) - ‘gemma-3n-E4B-it-Q4_K_M.gguf’ saved [4539054208/4539054208]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/content/build/bin && /content/build/bin/llama-cli -h"
      ],
      "metadata": {
        "id": "T-aacapNwjXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/ggml-org/llama.cpp/releases/download/b5797/llama-b5797-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5w8yLlQw9aK",
        "outputId": "d2bffaf1-3dbd-450a-f68d-a6289f3de366"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-02 01:32:52--  https://github.com/ggml-org/llama.cpp/releases/download/b5797/llama-b5797-bin-ubuntu-x64.zip\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/612354784/b53cfb4a-db79-4fef-b80a-34d8fc50fff4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250702%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250702T013252Z&X-Amz-Expires=1800&X-Amz-Signature=2f80e483d893a6c62511deef858e56329c0f07ee56e9bd9947dbd3aa8b6df7a3&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dllama-b5797-bin-ubuntu-x64.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-07-02 01:32:52--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/612354784/b53cfb4a-db79-4fef-b80a-34d8fc50fff4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250702%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250702T013252Z&X-Amz-Expires=1800&X-Amz-Signature=2f80e483d893a6c62511deef858e56329c0f07ee56e9bd9947dbd3aa8b6df7a3&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dllama-b5797-bin-ubuntu-x64.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12951806 (12M) [application/octet-stream]\n",
            "Saving to: ‘llama-b5797-bin-ubuntu-x64.zip’\n",
            "\n",
            "llama-b5797-bin-ubu 100%[===================>]  12.35M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-07-02 01:32:52 (179 MB/s) - ‘llama-b5797-bin-ubuntu-x64.zip’ saved [12951806/12951806]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip llama-b5797-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PArOAtJIw-oc",
        "outputId": "f493a249-95f4-4f28-91e5-73b9ebe2a74f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  llama-b5797-bin-ubuntu-x64.zip\n",
            "  inflating: build/bin/LICENSE       \n",
            "  inflating: build/bin/LICENSE-curl  \n",
            "  inflating: build/bin/LICENSE-httplib  \n",
            "  inflating: build/bin/LICENSE-jsonhpp  \n",
            "  inflating: build/bin/LICENSE-linenoise  \n",
            "  inflating: build/bin/libggml-base.so  \n",
            "  inflating: build/bin/libggml-cpu-alderlake.so  \n",
            "  inflating: build/bin/libggml-cpu-haswell.so  \n",
            "  inflating: build/bin/libggml-cpu-icelake.so  \n",
            "  inflating: build/bin/libggml-cpu-sandybridge.so  \n",
            "  inflating: build/bin/libggml-cpu-sapphirerapids.so  \n",
            "  inflating: build/bin/libggml-cpu-skylakex.so  \n",
            "  inflating: build/bin/libggml-cpu-sse42.so  \n",
            "  inflating: build/bin/libggml-cpu-x64.so  \n",
            "  inflating: build/bin/libggml-rpc.so  \n",
            "  inflating: build/bin/libggml.so    \n",
            "  inflating: build/bin/libllama.so   \n",
            "  inflating: build/bin/libmtmd.so    \n",
            "  inflating: build/bin/llama-batched-bench  \n",
            "  inflating: build/bin/llama-bench   \n",
            "  inflating: build/bin/llama-cli     \n",
            "  inflating: build/bin/llama-gemma3-cli  \n",
            "  inflating: build/bin/llama-gguf-split  \n",
            "  inflating: build/bin/llama-imatrix  \n",
            "  inflating: build/bin/llama-llava-cli  \n",
            "  inflating: build/bin/llama-minicpmv-cli  \n",
            "  inflating: build/bin/llama-mtmd-cli  \n",
            "  inflating: build/bin/llama-perplexity  \n",
            "  inflating: build/bin/llama-quantize  \n",
            "  inflating: build/bin/llama-qwen2vl-cli  \n",
            "  inflating: build/bin/llama-run     \n",
            "  inflating: build/bin/llama-server  \n",
            "  inflating: build/bin/llama-tokenize  \n",
            "  inflating: build/bin/llama-tts     \n",
            "  inflating: build/bin/rpc-server    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/content/build/bin && /content/build/bin/llama-cli -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ztBgiljxDup",
        "outputId": "2ba160c6-de4c-4d52-9aa9-364999147355"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_backend: loaded RPC backend from /content/build/bin/libggml-rpc.so\n",
            "load_backend: loaded CPU backend from /content/build/bin/libggml-cpu-haswell.so\n",
            "----- common params -----\n",
            "\n",
            "-h,    --help, --usage                  print usage and exit\n",
            "--version                               show version and build info\n",
            "--completion-bash                       print source-able bash completion script for llama.cpp\n",
            "--verbose-prompt                        print a verbose prompt before generation (default: false)\n",
            "-t,    --threads N                      number of threads to use during generation (default: -1)\n",
            "                                        (env: LLAMA_ARG_THREADS)\n",
            "-tb,   --threads-batch N                number of threads to use during batch and prompt processing (default:\n",
            "                                        same as --threads)\n",
            "-C,    --cpu-mask M                     CPU affinity mask: arbitrarily long hex. Complements cpu-range\n",
            "                                        (default: \"\")\n",
            "-Cr,   --cpu-range lo-hi                range of CPUs for affinity. Complements --cpu-mask\n",
            "--cpu-strict <0|1>                      use strict CPU placement (default: 0)\n",
            "--prio N                                set process/thread priority : low(-1), normal(0), medium(1), high(2),\n",
            "                                        realtime(3) (default: 0)\n",
            "--poll <0...100>                        use polling level to wait for work (0 - no polling, default: 50)\n",
            "-Cb,   --cpu-mask-batch M               CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch\n",
            "                                        (default: same as --cpu-mask)\n",
            "-Crb,  --cpu-range-batch lo-hi          ranges of CPUs for affinity. Complements --cpu-mask-batch\n",
            "--cpu-strict-batch <0|1>                use strict CPU placement (default: same as --cpu-strict)\n",
            "--prio-batch N                          set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
            "                                        (default: 0)\n",
            "--poll-batch <0|1>                      use polling to wait for work (default: same as --poll)\n",
            "-c,    --ctx-size N                     size of the prompt context (default: 4096, 0 = loaded from model)\n",
            "                                        (env: LLAMA_ARG_CTX_SIZE)\n",
            "-n,    --predict, --n-predict N         number of tokens to predict (default: -1, -1 = infinity, -2 = until\n",
            "                                        context filled)\n",
            "                                        (env: LLAMA_ARG_N_PREDICT)\n",
            "-b,    --batch-size N                   logical maximum batch size (default: 2048)\n",
            "                                        (env: LLAMA_ARG_BATCH)\n",
            "-ub,   --ubatch-size N                  physical maximum batch size (default: 512)\n",
            "                                        (env: LLAMA_ARG_UBATCH)\n",
            "--keep N                                number of tokens to keep from the initial prompt (default: 0, -1 =\n",
            "                                        all)\n",
            "--swa-full                              use full-size SWA cache (default: false)\n",
            "                                        [(more\n",
            "                                        info)](https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n",
            "                                        (env: LLAMA_ARG_SWA_FULL)\n",
            "-fa,   --flash-attn                     enable Flash Attention (default: disabled)\n",
            "                                        (env: LLAMA_ARG_FLASH_ATTN)\n",
            "-p,    --prompt PROMPT                  prompt to start generation with; for system message, use -sys\n",
            "--no-perf                               disable internal libllama performance timings (default: false)\n",
            "                                        (env: LLAMA_ARG_NO_PERF)\n",
            "-f,    --file FNAME                     a file containing the prompt (default: none)\n",
            "-bf,   --binary-file FNAME              binary file containing the prompt (default: none)\n",
            "-e,    --escape                         process escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\) (default: true)\n",
            "--no-escape                             do not process escape sequences\n",
            "--rope-scaling {none,linear,yarn}       RoPE frequency scaling method, defaults to linear unless specified by\n",
            "                                        the model\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALING_TYPE)\n",
            "--rope-scale N                          RoPE context scaling factor, expands context by a factor of N\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALE)\n",
            "--rope-freq-base N                      RoPE base frequency, used by NTK-aware scaling (default: loaded from\n",
            "                                        model)\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_BASE)\n",
            "--rope-freq-scale N                     RoPE frequency scaling factor, expands context by a factor of 1/N\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_SCALE)\n",
            "--yarn-orig-ctx N                       YaRN: original context size of model (default: 0 = model training\n",
            "                                        context size)\n",
            "                                        (env: LLAMA_ARG_YARN_ORIG_CTX)\n",
            "--yarn-ext-factor N                     YaRN: extrapolation mix factor (default: -1.0, 0.0 = full\n",
            "                                        interpolation)\n",
            "                                        (env: LLAMA_ARG_YARN_EXT_FACTOR)\n",
            "--yarn-attn-factor N                    YaRN: scale sqrt(t) or attention magnitude (default: 1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_ATTN_FACTOR)\n",
            "--yarn-beta-slow N                      YaRN: high correction dim or alpha (default: 1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_SLOW)\n",
            "--yarn-beta-fast N                      YaRN: low correction dim or beta (default: 32.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_FAST)\n",
            "-nkvo, --no-kv-offload                  disable KV offload\n",
            "                                        (env: LLAMA_ARG_NO_KV_OFFLOAD)\n",
            "-ctk,  --cache-type-k TYPE              KV cache data type for K\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_K)\n",
            "-ctv,  --cache-type-v TYPE              KV cache data type for V\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_V)\n",
            "-dt,   --defrag-thold N                 KV cache defragmentation threshold (default: 0.1, < 0 - disabled)\n",
            "                                        (env: LLAMA_ARG_DEFRAG_THOLD)\n",
            "-np,   --parallel N                     number of parallel sequences to decode (default: 1)\n",
            "                                        (env: LLAMA_ARG_N_PARALLEL)\n",
            "--rpc SERVERS                           comma separated list of RPC servers\n",
            "                                        (env: LLAMA_ARG_RPC)\n",
            "--mlock                                 force system to keep model in RAM rather than swapping or compressing\n",
            "                                        (env: LLAMA_ARG_MLOCK)\n",
            "--no-mmap                               do not memory-map model (slower load but may reduce pageouts if not\n",
            "                                        using mlock)\n",
            "                                        (env: LLAMA_ARG_NO_MMAP)\n",
            "--numa TYPE                             attempt optimizations that help on some NUMA systems\n",
            "                                        - distribute: spread execution evenly over all nodes\n",
            "                                        - isolate: only spawn threads on CPUs on the node that execution\n",
            "                                        started on\n",
            "                                        - numactl: use the CPU map provided by numactl\n",
            "                                        if run without this previously, it is recommended to drop the system\n",
            "                                        page cache before using this\n",
            "                                        see https://github.com/ggml-org/llama.cpp/issues/1437\n",
            "                                        (env: LLAMA_ARG_NUMA)\n",
            "-dev,  --device <dev1,dev2,..>          comma-separated list of devices to use for offloading (none = don't\n",
            "                                        offload)\n",
            "                                        use --list-devices to see a list of available devices\n",
            "                                        (env: LLAMA_ARG_DEVICE)\n",
            "--list-devices                          print list of available devices and exit\n",
            "--override-tensor, -ot <tensor name pattern>=<buffer type>,...\n",
            "                                        override tensor buffer type\n",
            "-ngl,  --gpu-layers, --n-gpu-layers N   number of layers to store in VRAM\n",
            "                                        (env: LLAMA_ARG_N_GPU_LAYERS)\n",
            "-sm,   --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:\n",
            "                                        - none: use one GPU only\n",
            "                                        - layer (default): split layers and KV across GPUs\n",
            "                                        - row: split rows across GPUs\n",
            "                                        (env: LLAMA_ARG_SPLIT_MODE)\n",
            "-ts,   --tensor-split N0,N1,N2,...      fraction of the model to offload to each GPU, comma-separated list of\n",
            "                                        proportions, e.g. 3,1\n",
            "                                        (env: LLAMA_ARG_TENSOR_SPLIT)\n",
            "-mg,   --main-gpu INDEX                 the GPU to use for the model (with split-mode = none), or for\n",
            "                                        intermediate results and KV (with split-mode = row) (default: 0)\n",
            "                                        (env: LLAMA_ARG_MAIN_GPU)\n",
            "--check-tensors                         check model tensor data for invalid values (default: false)\n",
            "--override-kv KEY=TYPE:VALUE            advanced option to override model metadata by key. may be specified\n",
            "                                        multiple times.\n",
            "                                        types: int, float, bool, str. example: --override-kv\n",
            "                                        tokenizer.ggml.add_bos_token=bool:false\n",
            "--no-op-offload                         disable offloading host tensor operations to device (default: false)\n",
            "--lora FNAME                            path to LoRA adapter (can be repeated to use multiple adapters)\n",
            "--lora-scaled FNAME SCALE               path to LoRA adapter with user defined scaling (can be repeated to use\n",
            "                                        multiple adapters)\n",
            "--control-vector FNAME                  add a control vector\n",
            "                                        note: this argument can be repeated to add multiple control vectors\n",
            "--control-vector-scaled FNAME SCALE     add a control vector with user defined scaling SCALE\n",
            "                                        note: this argument can be repeated to add multiple scaled control\n",
            "                                        vectors\n",
            "--control-vector-layer-range START END\n",
            "                                        layer range to apply the control vector(s) to, start and end inclusive\n",
            "-m,    --model FNAME                    model path (default: `models/$filename` with filename from `--hf-file`\n",
            "                                        or `--model-url` if set, otherwise models/7B/ggml-model-f16.gguf)\n",
            "                                        (env: LLAMA_ARG_MODEL)\n",
            "-mu,   --model-url MODEL_URL            model download url (default: unused)\n",
            "                                        (env: LLAMA_ARG_MODEL_URL)\n",
            "-hf,   -hfr, --hf-repo <user>/<model>[:quant]\n",
            "                                        Hugging Face model repository; quant is optional, case-insensitive,\n",
            "                                        default to Q4_K_M, or falls back to the first file in the repo if\n",
            "                                        Q4_K_M doesn't exist.\n",
            "                                        mmproj is also downloaded automatically if available. to disable, add\n",
            "                                        --no-mmproj\n",
            "                                        example: unsloth/phi-4-GGUF:q4_k_m\n",
            "                                        (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_REPO)\n",
            "-hfd,  -hfrd, --hf-repo-draft <user>/<model>[:quant]\n",
            "                                        Same as --hf-repo, but for the draft model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HFD_REPO)\n",
            "-hff,  --hf-file FILE                   Hugging Face model file. If specified, it will override the quant in\n",
            "                                        --hf-repo (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_FILE)\n",
            "-hfv,  -hfrv, --hf-repo-v <user>/<model>[:quant]\n",
            "                                        Hugging Face model repository for the vocoder model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_REPO_V)\n",
            "-hffv, --hf-file-v FILE                 Hugging Face model file for the vocoder model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_FILE_V)\n",
            "-hft,  --hf-token TOKEN                 Hugging Face access token (default: value from HF_TOKEN environment\n",
            "                                        variable)\n",
            "                                        (env: HF_TOKEN)\n",
            "--log-disable                           Log disable\n",
            "--log-file FNAME                        Log to file\n",
            "--log-colors                            Enable colored logging\n",
            "                                        (env: LLAMA_LOG_COLORS)\n",
            "-v,    --verbose, --log-verbose         Set verbosity level to infinity (i.e. log all messages, useful for\n",
            "                                        debugging)\n",
            "--offline                               Offline mode: forces use of cache, prevents network access\n",
            "                                        (env: LLAMA_OFFLINE)\n",
            "-lv,   --verbosity, --log-verbosity N   Set the verbosity threshold. Messages with a higher verbosity will be\n",
            "                                        ignored.\n",
            "                                        (env: LLAMA_LOG_VERBOSITY)\n",
            "--log-prefix                            Enable prefix in log messages\n",
            "                                        (env: LLAMA_LOG_PREFIX)\n",
            "--log-timestamps                        Enable timestamps in log messages\n",
            "                                        (env: LLAMA_LOG_TIMESTAMPS)\n",
            "-ctkd, --cache-type-k-draft TYPE        KV cache data type for K for the draft model\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_K_DRAFT)\n",
            "-ctvd, --cache-type-v-draft TYPE        KV cache data type for V for the draft model\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_V_DRAFT)\n",
            "\n",
            "\n",
            "----- sampling params -----\n",
            "\n",
            "--samplers SAMPLERS                     samplers that will be used for generation in the order, separated by\n",
            "                                        ';'\n",
            "                                        (default:\n",
            "                                        penalties;dry;top_n_sigma;top_k;typ_p;top_p;min_p;xtc;temperature)\n",
            "-s,    --seed SEED                      RNG seed (default: -1, use random seed for -1)\n",
            "--sampling-seq, --sampler-seq SEQUENCE\n",
            "                                        simplified sequence for samplers that will be used (default:\n",
            "                                        edskypmxt)\n",
            "--ignore-eos                            ignore end of stream token and continue generating (implies\n",
            "                                        --logit-bias EOS-inf)\n",
            "--temp N                                temperature (default: 0.8)\n",
            "--top-k N                               top-k sampling (default: 40, 0 = disabled)\n",
            "--top-p N                               top-p sampling (default: 0.9, 1.0 = disabled)\n",
            "--min-p N                               min-p sampling (default: 0.1, 0.0 = disabled)\n",
            "--top-nsigma N                          top-n-sigma sampling (default: -1.0, -1.0 = disabled)\n",
            "--xtc-probability N                     xtc probability (default: 0.0, 0.0 = disabled)\n",
            "--xtc-threshold N                       xtc threshold (default: 0.1, 1.0 = disabled)\n",
            "--typical N                             locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)\n",
            "--repeat-last-n N                       last n tokens to consider for penalize (default: 64, 0 = disabled, -1\n",
            "                                        = ctx_size)\n",
            "--repeat-penalty N                      penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)\n",
            "--presence-penalty N                    repeat alpha presence penalty (default: 0.0, 0.0 = disabled)\n",
            "--frequency-penalty N                   repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)\n",
            "--dry-multiplier N                      set DRY sampling multiplier (default: 0.0, 0.0 = disabled)\n",
            "--dry-base N                            set DRY sampling base value (default: 1.75)\n",
            "--dry-allowed-length N                  set allowed length for DRY sampling (default: 2)\n",
            "--dry-penalty-last-n N                  set DRY penalty for the last n tokens (default: -1, 0 = disable, -1 =\n",
            "                                        context size)\n",
            "--dry-sequence-breaker STRING           add sequence breaker for DRY sampling, clearing out default breakers\n",
            "                                        ('\\n', ':', '\"', '*') in the process; use \"none\" to not use any\n",
            "                                        sequence breakers\n",
            "--dynatemp-range N                      dynamic temperature range (default: 0.0, 0.0 = disabled)\n",
            "--dynatemp-exp N                        dynamic temperature exponent (default: 1.0)\n",
            "--mirostat N                            use Mirostat sampling.\n",
            "                                        Top K, Nucleus and Locally Typical samplers are ignored if used.\n",
            "                                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n",
            "--mirostat-lr N                         Mirostat learning rate, parameter eta (default: 0.1)\n",
            "--mirostat-ent N                        Mirostat target entropy, parameter tau (default: 5.0)\n",
            "-l,    --logit-bias TOKEN_ID(+/-)BIAS   modifies the likelihood of token appearing in the completion,\n",
            "                                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',\n",
            "                                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'\n",
            "--grammar GRAMMAR                       BNF-like grammar to constrain generations (see samples in grammars/\n",
            "                                        dir) (default: '')\n",
            "--grammar-file FNAME                    file to read grammar from\n",
            "-j,    --json-schema SCHEMA             JSON schema to constrain generations (https://json-schema.org/), e.g.\n",
            "                                        `{}` for any JSON object\n",
            "                                        For schemas w/ external $refs, use --grammar +\n",
            "                                        example/json_schema_to_grammar.py instead\n",
            "-jf,   --json-schema-file FILE          File containing a JSON schema to constrain generations\n",
            "                                        (https://json-schema.org/), e.g. `{}` for any JSON object\n",
            "                                        For schemas w/ external $refs, use --grammar +\n",
            "                                        example/json_schema_to_grammar.py instead\n",
            "\n",
            "\n",
            "----- example-specific params -----\n",
            "\n",
            "--no-display-prompt                     don't print prompt at generation (default: false)\n",
            "-co,   --color                          colorise output to distinguish prompt and user input from generations\n",
            "                                        (default: false)\n",
            "--no-context-shift                      disables context shift on infinite text generation (default: disabled)\n",
            "                                        (env: LLAMA_ARG_NO_CONTEXT_SHIFT)\n",
            "-sys,  --system-prompt PROMPT           system prompt to use with model (if applicable, depending on chat\n",
            "                                        template)\n",
            "-sysf, --system-prompt-file FNAME       a file containing the system prompt (default: none)\n",
            "-ptc,  --print-token-count N            print token count every N tokens (default: -1)\n",
            "--prompt-cache FNAME                    file to cache prompt state for faster startup (default: none)\n",
            "--prompt-cache-all                      if specified, saves user input and generations to cache as well\n",
            "--prompt-cache-ro                       if specified, uses the prompt cache but does not update it\n",
            "-r,    --reverse-prompt PROMPT          halt generation at PROMPT, return control in interactive mode\n",
            "-sp,   --special                        special tokens output enabled (default: false)\n",
            "-cnv,  --conversation                   run in conversation mode:\n",
            "                                        - does not print special tokens and suffix/prefix\n",
            "                                        - interactive mode is also enabled\n",
            "                                        (default: auto enabled if chat template is available)\n",
            "-no-cnv, --no-conversation              force disable conversation mode (default: false)\n",
            "-st,   --single-turn                    run conversation for a single turn only, then exit when done\n",
            "                                        will not be interactive if first turn is predefined with --prompt\n",
            "                                        (default: false)\n",
            "-i,    --interactive                    run in interactive mode (default: false)\n",
            "-if,   --interactive-first              run in interactive mode and wait for input right away (default: false)\n",
            "-mli,  --multiline-input                allows you to write or paste multiple lines without ending each in '\\'\n",
            "--in-prefix-bos                         prefix BOS to user inputs, preceding the `--in-prefix` string\n",
            "--in-prefix STRING                      string to prefix user inputs with (default: empty)\n",
            "--in-suffix STRING                      string to suffix after user inputs with (default: empty)\n",
            "--no-warmup                             skip warming up the model with an empty run\n",
            "-gan,  --grp-attn-n N                   group-attention factor (default: 1)\n",
            "                                        (env: LLAMA_ARG_GRP_ATTN_N)\n",
            "-gaw,  --grp-attn-w N                   group-attention width (default: 512)\n",
            "                                        (env: LLAMA_ARG_GRP_ATTN_W)\n",
            "--jinja                                 use jinja template for chat (default: disabled)\n",
            "                                        (env: LLAMA_ARG_JINJA)\n",
            "--reasoning-format FORMAT               controls whether thought tags are allowed and/or extracted from the\n",
            "                                        response, and in which format they're returned; one of:\n",
            "                                        - none: leaves thoughts unparsed in `message.content`\n",
            "                                        - deepseek: puts thoughts in `message.reasoning_content` (except in\n",
            "                                        streaming mode, which behaves as `none`)\n",
            "                                        (default: deepseek)\n",
            "                                        (env: LLAMA_ARG_THINK)\n",
            "--reasoning-budget N                    controls the amount of thinking allowed; currently only one of: -1 for\n",
            "                                        unrestricted thinking budget, or 0 to disable thinking (default: -1)\n",
            "                                        (env: LLAMA_ARG_THINK_BUDGET)\n",
            "--chat-template JINJA_TEMPLATE          set custom jinja chat template (default: template taken from model's\n",
            "                                        metadata)\n",
            "                                        if suffix/prefix are specified, template will be disabled\n",
            "                                        only commonly used templates are accepted (unless --jinja is set\n",
            "                                        before this flag):\n",
            "                                        list of built-in templates:\n",
            "                                        bailing, chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2,\n",
            "                                        deepseek3, exaone3, falcon3, gemma, gigachat, glmedge, granite,\n",
            "                                        llama2, llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, llama4,\n",
            "                                        megrez, minicpm, mistral-v1, mistral-v3, mistral-v3-tekken,\n",
            "                                        mistral-v7, mistral-v7-tekken, monarch, openchat, orion, phi3, phi4,\n",
            "                                        rwkv-world, smolvlm, vicuna, vicuna-orca, yandex, zephyr\n",
            "                                        (env: LLAMA_ARG_CHAT_TEMPLATE)\n",
            "--chat-template-file JINJA_TEMPLATE_FILE\n",
            "                                        set custom jinja chat template file (default: template taken from\n",
            "                                        model's metadata)\n",
            "                                        if suffix/prefix are specified, template will be disabled\n",
            "                                        only commonly used templates are accepted (unless --jinja is set\n",
            "                                        before this flag):\n",
            "                                        list of built-in templates:\n",
            "                                        bailing, chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2,\n",
            "                                        deepseek3, exaone3, falcon3, gemma, gigachat, glmedge, granite,\n",
            "                                        llama2, llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, llama4,\n",
            "                                        megrez, minicpm, mistral-v1, mistral-v3, mistral-v3-tekken,\n",
            "                                        mistral-v7, mistral-v7-tekken, monarch, openchat, orion, phi3, phi4,\n",
            "                                        rwkv-world, smolvlm, vicuna, vicuna-orca, yandex, zephyr\n",
            "                                        (env: LLAMA_ARG_CHAT_TEMPLATE_FILE)\n",
            "--simple-io                             use basic IO for better compatibility in subprocesses and limited\n",
            "                                        consoles\n",
            "\n",
            "example usage:\n",
            "\n",
            "  text generation:     /content/build/bin/llama-cli -m your_model.gguf -p \"I believe the meaning of life is\" -n 128 -no-cnv\n",
            "\n",
            "  chat (conversation): /content/build/bin/llama-cli -m your_model.gguf -sys \"You are a helpful assistant\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/content/build/bin && /content/build/bin/llama-cli -m /content/gemma-3n-E4B-it-Q4_K_M.gguf -p \"hi\" -n 32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_LeObs3xGV5",
        "outputId": "3bde2868-d8a2-4b3d-f986-a70507b4fcc3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_backend: loaded RPC backend from /content/build/bin/libggml-rpc.so\n",
            "load_backend: loaded CPU backend from /content/build/bin/libggml-cpu-haswell.so\n",
            "build: 5797 (de569441) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 51 key-value pairs and 847 tensors from /content/gemma-3n-E4B-it-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gemma3n\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Gemma-3N-E4B-It\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = 3n-E4B-it\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Gemma-3N-E4B-It\n",
            "llama_model_loader: - kv   5:                       general.quantized_by str              = Unsloth\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 6.9B\n",
            "llama_model_loader: - kv   7:                            general.license str              = gemma\n",
            "llama_model_loader: - kv   8:                           general.repo_url str              = https://huggingface.co/unsloth\n",
            "llama_model_loader: - kv   9:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv  10:                  general.base_model.0.name str              = Gemma 3n E4B It\n",
            "llama_model_loader: - kv  11:          general.base_model.0.organization str              = Google\n",
            "llama_model_loader: - kv  12:              general.base_model.0.repo_url str              = https://huggingface.co/google/gemma-3...\n",
            "llama_model_loader: - kv  13:                               general.tags arr[str,6]       = [\"automatic-speech-recognition\", \"uns...\n",
            "llama_model_loader: - kv  14:                     gemma3n.context_length u32              = 32768\n",
            "llama_model_loader: - kv  15:                   gemma3n.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv  16:                        gemma3n.block_count u32              = 35\n",
            "llama_model_loader: - kv  17:                gemma3n.feed_forward_length arr[i32,35]      = [16384, 16384, 16384, 16384, 16384, 1...\n",
            "llama_model_loader: - kv  18:               gemma3n.attention.head_count u32              = 8\n",
            "llama_model_loader: - kv  19:   gemma3n.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  20:               gemma3n.attention.key_length u32              = 256\n",
            "llama_model_loader: - kv  21:             gemma3n.attention.value_length u32              = 256\n",
            "llama_model_loader: - kv  22:                     gemma3n.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  23:           gemma3n.attention.sliding_window u32              = 512\n",
            "llama_model_loader: - kv  24:            gemma3n.attention.head_count_kv u32              = 2\n",
            "llama_model_loader: - kv  25:                   gemma3n.altup.active_idx u32              = 0\n",
            "llama_model_loader: - kv  26:                   gemma3n.altup.num_inputs u32              = 4\n",
            "llama_model_loader: - kv  27:   gemma3n.embedding_length_per_layer_input u32              = 256\n",
            "llama_model_loader: - kv  28:         gemma3n.attention.shared_kv_layers u32              = 15\n",
            "llama_model_loader: - kv  29:          gemma3n.activation_sparsity_scale arr[f32,35]      = [1.644854, 1.644854, 1.644854, 1.6448...\n",
            "llama_model_loader: - kv  30:   gemma3n.attention.sliding_window_pattern arr[bool,35]     = [true, true, true, true, false, true,...\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {{ bos_token }}\\n{%- if messages[0]['r...\n",
            "llama_model_loader: - kv  32:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  33:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  34:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
            "llama_model_loader: - kv  35:                      tokenizer.ggml.scores arr[f32,262144]  = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  36:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  37:                tokenizer.ggml.bos_token_id u32              = 2\n",
            "llama_model_loader: - kv  38:                tokenizer.ggml.eos_token_id u32              = 106\n",
            "llama_model_loader: - kv  39:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  40:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  41:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  42:               tokenizer.ggml.add_sep_token bool             = false\n",
            "llama_model_loader: - kv  43:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  44:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  45:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  46:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  47:                      quantize.imatrix.file str              = gemma-3n-E4B-it-GGUF/imatrix_unsloth.dat\n",
            "llama_model_loader: - kv  48:                   quantize.imatrix.dataset str              = unsloth_calibration_gemma-3n-E4B-it.txt\n",
            "llama_model_loader: - kv  49:             quantize.imatrix.entries_count u32              = 459\n",
            "llama_model_loader: - kv  50:              quantize.imatrix.chunks_count u32              = 1326\n",
            "llama_model_loader: - type  f32:  422 tensors\n",
            "llama_model_loader: - type  f16:  108 tensors\n",
            "llama_model_loader: - type q5_1:    1 tensors\n",
            "llama_model_loader: - type q4_K:  282 tensors\n",
            "llama_model_loader: - type q6_K:   34 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 4.22 GiB (5.28 BPW) \n",
            "load: special tokens cache size = 6414\n",
            "load: token to piece cache size = 1.9446 MB\n",
            "print_info: arch             = gemma3n\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 2048\n",
            "print_info: n_layer          = 35\n",
            "print_info: n_head           = 8\n",
            "print_info: n_head_kv        = 2\n",
            "print_info: n_rot            = 256\n",
            "print_info: n_swa            = 512\n",
            "print_info: is_swa_any       = 1\n",
            "print_info: n_embd_head_k    = 256\n",
            "print_info: n_embd_head_v    = 256\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 512\n",
            "print_info: n_embd_v_gqa     = 512\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 1.0e+00\n",
            "print_info: n_ff             = 16384\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = E4B\n",
            "print_info: model params     = 6.87 B\n",
            "print_info: general.name     = Gemma-3N-E4B-It\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 262144\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 2 '<bos>'\n",
            "print_info: EOS token        = 106 '<end_of_turn>'\n",
            "print_info: EOT token        = 106 '<end_of_turn>'\n",
            "print_info: UNK token        = 3 '<unk>'\n",
            "print_info: PAD token        = 0 '<pad>'\n",
            "print_info: LF token         = 248 '<0x0A>'\n",
            "print_info: EOG token        = 106 '<end_of_turn>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: offloading 0 repeating layers to GPU\n",
            "load_tensors: offloaded 0/36 layers to GPU\n",
            "load_tensors:   CPU_REPACK model buffer size =  2079.00 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =  4322.24 MiB\n",
            ".......................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 2048\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: freq_base     = 1000000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "llama_context:        CPU  output buffer size =     1.00 MiB\n",
            "llama_kv_cache_unified_iswa: creating non-SWA KV cache, size = 4096 cells\n",
            "llama_kv_cache_unified:        CPU KV buffer size =    32.00 MiB\n",
            "llama_kv_cache_unified: size =   32.00 MiB (  4096 cells,   4 layers,  1 seqs), K (f16):   16.00 MiB, V (f16):   16.00 MiB\n",
            "llama_kv_cache_unified_iswa: creating     SWA KV cache, size = 1024 cells\n",
            "llama_kv_cache_unified:        CPU KV buffer size =    32.00 MiB\n",
            "llama_kv_cache_unified: size =   32.00 MiB (  1024 cells,  16 layers,  1 seqs), K (f16):   16.00 MiB, V (f16):   16.00 MiB\n",
            "llama_context:        CPU compute buffer size =   516.00 MiB\n",
            "llama_context: graph nodes  = 3266\n",
            "llama_context: graph splits = 1\n",
            "common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting\n",
            "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: llama threadpool init, n_threads = 1\n",
            "main: chat template is available, enabling conversation mode (disable it with -no-cnv)\n",
            "*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?\n",
            "main: chat template example:\n",
            "<start_of_turn>user\n",
            "You are a helpful assistant\n",
            "\n",
            "Hello<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Hi there<end_of_turn>\n",
            "<start_of_turn>user\n",
            "How are you?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "\n",
            "main: interactive mode on.\n",
            "sampler seed: 3533573174\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n",
            "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
            "generate: n_ctx = 4096, n_batch = 2048, n_predict = 32, n_keep = 1\n",
            "\n",
            "== Running in interactive mode. ==\n",
            " - Press Ctrl+C to interject at any time.\n",
            " - Press Return to return control to the AI.\n",
            " - To return control without starting a new line, end your input with '/'.\n",
            " - If you want to submit another line, end your input with '\\'.\n",
            " - Not using system message. To change it, set a different value via -sys PROMPT\n",
            "\n",
            "user\n",
            "hi\n",
            "model\n",
            "Hi there! 😊 \n",
            "\n",
            "How can I help you today?  \n",
            "\n",
            "I'm ready to answer questions, generate creative content, have a conversation, or\n",
            "> write python code for print welcome world\n",
            "```python\n",
            "print(\"Hello, World!\")\n",
            "```\n",
            "\n",
            "That's it\n",
            "> هل تفهم اللغة العربية؟\n",
            "نعم، أفهم اللغة العربية. يمكنني فهم النصوص العربية والإجابة على\n",
            "> هل يمكنك فهم نص فى ملف pdf\n",
            "أنا آسف، لا يمكنني حاليًا الوصول إلى الملفات المحلية على\n",
            "> خروج\n",
            "حسنًا، إلى اللقاء! إذا احتجت إلى أي شيء آخر في المستقبل، فلا تتر\n",
            "> "
          ]
        }
      ]
    }
  ]
}