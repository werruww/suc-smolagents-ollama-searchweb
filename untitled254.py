# -*- coding: utf-8 -*-
"""Untitled254.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TON-nou98yTiJAxo6lW51Sjikm4fGUzx
"""





!git clone https://github.com/kvcache-ai/ktransformers.git

!sudo apt install libtbb-dev libssl-dev libcurl4-openssl-dev libaio1 libaio-dev libgflags-dev zlib1g-dev libfmt-dev

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/kvcache-ai/ktransformers.git
# %cd ktransformers
!git submodule update --init --recursive

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/ktransformers
!bash install.sh

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/ktransformers

!git submodule update --init --recursive # Update PhotonLibOS submodule

# Begin from root of your cloned repo!
# Begin from root of your cloned repo!!
# Begin from root of your cloned repo!!!

# Download mzwing/DeepSeek-V2-Lite-Chat-GGUF from huggingface
mkdir DeepSeek-V2-Lite-Chat-GGUF
cd DeepSeek-V2-Lite-Chat-GGUF

wget https://huggingface.co/mradermacher/DeepSeek-V2-Lite-GGUF/resolve/main/DeepSeek-V2-Lite.Q4_K_M.gguf -O DeepSeek-V2-Lite-Chat.Q4_K_M.gguf

cd .. # Move to repo's root dir

# Start local chat
python -m ktransformers.local_chat --model_path deepseek-ai/DeepSeek-V2-Lite-Chat --gguf_path ./DeepSeek-V2-Lite-Chat-GGUF

# If you see “OSError: We couldn't connect to 'https://huggingface.co' to load this file”, try：
# GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite
# python  ktransformers.local_chat --model_path ./DeepSeek-V2-Lite --gguf_path ./DeepSeek-V2-Lite-Chat-GGUF

!mkdir a

# Commented out IPython magic to ensure Python compatibility.
# %cd a

!wget https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q2_K.gguf

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/ktransformers
!mkdir DeepSeek-V2-Lite-Chat-GGUF
# %cd DeepSeek-V2-Lite-Chat-GGUF

!wget https://huggingface.co/mradermacher/DeepSeek-V2-Lite-GGUF/resolve/main/DeepSeek-V2-Lite.Q4_K_M.gguf -O DeepSeek-V2-Lite-Chat.Q4_K_M.gguf

# Commented out IPython magic to ensure Python compatibility.
# %cd ..

!python -m ktransformers.local_chat --model_path deepseek-ai/DeepSeek-V2-Lite-Chat --gguf_path ./DeepSeek-V2-Lite-Chat-GGUF

# Example of how to initialize and use the submit method from the ktransformers documentation

from ktransformers.local_chat import init_local_chat

# Initialize the chat model. Replace with your actual model and gguf paths if needed.
# The init_local_chat function should return an object with a submit method.
chat_model = init_local_chat(
    model_path='deepseek-ai/DeepSeek-V2-Lite-Chat',
    gguf_path='./DeepSeek-V2-Lite-Chat-GGUF/DeepSeek-V2-Lite-Chat.Q4_K_M.gguf'
)

# Now you can use the submit method
if chat_model:
    response = chat_model.submit("Hello, how are you?")
    print(response)
else:
    print("Failed to initialize chat model.")

!pip install --upgrade ktransformers

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/ktransformers
!python -m ktransformers.local_chat --model_path ZZichen/DeepSeek-V2-Lite --gguf_path /content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/ktransformers
!python -m ktransformers.local_chat --model_path ./DeepSeek-V2-Lite --gguf_path ./DeepSeek-V2-Lite-Chat-GGUF

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/ktransformers
!python -m ktransformers.local_chat --model_path ZZichen/DeepSeek-V2-Lite --gguf_path /content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF

numactl -N 1 -m 1 python ./ktransformers/local_chat.py --model_path <your model path> --gguf_path <your gguf path>  --prompt_file <your prompt txt file>  --cpu_infer 33 --max_new_tokens 1000
<when you see chat, then press enter to load the text prompt_file>

!python ./ktransformers/local_chat.py --model_path ZZichen/DeepSeek-V2-Lite --gguf_path /content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF/DeepSeek-V2-Lite-Chat.Q4_K_M.gguf  --prompt_file /content/1.txt  --cpu_infer 3 --max_new_tokens 1

!CUDA_LAUNCH_BLOCKING=1 python ./ktransformers/local_chat.py --model_path ZZichen/DeepSeek-V2-Lite --gguf_path /content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF/DeepSeek-V2-Lite-Chat.Q4_K_M.gguf  --prompt_file /content/1.txt  --cpu_infer 3 --max_new_tokens 1

#!pip uninstall llama-cpp-python -y
!CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir

# الخطوة 1: تحديث وتثبيت أدوات البناء ومجموعة أدوات CUDA
!apt-get update
!apt-get install -y build-essential nvidia-cuda-toolkit

# الخطوة 2: الآن قم بتثبيت المكتبة مع تفعيل دعم CUDA
# نبدأ بإزالة أي محاولة تثبيت فاشلة سابقة لضمان البدء من الصفر
!pip uninstall llama-cpp-python -y
!CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir

!pip show torch

!python

# أولاً، تثبيت المكتبة مع دعم CUDA
#!pip uninstall llama-cpp-python -y
#!CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir

# ثانياً، استخدام المكتبة لتحميل النموذج والرد على سؤال
from llama_cpp import Llama

# تأكد من أن المسار صحيح
gguf_path = "/content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF/DeepSeek-V2-Lite-Chat.Q4_K_M.gguf"

# تحميل النموذج مع تحديد عدد الطبقات التي سيتم وضعها على الـ GPU
# n_gpu_layers=-1 يعني وضع كل الطبقات الممكنة على الـ GPU
llm = Llama(
  model_path=gguf_path,
  n_gpu_layers=-1,
  n_ctx=4096, # حجم السياق
  verbose=True # إظهار معلومات التحميل
)

# الآن يمكنك استخدامه
prompt = "ما هي عاصمة المملكة العربية السعودية؟"
output = llm(f"User: {prompt}\nAssistant:", max_tokens=100) # اضبط عدد التوكنات حسب الحاجة

print(output['choices'][0]['text'])

!wget https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu124/llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl

!pip install llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl

from llama_cpp import Llama

# تأكد من أن المسار صحيح
gguf_path = "/content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF/DeepSeek-V2-Lite-Chat.Q4_K_M.gguf"

# تحميل النموذج مع تحديد عدد الطبقات التي سيتم وضعها على الـ GPU
# n_gpu_layers=-1 يعني وضع كل الطبقات الممكنة على الـ GPU
llm = Llama(
  model_path=gguf_path,
  n_gpu_layers=-1,
  n_ctx=4096, # حجم السياق
  verbose=True # إظهار معلومات التحميل
)

# الآن يمكنك استخدامه
prompt = "ما هي عاصمة المملكة العربية السعودية؟"
output = llm(f"User: {prompt}\nAssistant:", max_tokens=100) # اضبط عدد التوكنات حسب الحاجة

print(output['choices'][0]['text'])

from llama_cpp import Llama

# تأكد من أن المسار صحيح
gguf_path = "/content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF/DeepSeek-V2-Lite-Chat.Q4_K_M.gguf"

# تحميل النموذج مع تحديد عدد الطبقات التي سيتم وضعها على الـ GPU
# n_gpu_layers=-1 يعني وضع كل الطبقات الممكنة على الـ GPU
llm = Llama(
  model_path=gguf_path,
  n_gpu_layers=-1,
  n_ctx=512, # حجم السياق
  verbose=True # إظهار معلومات التحميل
)

# الآن يمكنك استخدامه
prompt = "who is python?"
output = llm(f"User: {prompt}\nAssistant:", max_tokens=100) # اضبط عدد التوكنات حسب الحاجة

print(output['choices'][0]['text'])

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/ktransformers
!python -m ktransformers.local_chat --model_path ZZichen/DeepSeek-V2-Lite --gguf_path /content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/ktransformers
!python -m ktransformers.local_chat --model_path ZZichen/DeepSeek-V2-Lite --gguf_path /content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF -optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat-gpu-cpu.yaml --prompt_file /content/1.txt

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/ktransformers
!python -m ktransformers.local_chat --help

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/ktransformers
!python -m ktransformers.local_chat --model_path ZZichen/DeepSeek-V2-Lite --gguf_path /content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF -optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat-gpu-cpu.yaml --prompt_file /content/1.txt --device cuda

!python /content/ktransformers/ktransformers/local_chat.py --model_path ZZichen/DeepSeek-V2-Lite --gguf_path /content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF -optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat-gpu-cpu.yaml --prompt_file /content/1.txt --cpu_infer 2 --cpu_offload_gb 10

!python /content/ktransformers/ktransformers/local_chat.py \
  --model_path ZZichen/DeepSeek-V2-Lite \
  --gguf_path /content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF \
  --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat.yaml \
  --cpu_infer 2 \
  --cpu_offload_gb 10 \
  --prompt_file /content/1.txt \
  --device cpu

!python /content/ktransformers/ktransformers/local_chat.py \
  --model_path ZZichen/DeepSeek-V2-Lite \
  --gguf_path /content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF \
  --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat.yaml \
  --cpu_infer 1 \
  --prompt_file /content/1.txt \
  --device cpu

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/ktransformers

!python /content/ktransformers/ktransformers/local_chat.py \
  --model_path ZZichen/DeepSeek-V2-Lite \
  --gguf_path /content/ktransformers/DeepSeek-V2-Lite-Chat-GGUF/DeepSeek-V2-Lite-Chat.Q4_K_M.gguf \
  --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat-gpu-cpu.yaml \
  --cpu_infer 2 \
  --device cpu \