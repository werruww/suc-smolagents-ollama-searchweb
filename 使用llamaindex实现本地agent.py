# -*- coding: utf-8 -*-
"""使用LlamaIndex实现本地Agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wg1ZS0vCU_hUS-PYvFPUR52zU7B8ZHpB
"""

curl -fsSL https://ollama.com/install.sh | sh
nohup ollama serve &
ollama pull llama3:8b
ollama list

"""https://github.com/datawhalechina/handy-ollama/blob/main/notebook/C7/LlamaIndex_Agent/%E4%BD%BF%E7%94%A8LlamaIndex%E5%AE%9E%E7%8E%B0%E6%9C%AC%E5%9C%B0Agent.ipynb

https://github.com/datawhalechina/handy-ollama/tree/main/notebook
"""

!curl -fsSL https://ollama.com/install.sh | sh

!nohup ollama serve &qwen2.5:0.5b
!ollama pull qwen2.5:0.5b

!ollama list

!pip install llama-index

!nohup ollama serve &

from llama_index.core.tools import FunctionTool
from llama_index.core.agent import ReActAgent
from llama_index.llms.ollama import Ollama

# Define tools
def multiply(a: float, b: float) -> str:
    """Multiply two numbers and return the result as a formatted string."""
    result = a * b
    return f"The result of multiplying {a} and {b} is {result}."

# Create FunctionTool instances
multiply_tool = FunctionTool.from_defaults(
    fn=multiply,
    name="MultiplyTool",
    description="A tool that multiplies two floats."
    # Removed return_direct=True
)

# Initialize LLM
llm = Ollama(model="qwen2.5:0.5b", request_timeout=360.0)

# Initialize ReAct agent with tools
agent = ReActAgent(tools=[multiply_tool], llm=llm, verbose=True, agent_type="react")

# direct response
res_llm = llm.complete("What is 2.3 × 4.8 ? Calculate step by step")
print(res_llm)

# use agent
response = agent.run("What is 2.3 × 4.8 ? Calculate step by step")
# Try accessing different attributes to find the execution history
try:
    print("Response trace:", response.trace)
except AttributeError:
    try:
        print("Response steps:", response.steps)
    except AttributeError:
        try:
            print("Response events:", response.events)
        except AttributeError:
            print("Could not find trace, steps, or events attribute on the response object.")

!pip install llama-index-llms-ollama

# use agent
response = agent.chat("What is 2.3 × 4.8 ? Calculate step by step")
response.response

response = agent.query("What is 2.3 × 4.8 ? Calculate step by step")
print(response)

response = agent.("What is 2.3 × 4.8 ? Calculate step by step")
print(response)

response = agent.run("What is 2.3 × 4.8? Calculate step by step")
print(response)

response = agent.complete("What is 2.3 × 4.8? Calculate step by step")
print(response)

import llama_index
print(llama_index.core.__version__)









"""https://docs.llamaindex.ai/en/stable/examples/llm/ollama/"""

from llama_index.llms.ollama import Ollama

llm = Ollama(
    model="qwen2.5:0.5b",
    request_timeout=120.0,
    # Manually set the context window to limit memory usage
    context_window=8000,
)

resp = llm.complete("Who is Paul Graham?")

print(resp)



resp = llm.complete("What is 2.3 × 4.8? Calculate step by step")

print(resp)

from llama_index.core.llms import ChatMessage

messages = [
    ChatMessage(
        role="system", content="You are a pirate with a colorful personality"
    ),
    ChatMessage(role="user", content="What is your name"),
]
resp = llm.chat(messages)

print(resp)

from llama_index.llms.ollama import Ollama

llm = Ollama(
    model="qwen2.5:0.5b",
    request_timeout=360,
    thinking=True,
    # Manually set the context window to limit memory usage
    context_window=8000,
)

resp = llm.complete("What is 434 / 22?")

print(resp.additional_kwargs["thinking"])







!pip install llama-index-tools-mcp

!nohup ollama serve &

from llama_index.tools.mcp import (
    get_tools_from_mcp_url,
    aget_tools_from_mcp_url,
)

# async
tools = await aget_tools_from_mcp_url("http://127.0.0.1:8000/mcp")

from llama_index.tools.mcp import BasicMCPClient

client = BasicMCPClient("http://127.0.0.1:8000/mcp")

tools = await aget_tools_from_mcp_url(
    "http://127.0.0.1:8000/mcp",
    client=client,
    allowed_tools=["tool1", "tool2"],
)













"""https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook_ollama_replicate/#define-tools

https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook_ollama_replicate/#7-agents

https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook_ollama_replicate/#agents-and-tools
"""

!pip install llama-index
!pip install llama-index-llms-ollama
!pip install llama-index-llms-replicate
!pip install llama-index-embeddings-huggingface
!pip install llama-parse
!pip install replicate

!nohup ollama serve &
!ollama list

import nest_asyncio

nest_asyncio.apply()

from llama_index.llms.ollama import Ollama

llm = Ollama(
    model="qwen2.5:0.5b",
    request_timeout=120.0,
    # Manually set the context window to limit memory usage
    context_window=8000,
)

import json
from typing import Sequence, List

from llama_index.core.llms import ChatMessage
from llama_index.core.tools import BaseTool, FunctionTool
from llama_index.core.agent import ReActAgent

import nest_asyncio

nest_asyncio.apply()

def multiply(a: int, b: int) -> int:
    """Multiple two integers and returns the result integer"""
    return a * b


def add(a: int, b: int) -> int:
    """Add two integers and returns the result integer"""
    return a + b


def subtract(a: int, b: int) -> int:
    """Subtract two integers and returns the result integer"""
    return a - b


def divide(a: int, b: int) -> int:
    """Divides two integers and returns the result integer"""
    return a / b


multiply_tool = FunctionTool.from_defaults(fn=multiply)
add_tool = FunctionTool.from_defaults(fn=add)
subtract_tool = FunctionTool.from_defaults(fn=subtract)
divide_tool = FunctionTool.from_defaults(fn=divide)

agent = ReActAgent(
    tools=[multiply_tool, add_tool, subtract_tool, divide_tool],
    llm=llm, # Changed from llm_replicate to llm
    verbose=True,
)

response = agent.run("What is (121 + 2) * 5?")
print(str(response))













"""https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook_ollama_replicate/#7-agents"""

!pip install llama-index
!pip install llama-index-llms-ollama
!pip install llama-index-llms-replicate
!pip install llama-index-embeddings-huggingface
!pip install llama-parse
!pip install replicate

import nest_asyncio

nest_asyncio.apply()

from llama_index.llms.ollama import Ollama

llm = Ollama(
    model="qwen2.5:0.5b",
    request_timeout=120.0,
    # Manually set the context window to limit memory usage
    context_window=8000,
)

from llama_index.embeddings.huggingface import HuggingFaceEmbedding

embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")

from llama_index.core import Settings

Settings.llm = llm
Settings.embed_model = embed_model

!mkdir data
!wget "https://www.dropbox.com/scl/fi/t1soxfjdp0v44an6sdymd/drake_kendrick_beef.pdf?rlkey=u9546ymb7fj8lk2v64r6p5r5k&st=wjzzrgil&dl=1" -O data/drake_kendrick_beef.pdf
!wget "https://www.dropbox.com/scl/fi/nts3n64s6kymner2jppd6/drake.pdf?rlkey=hksirpqwzlzqoejn55zemk6ld&st=mohyfyh4&dl=1" -O data/drake.pdf
!wget "https://www.dropbox.com/scl/fi/8ax2vnoebhmy44bes2n1d/kendrick.pdf?rlkey=fhxvn94t5amdqcv9vshifd3hj&st=dxdtytn6&dl=1" -O data/kendrick.pdf

from llama_parse import LlamaParse

docs_kendrick = LlamaParse(api_key="YOUR_LLAMAPARSE_API_KEY", result_type="text").load_data("./data/kendrick.pdf")
docs_drake = LlamaParse(api_key="YOUR_LLAMAPARSE_API_KEY", result_type="text").load_data("./data/drake.pdf")
docs_both = LlamaParse(api_key="YOUR_LLAMAPARSE_API_KEY", result_type="text").load_data(
    "./data/drake_kendrick_beef.pdf"
)


# from llama_index.core import SimpleDirectoryReader

# docs_kendrick = SimpleDirectoryReader(input_files=["data/kendrick.pdf"]).load_data()
# docs_drake = SimpleDirectoryReader(input_files=["data/drake.pdf"]).load_data()
# docs_both = SimpleDirectoryReader(input_files=["data/drake_kendrick_beef.pdf"]).load_data()

from llama_parse import LlamaParse

docs_kendrick = LlamaParse(result_type="text").load_data("./data/kendrick.pdf")
docs_drake = LlamaParse(result_type="text").load_data("./data/drake.pdf")
docs_both = LlamaParse(result_type="text").load_data(
    "./data/drake_kendrick_beef.pdf"
)

from llama_parse import LlamaParse

from llama_index.core import SimpleDirectoryReader

docs_kendrick = SimpleDirectoryReader(input_files=["data/kendrick.pdf"]).load_data()
docs_drake = SimpleDirectoryReader(input_files=["data/drake.pdf"]).load_data()
docs_both = SimpleDirectoryReader(input_files=["data/drake_kendrick_beef.pdf"]).load_data()

response = llm.complete("do you like drake or kendrick better?")

print(response)

stream_response = llm.stream_complete(
    "you're a drake fan. tell me why you like drake more than kendrick"
)

for t in stream_response:
    print(t.delta, end="")

from llama_index.core.llms import ChatMessage

messages = [
    ChatMessage(role="system", content="You are Kendrick."),
    ChatMessage(role="user", content="Write a verse."),
]
response = llm.chat(messages)

print(response)

from llama_index.core import VectorStoreIndex

index = VectorStoreIndex.from_documents(docs_both)
query_engine = index.as_query_engine(similarity_top_k=3)

response = query_engine.query("Tell me about family matters")

print(str(response))

from llama_index.core import SummaryIndex

summary_index = SummaryIndex.from_documents(docs_both)
summary_engine = summary_index.as_query_engine()

response = summary_engine.query(
    "Given your assessment of this article, who won the beef?"
)

print(str(response))

from llama_index.core.tools import QueryEngineTool, ToolMetadata

vector_tool = QueryEngineTool(
    index.as_query_engine(),
    metadata=ToolMetadata(
        name="vector_search",
        description="Useful for searching for specific facts.",
    ),
)

summary_tool = QueryEngineTool(
    index.as_query_engine(response_mode="tree_summarize"),
    metadata=ToolMetadata(
        name="summary",
        description="Useful for summarizing an entire document.",
    ),
)

!pip install llama-index-program-openai

!nohup ollama serve &

from llama_index.core.query_engine import RouterQueryEngine

query_engine = RouterQueryEngine.from_defaults(
    [vector_tool, summary_tool], select_multi=False, verbose=True
)

response = query_engine.query(
    "Tell me about the song meet the grahams - why is it significant"
)

print(response)

drake_index = VectorStoreIndex.from_documents(docs_drake)
drake_query_engine = drake_index.as_query_engine(similarity_top_k=3)

kendrick_index = VectorStoreIndex.from_documents(docs_kendrick)
kendrick_query_engine = kendrick_index.as_query_engine(similarity_top_k=3)

from llama_index.core.tools import QueryEngineTool, ToolMetadata

drake_tool = QueryEngineTool(
    drake_index.as_query_engine(),
    metadata=ToolMetadata(
        name="drake_search",
        description="Useful for searching over Drake's life.",
    ),
)

kendrick_tool = QueryEngineTool(
    kendrick_index.as_query_engine(),
    metadata=ToolMetadata(
        name="kendrick_summary",
        description="Useful for searching over Kendrick's life.",
    ),
)

from llama_index.core.query_engine import SubQuestionQueryEngine

query_engine = SubQuestionQueryEngine.from_defaults(
    [drake_tool, kendrick_tool],
    llm=llm_replicate,  # llama3-70b
    verbose=True,
)

response = query_engine.query("Which albums did Drake release in his career?")

print(response)

!wget "https://www.sqlitetutorial.net/wp-content/uploads/2018/03/chinook.zip" -O "./data/chinook.zip"
!unzip "./data/chinook.zip"

from sqlalchemy import (
    create_engine,
    MetaData,
    Table,
    Column,
    String,
    Integer,
    select,
    column,
)

engine = create_engine("sqlite:///chinook.db")

from llama_index.core import SQLDatabase

sql_database = SQLDatabase(engine)

from llama_index.core.indices.struct_store import NLSQLTableQueryEngine

query_engine = NLSQLTableQueryEngine(
    sql_database=sql_database,
    tables=["albums", "tracks", "artists"],
    llm=llm_replicate,
)

response = query_engine.query("What are some albums?")

print(response)

response = query_engine.query("What are some artists? Limit it to 5.")

print(response)

response = query_engine.query(
    "What are some tracks from the artist AC/DC? Limit it to 3"
)

print(response)

print(response.metadata["sql_query"])

from llama_index.llms.ollama import Ollama
from llama_index.core.prompts import PromptTemplate
from pydantic import BaseModel


class Restaurant(BaseModel):
    """A restaurant with name, city, and cuisine."""

    name: str
    city: str
    cuisine: str


llm = Ollama(
    model="qwen2.5:0.5b",
    request_timeout=120.0,
    # Manually set the context window to limit memory usage
    context_window=8000,
)
prompt_tmpl = PromptTemplate(
    "Generate a restaurant in a given city {city_name}"
)

restaurant_obj = llm.structured_predict(
    Restaurant, prompt_tmpl, city_name="Miami"
)
print(restaurant_obj)

from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core.chat_engine import CondensePlusContextChatEngine

memory = ChatMemoryBuffer.from_defaults(token_limit=3900)

chat_engine = CondensePlusContextChatEngine.from_defaults(
    index.as_retriever(),
    memory=memory,
    llm=llm,
    context_prompt=(
        "You are a chatbot, able to have normal interactions, as well as talk"
        " about the Kendrick and Drake beef."
        "Here are the relevant documents for the context:\n"
        "{context_str}"
        "\nInstruction: Use the previous chat history, or the context above, to interact and help the user."
    ),
    verbose=True,
)

response = chat_engine.chat(
    "Tell me about the songs Drake released in the beef."
)
print(str(response))

response = chat_engine.chat("What about Kendrick?")
print(str(response))







import json
from typing import Sequence, List

from llama_index.core.llms import ChatMessage
from llama_index.core.tools import BaseTool, FunctionTool
from llama_index.core.agent import ReActAgent

import nest_asyncio

nest_asyncio.apply()

def multiply(a: int, b: int) -> int:
    """Multiple two integers and returns the result integer"""
    return a * b


def add(a: int, b: int) -> int:
    """Add two integers and returns the result integer"""
    return a + b


def subtract(a: int, b: int) -> int:
    """Subtract two integers and returns the result integer"""
    return a - b


def divide(a: int, b: int) -> int:
    """Divides two integers and returns the result integer"""
    return a / b


multiply_tool = FunctionTool.from_defaults(fn=multiply)
add_tool = FunctionTool.from_defaults(fn=add)
subtract_tool = FunctionTool.from_defaults(fn=subtract)
divide_tool = FunctionTool.from_defaults(fn=divide)

agent = ReActAgent.from_tools(
    query_engine_tools,  ## TODO: define query tools
    llm=llm,
    verbose=True,
)

response = agent.chat("What is (121 + 2) * 5?")
print(str(response))

!ollama list

from llama_parse import LlamaParse

docs_kendrick = LlamaParse(result_type="text").load_data("./data/kendrick.pdf")
docs_drake = LlamaParse(result_type="text").load_data("./data/drake.pdf")
docs_both = LlamaParse(result_type="text").load_data(
    "./data/drake_kendrick_beef.pdf"
)

from llama_index.llms.ollama import Ollama

llm = Ollama(
    model="qwen2.5:0.5b",
    request_timeout=120.0,
    # Manually set the context window to limit memory usage
    context_window=8000,
)

from llama_index.core import (
    SimpleDirectoryReader,
    VectorStoreIndex,
    StorageContext,
    load_index_from_storage,
)

from llama_index.core.tools import QueryEngineTool, ToolMetadata

from llama_index.embeddings.huggingface import HuggingFaceEmbedding

embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")

from llama_index.core import Settings

Settings.llm = llm
Settings.embed_model = embed_model

drake_tool = QueryEngineTool(
    drake_index.as_query_engine(),
    metadata=ToolMetadata(
        name="drake_search",
        description="Useful for searching over Drake's life.",
    ),
)

kendrick_tool = QueryEngineTool(
    kendrick_index.as_query_engine(),
    metadata=ToolMetadata(
        name="kendrick_search",
        description="Useful for searching over Kendrick's life.",
    ),
)

query_engine_tools = [drake_tool, kendrick_tool]

agent = ReActAgent(
    tools=[multiply_tool, add_tool, subtract_tool, divide_tool],
    llm=llm, # Changed from llm_replicate to llm
    verbose=True,
)

agent = ReActAgent.from_tools(
    query_engine_tools,  ## TODO: define query tools
    llm=llm_replicate,
    verbose=True,
)

response = agent.chat("Tell me about how Kendrick and Drake grew up")
print(str(response))









from llama_index.core.tools import FunctionTool
from llama_index.llms.ollama import Ollama
from llama_index.core.agent import ReActAgent

# تعريف الأدوات
def multiply(a: int, b: int) -> int:
    """يضرب عددين صحيحين ويعيد النتيجة"""
    return a * b

def add(a: int, b: int) -> int:
    """يجمع عددين صحيحين ويعيد النتيجة"""
    return a + b

multiply_tool = FunctionTool.from_defaults(fn=multiply)
add_tool = FunctionTool.from_defaults(fn=add)

# تهيئة النموذج من Ollama
llm = Ollama(model="qwen2.5:0.5b")

# إنشاء وكيل ReAct
agent = ReActAgent.from_tools([multiply_tool, add_tool], llm=llm, verbose=True)

# التفاعل مع الوكيل
response = agent.chat("ما هو ناتج 20 + (2 * 4)؟ قم بالحساب خطوة بخطوة.")
print(str(response))

# مثال آخر للتفاعل
response = agent.chat("ما هو 121 مضافًا إليه 2 ثم مضروبًا في 5؟")
print(str(response))

"""شغال"""

from llama_index.core.tools import FunctionTool
from llama_index.llms.ollama import Ollama
from llama_index.core.agent import ReActAgent

# تعريف الأدوات
def multiply(a: int, b: int) -> int:
    """يضرب عددين صحيحين ويعيد النتيجة"""
    return a * b

def add(a: int, b: int) -> int:
    """يجمع عددين صحيحين ويعيد النتيجة"""
    return a + b

multiply_tool = FunctionTool.from_defaults(fn=multiply)
add_tool = FunctionTool.from_defaults(fn=add)

# تهيئة النموذج من Ollama مع زيادة مدة المهلة
# الحل هنا: أضفنا معامل request_timeout
llm = Ollama(model="qwen2.5:0.5b", request_timeout=120.0)

# إنشاء وكيل ReAct
agent = ReActAgent.from_tools([multiply_tool, add_tool], llm=llm, verbose=True)

# التفاعل مع الوكيل
print("--- بدء الاستعلام الأول ---")
response = agent.chat("ما هو ناتج 20 + (2 * 4)؟ قم بالحساب خطوة بخطوة.")
print(str(response))

print("\n--- بدء الاستعلام الثاني ---")
response = agent.chat("ما هو 121 مضافًا إليه 2 ثم مضروبًا في 5؟")
print(str(response))

"""نعم، الكود الذي قدمته في المرة السابقة هو بالفعل agent يستخدم Ollama. هو يستخدم مكتبة LlamaIndex لإنشاء وكيل من نوع ReActAgent، ويعتمد على نموذج اللغة الذي تقدمه من خلال Ollama (qwen2.5:0.5b) ليفكر ويقرر أي أداة (جمع أو ضرب) سيستخدمها.
فالكود صحيح من حيث المبدأ والمن
"""









from llama_index.core import (
    SimpleDirectoryReader,
    VectorStoreIndex,
    StorageContext,
    load_index_from_storage,
)

from llama_index.core.tools import QueryEngineTool, ToolMetadata
drake_tool = QueryEngineTool(
    drake_index.as_query_engine(),
    metadata=ToolMetadata(
        name="drake_search",
        description="Useful for searching over Drake's life.",
    ),
)

kendrick_tool = QueryEngineTool(
    kendrick_index.as_query_engine(),
    metadata=ToolMetadata(
        name="kendrick_search",
        description="Useful for searching over Kendrick's life.",
    ),
)

query_engine_tools = [drake_tool, kendrick_tool]
agent = ReActAgent.from_tools(
    query_engine_tools,  ## TODO: define query tools
    llm=llm_replicate,
    verbose=True,
)

"""https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook_ollama_replicate/#7-agents

### شغال
"""

import os
from llama_index.core import (
    SimpleDirectoryReader,
    VectorStoreIndex,
)
from llama_index.core.tools import QueryEngineTool, ToolMetadata
from llama_index.llms.ollama import Ollama
from llama_index.core.agent import ReActAgent

# --- الخطوة 1: تهيئة نموذج اللغة من Ollama ---
# هذا هو المتغير 'llm' الذي يجب أن نستخدمه
llm = Ollama(model="qwen2.5:0.5b", request_timeout=120.0)


# --- الخطوة 2: إنشاء المجلدات والملفات (للتجربة فقط) ---
# يمكنك حذف هذا الجزء إذا قمت بإنشاء الملفات يدوياً
if not os.path.exists("drake_data"):
    os.makedirs("drake_data")
with open("drake_data/drake_facts.txt", "w") as f:
    f.write("Drake is a Canadian rapper, singer, and songwriter. He released the highly successful album Views in 2016.")

if not os.path.exists("kendrick_data"):
    os.makedirs("kendrick_data")
with open("kendrick_data/kendrick_facts.txt", "w") as f:
    f.write("Kendrick Lamar is an American rapper from Compton. He won a Pulitzer Prize for his album DAMN.")


# --- الخطوة 3: تحميل البيانات وإنشاء الفهارس (Indexes) ---
# تحميل بيانات Drake وإنشاء الفهرس الخاص به
drake_documents = SimpleDirectoryReader("./drake_data").load_data()
drake_index = VectorStoreIndex.from_documents(drake_documents)

# تحميل بيانات Kendrick وإنشاء الفهرس الخاص به
kendrick_documents = SimpleDirectoryReader("./kendrick_data").load_data()
kendrick_index = VectorStoreIndex.from_documents(kendrick_documents)


# --- الخطوة 4: تحويل الفهارس إلى أدوات (Tools) ---
drake_tool = QueryEngineTool(
    drake_index.as_query_engine(llm=llm), # من الأفضل تمرير النموذج هنا أيضاً
    metadata=ToolMetadata(
        name="drake_search",
        description="مفيد للبحث عن معلومات حول حياة دريك Drake وألبوماته.",
    ),
)

kendrick_tool = QueryEngineTool(
    kendrick_index.as_query_engine(llm=llm), # من الأفضل تمرير النموذج هنا أيضاً
    metadata=ToolMetadata(
        name="kendrick_search",
        description="مفيد للبحث عن معلومات حول حياة كيندريك لامار Kendrick Lamar وجوائزه.",
    ),
)


# --- الخطوة 5: إنشاء الوكيل (Agent) باستخدام الأدوات والمتغير 'llm' الصحيح ---
query_engine_tools = [drake_tool, kendrick_tool]

agent = ReActAgent.from_tools(
    query_engine_tools,
    llm=llm,  # <-- هنا تم استخدام المتغير الصحيح
    verbose=True,
)

# --- الخطوة 6: التفاعل مع الوكيل ---
response = agent.chat("ما هي الجائزة التي فاز بها كيندريك لامار؟")
print(str(response))

print("\n-------------------\n")

response = agent.chat("ما هو اسم الألبوم الذي أصدره دريك في عام 2016؟")
print(str(response))











!pip install llama-index llama-index-llms-ollama pypdf

import os
import asyncio

# --- تحميل وتثبيت Ollama ---
!curl -fsSL https://ollama.com/install.sh | sh

# --- تشغيل الخادم في الخلفية ---
# سيؤدي هذا إلى تشغيل الخادم والاحتفاظ به نشطًا أثناء جلسة Colab
# ستحتاج إلى تشغيله مرة واحدة فقط في بداية دفتر الملاحظات
async def run_ollama_serve():
    process = await asyncio.create_subprocess_shell(
        "ollama serve",
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    print("Ollama server started.")
    # لا ننتظر هنا حتى لا يتم حظر الخلية
    # سيعمل في الخلفية

# بدء تشغيل الخادم
# نتجاهل الخطأ إذا كانت الحلقة قيد التشغيل بالفعل
try:
    loop = asyncio.get_running_loop()
except RuntimeError:
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

loop.create_task(run_ollama_serve())

# انتظر بضع ثوانٍ للتأكد من بدء تشغيل الخادم
import time
time.sleep(5)

# --- سحب النموذج الذي تحتاجه ---
!ollama pull qwen2.5:0.5b

الخطوة 3: رفع ملف الـ PDF
في قائمة الملفات على يسار شاشة Colab، انقر على أيقونة المجلد.
أنشئ مجلداً جديداً باسم data.
ادخل إلى مجلد data ثم قم برفع ملف drake.pdf الخاص بك داخله.
تأكد من أن المسار النهائي للملف هو بالضبط: /content/data/drake.pdf.

from llama_index.core import (
    SimpleDirectoryReader,
    VectorStoreIndex,
)
from llama_index.core.tools import QueryEngineTool, ToolMetadata
from llama_index.llms.ollama import Ollama
from llama_index.core.agent import ReActAgent

# --- 1. تهيئة نموذج اللغة من Ollama (الذي يعمل الآن في Colab) ---
# تأكد من أن اسم النموذج يطابق النموذج الذي قمت بسحبه في الخطوة 2
llm = Ollama(model="qwen2.5:0.5b", request_timeout=240.0) # زدنا المهلة أكثر للاحتياط

# --- 2. تحميل البيانات من ملف PDF وإنشاء الفهرس ---
# يقرأ LlamaIndex جميع الملفات الموجودة في هذا المجلد
# في حالتنا، سيجد drake.pdf فقط
try:
    documents = SimpleDirectoryReader(input_dir="/content/data").load_data()
    drake_index = VectorStoreIndex.from_documents(documents)
except ValueError as e:
    print(f"خطأ: {e}")
    print("هل تأكدت من رفع ملف PDF إلى مجلد /content/data؟")


# --- 3. تحويل الفهرس إلى أداة (Tool) ---
# أنشأنا أداة واحدة فقط لأن لدينا مصدر بيانات واحد فقط
drake_tool = QueryEngineTool(
    query_engine=drake_index.as_query_engine(llm=llm),
    metadata=ToolMetadata(
        name="drake_document_search",
        description="مفيد للبحث عن معلومات موجودة في مستند PDF حول دريك (Drake).",
    ),
)

# --- 4. إنشاء الوكيل (Agent) ---
# نستخدم قائمة تحتوي على الأداة الوحيدة التي أنشأناها
agent = ReActAgent.from_tools(
    [drake_tool],
    llm=llm,
    verbose=True,
)

# --- 5. التفاعل مع الوكيل ---
# اطرح سؤالاً تكون إجابته موجودة في ملف الـ PDF
response = agent.chat("ما هي المعلومات التي يمكنك إيجادها في المستند عن Drake؟")
print(str(response))

"""Expand to view model thoughts

chevron_right
نعم، بالتأكيد هذا agent (وكيل). أنت نجحت في بناء وتشغيل وكيل ReAct كامل.
الكود الذي قمت بتشغيله يتبع بالضبط نمط عمل الوكيل:
يتلقى سؤالاً (مثل "ما هي الجائزة التي فاز بها كيندريك لامار؟").
يفكر (Thought): يحاول أن يضع خطة.
يتصرف (Action): يقرر إذا ما كان سيستخدم أداة (مثل kendrick_search) أو سيجيب مباشرة.
يراقب (Observation): يرى نتيجة تصرفه.
يكرر الخطوات حتى يصل إلى إجابة نهائية.
ولكن، النتائج التي حصلت عليها هي مثال ممتاز لتوضيح التحديات التي تواجه الوكلاء عند استخدام نماذج لغوية صغيرة.
دعنا نحلل لماذا كانت الإجابات خاطئة، على الرغم من أن بنية الكود صحيحة.
تحليل المشكلة: لماذا كانت الإجابات خاطئة؟
المشكلة ليست في الكود، بل في قدرة نموذج اللغة (qwen2.5:0.5b) على اتخاذ القرار الصحيح.
1. السؤال الأول (Kendrick Lamar):
ماذا فعل الوكيل:
Thought: I can answer without using any more tools.
قرر النموذج أنه يعرف الإجابة بنفسه ولم يستخدم الأداة kendrick_search التي قمت بتزويده بها.
النتيجة:
لأنه لم يبحث في الملف الصحيح، قام "بالهلوسة" (Hallucination) واخترع إجابة خاطئة تمامًا ("جائزة فؤاد للتأليف في 2019..."). هذه المعلومات ليست موجودة في الملف الذي قدمته.
2. السؤال الثاني (Drake):
ماذا فعل الوكيل:
Thought: (Implicit) I can answer without any more tools!
مرة أخرى، قرر النموذج أنه لا يحتاج إلى استخدام الأداة drake_search.
النتيجة:
بدلاً من البحث في الملف، اعتمد على معرفته المحدودة جداً وفشل في العثور على إجابة، لذلك قدم ردًا عامًا بالرفض ("أنا آسف، لا أستطيع البحث...").
السبب الجذري: قدرة النموذج اللغوي
نموذج qwen2.5:0.5b هو نموذج صغير جداً (0.5 مليار مُعامل). هذه النماذج ممتازة في مهام بسيطة مثل إكمال النصوص، ولكنها ضعيفة جدًا في مهام التفكير المعقدة مثل "استخدام الأدوات" (Tool Use).
عملية اختيار الأداة المناسبة تتطلب من النموذج:
فهم السؤال جيدًا.
قراءة وصف كل أداة متاحة (description).
الربط بين السؤال ووصف الأداة الأنسب.
اتخاذ قرار "سأستخدم هذه الأداة الآن".
النماذج الصغيرة غالبًا ما تفشل في هذه العملية المعقدة وتختار الطريق الأسهل: محاولة الإجابة من "ذاكرتها" المحدودة، مما يؤدي إلى الهلوسة أو الفشل.
الحل: استخدم نموذجاً أكبر وأكثر قدرة
الحل الأكثر فعالية هو استبدال النموذج الصغير بنموذج أكبر وأكثر ذكاءً معروف بقدرته على اتباع التعليمات واستخدام الأدوات.
جرب استخدام أحد هذه النماذج:
llama3:8b (خيار ممتاز وموصى به)
mistral:7b
gemma:7b
خطوات التغيير:
قم بتنزيل النموذج الجديد في نافذة الأوامر (Terminal):
code
Bash
ollama pull llama3:8b
عدّل سطراً واحداً فقط في كود البايثون الخاص بك:
code
Python
# السطر القديم
# llm = Ollama(model="qwen2.5:0.5b", request_timeout=120.0)

# السطر الجديد
llm = Ollama(model="llama3:8b", request_timeout=120.0)
عندما تشغل الكود مع llama3:8b، ستلاحظ أن مخرجات التفكير (Thought) ستكون مختلفة تمامًا. من المرجح أن ترى شيئًا مثل:
code
Code
Thought: The user is asking about an award won by Kendrick Lamar. I should use the `kendrick_search` tool to find this information.
Action: kendrick_search
Action Input: {'query': 'award won by Kendrick Lamar'}
وهذا سيؤدي إلى الإجابة الصحيحة المستخرجة من ملفك.
ملاحظة جانبية بخصوص التحذيرات (DeprecationWarning):
التحذيرات التي ظهرت لك هي من مكتبة llama-index نفسها، وهي تخبرك بأن طريقة استدعاء ReActAgent ستتغير في الإصدارات المستقبلية. هذا التحذير لا يؤثر على عمل الكود حاليًا ولا يسبب الإجابات الخاطئة، ولكنه مجرد تنبيه للمطورين.
خلاصة:
أنت بنيت وكيلاً ناجحاً، ولكنك أعطيته "دماغاً" صغيراً جداً. قم بترقية الدماغ (النموذج اللغوي) وسترى أداءً أفضل بكثير.
"""











"""https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook_ollama_replicate/#7-agents

### ReAct Agent With RAG QueryEngine Tools
"""

from llama_index.core import (
    SimpleDirectoryReader,
    VectorStoreIndex,
    StorageContext,
    load_index_from_storage,
)

from llama_index.core.tools import QueryEngineTool, ToolMetadata

drake_tool = QueryEngineTool(
    drake_index.as_query_engine(),
    metadata=ToolMetadata(
        name="drake_search",
        description="Useful for searching over Drake's life.",
    ),
)

kendrick_tool = QueryEngineTool(
    kendrick_index.as_query_engine(),
    metadata=ToolMetadata(
        name="kendrick_search",
        description="Useful for searching over Kendrick's life.",
    ),
)

query_engine_tools = [drake_tool, kendrick_tool]

agent = ReActAgent.from_tools(
    query_engine_tools,  ## TODO: define query tools
    llm=llm,
    verbose=True,
)

response = agent.chat("Tell me about how Kendrick and Drake grew up")
print(str(response))









"""### شغال"""

from llama_index.core import (
    SimpleDirectoryReader,
    VectorStoreIndex,
    StorageContext,
    load_index_from_storage,
)

from llama_index.core.tools import QueryEngineTool, ToolMetadata
drake_tool = QueryEngineTool(
    drake_index.as_query_engine(),
    metadata=ToolMetadata(
        name="drake_search",
        description="Useful for searching over Drake's life.",
    ),
)

kendrick_tool = QueryEngineTool(
    kendrick_index.as_query_engine(),
    metadata=ToolMetadata(
        name="kendrick_search",
        description="Useful for searching over Kendrick's life.",
    ),
)

query_engine_tools = [drake_tool, kendrick_tool]
agent = ReActAgent.from_tools(
    query_engine_tools,  ## TODO: define query tools
    llm=llm,
    verbose=True,
)
response = agent.chat("Tell me about how Kendrick and Drake grew up")
print(str(response))









"""### شغال

https://docs.llamaindex.ai/en/stable/examples/cookbooks/ollama_gpt_oss_cookbook/
"""

from llama_index.llms.ollama import Ollama

llm = Ollama(
    model="qwen2.5:0.5b",
    request_timeout=360,
    thinking=False,
    temperature=1.0,
    # Supports up to 130K tokens, lowering to save memory
    context_window=8000,
)

resp_gen = await llm.astream_complete("What is 1234 * 5678?")

still_thinking = True
print("====== THINKING ======")
async for chunk in resp_gen:
    if still_thinking and chunk.additional_kwargs.get("thinking_delta"):
        print(chunk.additional_kwargs["thinking_delta"], end="", flush=True)
    elif still_thinking:
        still_thinking = False
        print("\n====== ANSWER ======")

    if not still_thinking:
        print(chunk.delta, end="", flush=True)

"""
Creating agents with gpt-oss"""

from llama_index.core.agent.workflow import FunctionAgent
from llama_index.llms.ollama import Ollama


def multiply(a: int, b: int) -> int:
    """Multiply two numbers"""
    return a * b


llm = Ollama(
    model="qwen2.5:0.5b",
    request_timeout=360,
    thinking=False,
    temperature=1.0,
    # Supports up to 130K tokens, lowering to save memory
    context_window=8000,
)

agent = FunctionAgent(
    tools=[multiply],
    llm=llm,
    system_prompt="You are a helpful assistant that can multiply and add numbers. Always rely on tools for math operations.",
)

from llama_index.core.agent.workflow import (
    ToolCall,
    ToolCallResult,
    AgentStream,
)

handler = agent.run("What is 1234 * 5678?")
async for ev in handler.stream_events():
    if isinstance(ev, ToolCall):
        print(f"\nTool call: {ev.tool_name}({ev.tool_kwargs}")
    elif isinstance(ev, ToolCallResult):
        print(
            f"\nTool call: {ev.tool_name}({ev.tool_kwargs}) -> {ev.tool_output}"
        )
    elif isinstance(ev, AgentStream):
        print(ev.delta, end="", flush=True)

resp = await handler

from llama_index.core.workflow import Context

ctx = Context(agent)

resp = await agent.run("What is 1234 * 5678?", ctx=ctx)
resp = await agent.run("What was the last question/answer pair?", ctx=ctx)

print(resp.response.content)











"""https://docs.llamaindex.ai/en/stable/api_reference/llms/ollama/"""

from llama_index.llms.ollama import Ollama

llm = Ollama(model="qwen2.5:0.5b", request_timeout=60.0)

response = llm.complete("What is the capital of France?")
print(response)

"""https://docs.llamaindex.ai/en/stable/examples/llm/ollama/"""



"""https://github.com/datawhalechina/handy-ollama/blob/main/README_en.md"""











# Commented out IPython magic to ensure Python compatibility.
# %cd /content

!git clone https://github.com/MikeyBeez/Ollama_Agents.git

# Commented out IPython magic to ensure Python compatibility.
# %cd Ollama_Agents

!pip install -r requirements.txt

!python -m src.main

!python /content/Ollama_Agents/run_tests.py

!pip install -r requirements.txt

!python -m src.main

!python -m src.main -help

!python /content/Ollama_Agents/test_agent.py

!ollama list

!nohup ollama serve &

!python /content/Ollama_Agents/test_agent.py

!python -m src.main

"""https://github.com/MikeyBeez/Ollama_Agents"""



# Commented out IPython magic to ensure Python compatibility.
# %cd /content

# Commented out IPython magic to ensure Python compatibility.
# src/agents/advanced_agent.py
# %cd /content/Ollama_Agents
from src.modules.adv_input_processor import InputProcessor
from src.modules.adv_context_manager import ContextManager
from src.modules.adv_reasoning_engine import ReasoningEngine
from src.modules.adv_planning_engine import PlanningEngine
from src.modules.adv_knowledge_manager import KnowledgeManager
from src.modules.adv_output_manager import OutputManager
from config import DEFAULT_MODEL, AGENT_NAME, USER_NAME

class AdvancedAgent:
    def __init__(self, model_name="qwen2.5:0.5b"):
        self.config = {
            'DEFAULT_MODEL': model_name,
            'AGENT_NAME': AGENT_NAME,
            'USER_NAME': USER_NAME
        }
        self.input_processor = InputProcessor(self.config)
        self.context_manager = ContextManager(self.config)
        self.reasoning_engine = ReasoningEngine(model_name)
        self.planning_engine = PlanningEngine(model_name)
        self.knowledge_manager = KnowledgeManager()
        self.output_manager = OutputManager(self.config)

    def run(self):
        self.output_manager.print_welcome_message()
        self.input_processor.setup_user_profile()

        while True:
            user_input = self.input_processor.get_user_input()
            if user_input is None:
                break
            if user_input.startswith('/'):
                response = self.output_manager.execute_debug_command(user_input)
                if response is None:
                    break
            else:
                response = self._process_input(user_input)
            self.output_manager.display_response(response)

        self.output_manager.print_farewell_message()

    def _process_input(self, user_input: str) -> str:
        analysis = self.input_processor.analyze_input(user_input)
        context = self.context_manager.gather_context(user_input, analysis)
        knowledge = self.context_manager.retrieve_knowledge(context, analysis)

        causal_relationships = self.reasoning_engine.perform_causal_analysis(context)
        hypotheses = self.reasoning_engine.generate_hypotheses(context)
        tested_hypotheses = self.reasoning_engine.test_hypotheses(hypotheses, context)
        analogies = self.reasoning_engine.find_analogies(user_input, context)
        contradictions = self.reasoning_engine.detect_contradictions(knowledge)
        resolved_contradictions = self.reasoning_engine.resolve_contradictions(contradictions)

        plan = self.planning_engine.create_and_analyze_plan(
            user_input, context, knowledge, causal_relationships,
            tested_hypotheses, analogies, resolved_contradictions
        )
        response = self.planning_engine.generate_response_from_plan(
            plan, user_input, context, knowledge, causal_relationships,
            tested_hypotheses, analogies, resolved_contradictions
        )

        progress = self.planning_engine.assess_progress(response, plan)
        next_step = self.planning_engine.determine_next_step(plan, progress)
        response += f"\n\nNext step: {next_step}"

        self.knowledge_manager.update_conversation_history(user_input, response)
        self.knowledge_manager.update_knowledge_graph(analysis['topics'], response)
        self.knowledge_manager.update_bullet_points(response)

        return response

def main():
    agent = AdvancedAgent()
    agent.run()

if __name__ == "__main__":
    main()







"""https://github.com/premthomas/Ollama-and-Agents/blob/main/BlogWriter.ipynb"""



!pip install langchain ## Sometimes, this code alone does not work.
!pip install langchain-community langchain-core
!pip install -U langchain-ollama


!pip install crewai
!pip install crewai-tools


!pip install duckduckgo-search

!nohup ollama serve &

from crewai import Agent, Task, Crew
from langchain_community.tools import DuckDuckGoSearchRun
from langchain_community.llms import Ollama
# ollama_llm = Ollama(model="openhermes") # assuming you have Ollama installed and have openhermes model pulled with `ollama pull openhermes`
# ollama_llm = Ollama(model="phi")
ollama_llm = Ollama(model="qwen2.5:0.5b")
from crewai_tools import tool
@tool("Duck_Duck_Go_Search")
def ddgsearch(question: str) -> str:
    """Clear description for what this tool is useful for, your agent will need this information to use it."""
    # Function logic here
    return DuckDuckGoSearchRun().run(question)

from crewai_tools.tools import tool
@tool("Duck_Duck_Go_Search")
def ddgsearch(question: str) -> str:
    """Clear description for what this tool is useful for, your agent will need this information to use it."""
    # Function logic here
    return DuckDuckGoSearchRun().run(question)



# What is it that needs to get done?
research_topic = "Differences between open source large language models and closed source large language models."

researcher = Agent(
  role='Researcher',
  goal='Search the internet for information about {research_topic}',
  backstory="""
  You are a researcher. Using the information in the task, you find out some of the most popular facts about the topic along with some of the trending aspects.
  You provide a lot of information thereby allowing a choice in the content selected for the final blog.
  """,
  verbose=True,
  allow_delegation=False,
  # tools=[DuckDuckGoSearchRun()],
  tools=[ddgsearch],
  llm=ollama_llm
)

!pip install --upgrade --force-reinstall crewai-tools

!pip install duckduckgo-search

from crewai import Agent, Task, Crew
from langchain_community.tools import DuckDuckGoSearchRun