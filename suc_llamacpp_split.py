# -*- coding: utf-8 -*-
"""suc_llamacpp_split.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dh9jvNSVwm5ut54aw822YdcA2_S6SUVx
"""







!wget https://github.com/ggml-org/llama.cpp/releases/download/b5689/llama-b5689-bin-ubuntu-x64.zip

!unzip /content/llama-b5689-bin-ubuntu-x64.zip

!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/content/build/bin && /content/build/bin/llama-cli -h

/content/build/bin/llama-gguf-split

!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/content/build/bin && /content/build/bin/llama-gguf-split -h

!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/content/build/bin && /content/build/bin/llama-gguf-split --split /content/gpt2-xl-Q2_K.gguf /content/gpt2-xl-Q2_K_split.gguf --split-max-size 500M

!wget https://huggingface.co/tensorblock/gpt2-xl-GGUF/resolve/main/gpt2-xl-Q2_K.gguf

!huggingface-cli login --token b

"""تقسيم"""

!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/content/build/bin && /content/build/bin/llama-gguf-split --split-max-size 500M /content/gpt2-xl-Q2_K.gguf /content/gpt2-xl-Q2_K_split.gguf

/content/gpt2-xl-Q2_K_split.gguf-00001-of-00002.gguf
/content/gpt2-xl-Q2_K_split.gguf-00002-of-00002.gguf

"""دمج"""

!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/content/build/bin && /content/build/bin/llama-gguf-split --merge /content/gpt2-xl-Q2_K_split.gguf-00001-of-00002.gguf /content/gpt2-xl-Q2_K_merged.gguf







#!wget https://huggingface.co/unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF/resolve/main/Q2_K/Qwen3-235B-A22B-Instruct-2507-Q2_K-00001-of-00002.gguf
!wget https://huggingface.co/unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF/resolve/main/Q2_K/Qwen3-235B-A22B-Instruct-2507-Q2_K-00002-of-00002.gguf

!wget https://huggingface.co/unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF/resolve/main/Q2_K/Qwen3-235B-A22B-Instruct-2507-Q2_K-00002-of-00002.gguf

# Install the Hugging Face CLI
pip install -U "huggingface_hub[cli]"

# Login with your Hugging Face credentials
hf auth login

# Push your model files
hf upload rakmik/Qwen3-235B .

from huggingface_hub import HfApi

api = HfApi(token=os.getenv("HF_TOKEN"))
api.upload_folder(
    folder_path="/path/to/local/model",
    repo_id="rakmik/Qwen3-235B",
    repo_type="model",
)

!rm -rf /content/build

!pip install huggingface_hub huggingface-cli

from huggingface_hub import notebook_login

notebook_login()

from huggingface_hub import HfApi

api = HfApi()
username = "YOUR_HUGGINGFACE_USERNAME"  # استبدل باسم المستخدم الخاص بك
repo_name = "my-awesome-model"      # اختر اسمًا لمستودعك

# إنشاء المستودع
api.create_repo(
    repo_id=f"{username}/{repo_name}",
    repo_type="model", # يمكن أن يكون 'model', 'dataset', or 'space'
    exist_ok=True  # إذا كان المستودع موجودًا بالفعل، فلن يظهر خطأ
)

from huggingface_hub import HfApi

api = HfApi()
username = "YOUR_HUGGINGFACE_USERNAME"
repo_name = "my-awesome-model"

# رفع محتويات المجلد بالكامل إلى المستودع
api.upload_folder(
    folder_path="/content/my_model_files", # المسار إلى المجلد الذي يحتوي على ملفاتك
    repo_id=f"{username}/{repo_name}",
    repo_type="model"
)

from huggingface_hub import HfApi

api = HfApi()
username = "YOUR_HUGGINGFACE_USERNAME"
repo_name = "my-awesome-model"

# مثال على رفع ملف واحد
api.upload_file(
    path_or_fileobj="/content/gpt2-xl-Q2_K_merged.gguf", # المسار إلى الملف الذي تريد رفعه
    path_in_repo="gpt2-xl-Q2_K_merged.gguf", # الاسم الذي سيظهر به الملف في المستودع
    repo_id=f"{username}/{repo_name}",
    repo_type="model"
)

from huggingface_hub import HfApi

# تهيئة الواجهة البرمجية
api = HfApi()

# تحديد معلومات المستودع والمجلد المحلي
repo_id = "rakmik/Qwen3-235B"
local_folder_path = "/content/your_model_folder"  # <-- غير هذا المسار إلى مسار المجلد الذي يحتوي على ملفاتك

print(f"Uploading folder {local_folder_path} to {repo_id}...")

# تنفيذ عملية الرفع
# سيتم رفع كل محتويات المجلد المحلي إلى المستودع
api.upload_folder(
    folder_path=local_folder_path,
    repo_id=repo_id,
    repo_type="model"
)

print("Upload complete!")



!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/content/build/bin && /content/build/bin/llama-gguf-split --merge /content/Qwen3-235B-A22B-Instruct-2507-Q2_K-00001-of-00002.gguf /content/qwen.gguf

!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/content/build/bin && /content/build/bin/llama-gguf-split --split-max-size 10G /content/qwen.gguf /content/a/qwen_split.gguf



from huggingface_hub import HfApi

api = HfApi(token=os.getenv("HF_TOKEN"))
api.upload_folder(
    folder_path="/content/a",
    repo_id="rakmik/Qwen3-235B",
    repo_type="model",
)

from huggingface_hub import HfApi

api = HfApi(token=os.getenv(""))
api.upload_folder(
    folder_path="/content/a",
    repo_id="rakmik/Qwen3-235B",
    repo_type="model",
)

from huggingface_hub import HfApi
import os

# تحذير: هذه الطريقة غير آمنة وتكشف التوكن الخاص بك.
# استخدم التوكن الجديد الذي أنشأته بعد حذف القديم.
DANGEROUS_TOKEN = "XXXXXXXXXXXXXX" # <-- ضع التوكن الجديد هنا

api = HfApi(token=DANGEROUS_TOKEN)

# بقية الكود...
api.upload_folder(
    folder_path="/content/a",
    repo_id="rakmik/Qwen3-235B",
    repo_type="model",
)

!/content/build/bin/llama-cli -m your_model.gguf -p "I believe the meaning of life is" -n 128 -no-cnv

!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/content/build/bin && /content/build/bin/llama-cli -m /content/a/qwen_split.gguf-00001-of-00009.gguf -p "I believe the meaning of life is" -n 128 -no-cnv