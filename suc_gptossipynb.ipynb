{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVgwZdFtvRoW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kbwa4m0RvxXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m15MP_zRvxZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-bf16.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VG8NkaSvxce",
        "outputId": "cc3f6eff-5a59-4112-a571-c08a46a11e39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-06 18:57:52--  https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-bf16.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.166.152.105, 3.166.152.44, 3.166.152.65, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.166.152.105|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/689277cb60b4c9edeb2c2696/45a7a45ce7a1266e1e05ad40336b16edc7dceb37f00e0c6be856e6b87ae34cb0?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250806%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250806T185753Z&X-Amz-Expires=3600&X-Amz-Signature=4db0e5e7b88d45fcc4520540323908bcd6dfa876fda3b49ddc3ae05c1d12dec6&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27openai_gpt-oss-20b-bf16.gguf%3B+filename%3D%22openai_gpt-oss-20b-bf16.gguf%22%3B&x-id=GetObject&Expires=1754510273&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NDUxMDI3M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODkyNzdjYjYwYjRjOWVkZWIyYzI2OTYvNDVhN2E0NWNlN2ExMjY2ZTFlMDVhZDQwMzM2YjE2ZWRjN2RjZWIzN2YwMGUwYzZiZTg1NmU2Yjg3YWUzNGNiMCoifV19&Signature=YwgsFuJFURrbFW5wTp%7EFAEt37TJouhFjMxyeniFe46rZH0xHVJRQiQnhnjdVtNgSQnkPl-pLSKxtvmdY4SRaP4iaYQ6opZWtovS8iyyvxLXbf8IZJry-KrjRn-F-J97ghxZgcLUzAYTUDqbw25wVmrJh7Irh4xCPBP9KC4UUAeAT4mBrlXWxXbVTRCRS1DXMCc81Ixjw3KO8VOMzhPC%7E7QJAH7m7eI0QS9hQfwpimCSrzgn1RcsFsBgqFk9qtwRwoEsDucBdtm3l51kIsELGjxinUZwwDNgcRLGMxxuTpiUTMJbdD73-6E15AjdUEpjwWeNtfmUvrYE6z3nmODGF8Q__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-08-06 18:57:53--  https://cas-bridge.xethub.hf.co/xet-bridge-us/689277cb60b4c9edeb2c2696/45a7a45ce7a1266e1e05ad40336b16edc7dceb37f00e0c6be856e6b87ae34cb0?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250806%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250806T185753Z&X-Amz-Expires=3600&X-Amz-Signature=4db0e5e7b88d45fcc4520540323908bcd6dfa876fda3b49ddc3ae05c1d12dec6&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27openai_gpt-oss-20b-bf16.gguf%3B+filename%3D%22openai_gpt-oss-20b-bf16.gguf%22%3B&x-id=GetObject&Expires=1754510273&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NDUxMDI3M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODkyNzdjYjYwYjRjOWVkZWIyYzI2OTYvNDVhN2E0NWNlN2ExMjY2ZTFlMDVhZDQwMzM2YjE2ZWRjN2RjZWIzN2YwMGUwYzZiZTg1NmU2Yjg3YWUzNGNiMCoifV19&Signature=YwgsFuJFURrbFW5wTp%7EFAEt37TJouhFjMxyeniFe46rZH0xHVJRQiQnhnjdVtNgSQnkPl-pLSKxtvmdY4SRaP4iaYQ6opZWtovS8iyyvxLXbf8IZJry-KrjRn-F-J97ghxZgcLUzAYTUDqbw25wVmrJh7Irh4xCPBP9KC4UUAeAT4mBrlXWxXbVTRCRS1DXMCc81Ixjw3KO8VOMzhPC%7E7QJAH7m7eI0QS9hQfwpimCSrzgn1RcsFsBgqFk9qtwRwoEsDucBdtm3l51kIsELGjxinUZwwDNgcRLGMxxuTpiUTMJbdD73-6E15AjdUEpjwWeNtfmUvrYE6z3nmODGF8Q__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.173.166.85, 18.173.166.74, 18.173.166.5, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.173.166.85|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13792637760 (13G)\n",
            "Saving to: ‘openai_gpt-oss-20b-bf16.gguf’\n",
            "\n",
            "openai_gpt-oss-20b- 100%[===================>]  12.84G   234MB/s    in 69s     \n",
            "\n",
            "2025-08-06 18:59:02 (189 MB/s) - ‘openai_gpt-oss-20b-bf16.gguf’ saved [13792637760/13792637760]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/ggml-org/llama.cpp/releases/download/b6096/llama-b6096-bin-win-cpu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5zv-eAtvyn6",
        "outputId": "aee82c11-deaa-44ea-ab63-8425ae0a20a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-06 18:59:03--  https://github.com/ggml-org/llama.cpp/releases/download/b6096/llama-b6096-bin-win-cpu-x64.zip\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/612354784/fc7a84a8-c8e7-4beb-b03e-363d5744ddff?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-08-06T19%3A57%3A00Z&rscd=attachment%3B+filename%3Dllama-b6096-bin-win-cpu-x64.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-08-06T18%3A56%3A10Z&ske=2025-08-06T19%3A57%3A00Z&sks=b&skv=2018-11-09&sig=giK7%2BpukIR1To558GFM%2F2SIkvVV8n0Ugjhi8Pa8cLTM%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1NDUwNzA0MywibmJmIjoxNzU0NTA2NzQzLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.uIM43i0hCsLXcEJ2-PHtoqzdmjcFChR8PT8RgqB4Y9k&response-content-disposition=attachment%3B%20filename%3Dllama-b6096-bin-win-cpu-x64.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-08-06 18:59:03--  https://release-assets.githubusercontent.com/github-production-release-asset/612354784/fc7a84a8-c8e7-4beb-b03e-363d5744ddff?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-08-06T19%3A57%3A00Z&rscd=attachment%3B+filename%3Dllama-b6096-bin-win-cpu-x64.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-08-06T18%3A56%3A10Z&ske=2025-08-06T19%3A57%3A00Z&sks=b&skv=2018-11-09&sig=giK7%2BpukIR1To558GFM%2F2SIkvVV8n0Ugjhi8Pa8cLTM%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1NDUwNzA0MywibmJmIjoxNzU0NTA2NzQzLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.uIM43i0hCsLXcEJ2-PHtoqzdmjcFChR8PT8RgqB4Y9k&response-content-disposition=attachment%3B%20filename%3Dllama-b6096-bin-win-cpu-x64.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14492534 (14M) [application/octet-stream]\n",
            "Saving to: ‘llama-b6096-bin-win-cpu-x64.zip’\n",
            "\n",
            "llama-b6096-bin-win 100%[===================>]  13.82M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-08-06 18:59:03 (110 MB/s) - ‘llama-b6096-bin-win-cpu-x64.zip’ saved [14492534/14492534]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip llama-b6096-bin-win-cpu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQGWKhT_v8dw",
        "outputId": "8582444f-9dbb-492e-d1a3-fb7a2dae4e31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  llama-b6096-bin-win-cpu-x64.zip\n",
            "  inflating: ggml-base.dll           \n",
            "  inflating: ggml-cpu-alderlake.dll  \n",
            "  inflating: ggml-cpu-haswell.dll    \n",
            "  inflating: ggml-cpu-icelake.dll    \n",
            "  inflating: ggml-cpu-sandybridge.dll  \n",
            "  inflating: ggml-cpu-sapphirerapids.dll  \n",
            "  inflating: ggml-cpu-skylakex.dll   \n",
            "  inflating: ggml-cpu-sse42.dll      \n",
            "  inflating: ggml-cpu-x64.dll        \n",
            "  inflating: ggml-rpc.dll            \n",
            "  inflating: ggml.dll                \n",
            "  inflating: libcurl-x64.dll         \n",
            "  inflating: libomp140.x86_64.dll    \n",
            "  inflating: LICENSE-curl            \n",
            "  inflating: LICENSE-httplib         \n",
            "  inflating: LICENSE-jsonhpp         \n",
            "  inflating: LICENSE-linenoise       \n",
            "  inflating: llama-batched-bench.exe  \n",
            "  inflating: llama-bench.exe         \n",
            "  inflating: llama-cli.exe           \n",
            "  inflating: llama-gemma3-cli.exe    \n",
            "  inflating: llama-gguf-split.exe    \n",
            "  inflating: llama-imatrix.exe       \n",
            "  inflating: llama-llava-cli.exe     \n",
            "  inflating: llama-minicpmv-cli.exe  \n",
            "  inflating: llama-mtmd-cli.exe      \n",
            "  inflating: llama-perplexity.exe    \n",
            "  inflating: llama-quantize.exe      \n",
            "  inflating: llama-qwen2vl-cli.exe   \n",
            "  inflating: llama-run.exe           \n",
            "  inflating: llama-server.exe        \n",
            "  inflating: llama-tokenize.exe      \n",
            "  inflating: llama-tts.exe           \n",
            "  inflating: llama.dll               \n",
            "  inflating: mtmd.dll                \n",
            "  inflating: rpc-server.exe          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!llama-cli.exe -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUxmvXb5wH9h",
        "outputId": "2faf0bf6-7175-4f72-d6c2-9006656dea08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: llama-cli.exe: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lW1kDfziwQnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd a\n",
        "!wget https://github.com/ggml-org/llama.cpp/releases/download/b6096/llama-b6096-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Plz9-1gwk53",
        "outputId": "644ebc12-9f78-4c31-e487-079b49bcd947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/a\n",
            "--2025-08-06 19:01:35--  https://github.com/ggml-org/llama.cpp/releases/download/b6096/llama-b6096-bin-ubuntu-x64.zip\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/612354784/2dadad44-8d73-4719-9df8-ce2c03903578?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-08-06T20%3A01%3A24Z&rscd=attachment%3B+filename%3Dllama-b6096-bin-ubuntu-x64.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-08-06T19%3A01%3A18Z&ske=2025-08-06T20%3A01%3A24Z&sks=b&skv=2018-11-09&sig=Q0JhnHVsclNlIuZJ5OsXFdtNRe4%2BBavfkU6K%2Fec%2BeXg%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1NDUwNzE5NSwibmJmIjoxNzU0NTA2ODk1LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.DP9vBWWL9wEA-jax_kSBCt3nXRa_CBCI3Zsr4_dp8Mk&response-content-disposition=attachment%3B%20filename%3Dllama-b6096-bin-ubuntu-x64.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-08-06 19:01:35--  https://release-assets.githubusercontent.com/github-production-release-asset/612354784/2dadad44-8d73-4719-9df8-ce2c03903578?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-08-06T20%3A01%3A24Z&rscd=attachment%3B+filename%3Dllama-b6096-bin-ubuntu-x64.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-08-06T19%3A01%3A18Z&ske=2025-08-06T20%3A01%3A24Z&sks=b&skv=2018-11-09&sig=Q0JhnHVsclNlIuZJ5OsXFdtNRe4%2BBavfkU6K%2Fec%2BeXg%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1NDUwNzE5NSwibmJmIjoxNzU0NTA2ODk1LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.DP9vBWWL9wEA-jax_kSBCt3nXRa_CBCI3Zsr4_dp8Mk&response-content-disposition=attachment%3B%20filename%3Dllama-b6096-bin-ubuntu-x64.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13290951 (13M) [application/octet-stream]\n",
            "Saving to: ‘llama-b6096-bin-ubuntu-x64.zip’\n",
            "\n",
            "llama-b6096-bin-ubu 100%[===================>]  12.67M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-08-06 19:01:35 (115 MB/s) - ‘llama-b6096-bin-ubuntu-x64.zip’ saved [13290951/13290951]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip llama-b6096-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rStApx-iwqRo",
        "outputId": "82a404ad-f84f-47bb-e8ae-6309d8eaedbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  llama-b6096-bin-ubuntu-x64.zip\n",
            "  inflating: build/bin/LICENSE       \n",
            "  inflating: build/bin/LICENSE-curl  \n",
            "  inflating: build/bin/LICENSE-httplib  \n",
            "  inflating: build/bin/LICENSE-jsonhpp  \n",
            "  inflating: build/bin/LICENSE-linenoise  \n",
            "  inflating: build/bin/libggml-base.so  \n",
            "  inflating: build/bin/libggml-cpu-alderlake.so  \n",
            "  inflating: build/bin/libggml-cpu-haswell.so  \n",
            "  inflating: build/bin/libggml-cpu-icelake.so  \n",
            "  inflating: build/bin/libggml-cpu-sandybridge.so  \n",
            "  inflating: build/bin/libggml-cpu-sapphirerapids.so  \n",
            "  inflating: build/bin/libggml-cpu-skylakex.so  \n",
            "  inflating: build/bin/libggml-cpu-sse42.so  \n",
            "  inflating: build/bin/libggml-cpu-x64.so  \n",
            "  inflating: build/bin/libggml-rpc.so  \n",
            "  inflating: build/bin/libggml.so    \n",
            "  inflating: build/bin/libllama.so   \n",
            "  inflating: build/bin/libmtmd.so    \n",
            "  inflating: build/bin/llama-batched-bench  \n",
            "  inflating: build/bin/llama-bench   \n",
            "  inflating: build/bin/llama-cli     \n",
            "  inflating: build/bin/llama-gemma3-cli  \n",
            "  inflating: build/bin/llama-gguf-split  \n",
            "  inflating: build/bin/llama-imatrix  \n",
            "  inflating: build/bin/llama-llava-cli  \n",
            "  inflating: build/bin/llama-minicpmv-cli  \n",
            "  inflating: build/bin/llama-mtmd-cli  \n",
            "  inflating: build/bin/llama-perplexity  \n",
            "  inflating: build/bin/llama-quantize  \n",
            "  inflating: build/bin/llama-qwen2vl-cli  \n",
            "  inflating: build/bin/llama-run     \n",
            "  inflating: build/bin/llama-server  \n",
            "  inflating: build/bin/llama-tokenize  \n",
            "  inflating: build/bin/llama-tts     \n",
            "  inflating: build/bin/rpc-server    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-cli -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS4wlruvwtmI",
        "outputId": "883bb772-187c-4f67-8319-269cce7cbe91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_backend: loaded RPC backend from /content/a/build/bin/libggml-rpc.so\n",
            "load_backend: loaded CPU backend from /content/a/build/bin/libggml-cpu-haswell.so\n",
            "----- common params -----\n",
            "\n",
            "-h,    --help, --usage                  print usage and exit\n",
            "--version                               show version and build info\n",
            "--completion-bash                       print source-able bash completion script for llama.cpp\n",
            "--verbose-prompt                        print a verbose prompt before generation (default: false)\n",
            "-t,    --threads N                      number of threads to use during generation (default: -1)\n",
            "                                        (env: LLAMA_ARG_THREADS)\n",
            "-tb,   --threads-batch N                number of threads to use during batch and prompt processing (default:\n",
            "                                        same as --threads)\n",
            "-C,    --cpu-mask M                     CPU affinity mask: arbitrarily long hex. Complements cpu-range\n",
            "                                        (default: \"\")\n",
            "-Cr,   --cpu-range lo-hi                range of CPUs for affinity. Complements --cpu-mask\n",
            "--cpu-strict <0|1>                      use strict CPU placement (default: 0)\n",
            "--prio N                                set process/thread priority : low(-1), normal(0), medium(1), high(2),\n",
            "                                        realtime(3) (default: 0)\n",
            "--poll <0...100>                        use polling level to wait for work (0 - no polling, default: 50)\n",
            "-Cb,   --cpu-mask-batch M               CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch\n",
            "                                        (default: same as --cpu-mask)\n",
            "-Crb,  --cpu-range-batch lo-hi          ranges of CPUs for affinity. Complements --cpu-mask-batch\n",
            "--cpu-strict-batch <0|1>                use strict CPU placement (default: same as --cpu-strict)\n",
            "--prio-batch N                          set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
            "                                        (default: 0)\n",
            "--poll-batch <0|1>                      use polling to wait for work (default: same as --poll)\n",
            "-c,    --ctx-size N                     size of the prompt context (default: 4096, 0 = loaded from model)\n",
            "                                        (env: LLAMA_ARG_CTX_SIZE)\n",
            "-n,    --predict, --n-predict N         number of tokens to predict (default: -1, -1 = infinity, -2 = until\n",
            "                                        context filled)\n",
            "                                        (env: LLAMA_ARG_N_PREDICT)\n",
            "-b,    --batch-size N                   logical maximum batch size (default: 2048)\n",
            "                                        (env: LLAMA_ARG_BATCH)\n",
            "-ub,   --ubatch-size N                  physical maximum batch size (default: 512)\n",
            "                                        (env: LLAMA_ARG_UBATCH)\n",
            "--keep N                                number of tokens to keep from the initial prompt (default: 0, -1 =\n",
            "                                        all)\n",
            "--swa-full                              use full-size SWA cache (default: false)\n",
            "                                        [(more\n",
            "                                        info)](https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n",
            "                                        (env: LLAMA_ARG_SWA_FULL)\n",
            "--kv-unified, -kvu                      use single unified KV buffer for the KV cache of all sequences\n",
            "                                        (default: false)\n",
            "                                        [(more info)](https://github.com/ggml-org/llama.cpp/pull/14363)\n",
            "                                        (env: LLAMA_ARG_KV_SPLIT)\n",
            "-fa,   --flash-attn                     enable Flash Attention (default: disabled)\n",
            "                                        (env: LLAMA_ARG_FLASH_ATTN)\n",
            "-p,    --prompt PROMPT                  prompt to start generation with; for system message, use -sys\n",
            "--no-perf                               disable internal libllama performance timings (default: false)\n",
            "                                        (env: LLAMA_ARG_NO_PERF)\n",
            "-f,    --file FNAME                     a file containing the prompt (default: none)\n",
            "-bf,   --binary-file FNAME              binary file containing the prompt (default: none)\n",
            "-e,    --escape                         process escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\) (default: true)\n",
            "--no-escape                             do not process escape sequences\n",
            "--rope-scaling {none,linear,yarn}       RoPE frequency scaling method, defaults to linear unless specified by\n",
            "                                        the model\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALING_TYPE)\n",
            "--rope-scale N                          RoPE context scaling factor, expands context by a factor of N\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALE)\n",
            "--rope-freq-base N                      RoPE base frequency, used by NTK-aware scaling (default: loaded from\n",
            "                                        model)\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_BASE)\n",
            "--rope-freq-scale N                     RoPE frequency scaling factor, expands context by a factor of 1/N\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_SCALE)\n",
            "--yarn-orig-ctx N                       YaRN: original context size of model (default: 0 = model training\n",
            "                                        context size)\n",
            "                                        (env: LLAMA_ARG_YARN_ORIG_CTX)\n",
            "--yarn-ext-factor N                     YaRN: extrapolation mix factor (default: -1.0, 0.0 = full\n",
            "                                        interpolation)\n",
            "                                        (env: LLAMA_ARG_YARN_EXT_FACTOR)\n",
            "--yarn-attn-factor N                    YaRN: scale sqrt(t) or attention magnitude (default: 1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_ATTN_FACTOR)\n",
            "--yarn-beta-slow N                      YaRN: high correction dim or alpha (default: 1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_SLOW)\n",
            "--yarn-beta-fast N                      YaRN: low correction dim or beta (default: 32.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_FAST)\n",
            "-nkvo, --no-kv-offload                  disable KV offload\n",
            "                                        (env: LLAMA_ARG_NO_KV_OFFLOAD)\n",
            "-nr,   --no-repack                      disable weight repacking\n",
            "                                        (env: LLAMA_ARG_NO_REPACK)\n",
            "-ctk,  --cache-type-k TYPE              KV cache data type for K\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_K)\n",
            "-ctv,  --cache-type-v TYPE              KV cache data type for V\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_V)\n",
            "-dt,   --defrag-thold N                 KV cache defragmentation threshold (default: 0.1, < 0 - disabled)\n",
            "                                        (env: LLAMA_ARG_DEFRAG_THOLD)\n",
            "-np,   --parallel N                     number of parallel sequences to decode (default: 1)\n",
            "                                        (env: LLAMA_ARG_N_PARALLEL)\n",
            "--rpc SERVERS                           comma separated list of RPC servers\n",
            "                                        (env: LLAMA_ARG_RPC)\n",
            "--mlock                                 force system to keep model in RAM rather than swapping or compressing\n",
            "                                        (env: LLAMA_ARG_MLOCK)\n",
            "--no-mmap                               do not memory-map model (slower load but may reduce pageouts if not\n",
            "                                        using mlock)\n",
            "                                        (env: LLAMA_ARG_NO_MMAP)\n",
            "--numa TYPE                             attempt optimizations that help on some NUMA systems\n",
            "                                        - distribute: spread execution evenly over all nodes\n",
            "                                        - isolate: only spawn threads on CPUs on the node that execution\n",
            "                                        started on\n",
            "                                        - numactl: use the CPU map provided by numactl\n",
            "                                        if run without this previously, it is recommended to drop the system\n",
            "                                        page cache before using this\n",
            "                                        see https://github.com/ggml-org/llama.cpp/issues/1437\n",
            "                                        (env: LLAMA_ARG_NUMA)\n",
            "-dev,  --device <dev1,dev2,..>          comma-separated list of devices to use for offloading (none = don't\n",
            "                                        offload)\n",
            "                                        use --list-devices to see a list of available devices\n",
            "                                        (env: LLAMA_ARG_DEVICE)\n",
            "--list-devices                          print list of available devices and exit\n",
            "--override-tensor, -ot <tensor name pattern>=<buffer type>,...\n",
            "                                        override tensor buffer type\n",
            "--cpu-moe, -cmoe                        keep all Mixture of Experts (MoE) weights in the CPU\n",
            "                                        (env: LLAMA_ARG_CPU_MOE)\n",
            "--n-cpu-moe, -ncmoe N                   keep the Mixture of Experts (MoE) weights of the first N layers in the\n",
            "                                        CPU\n",
            "                                        (env: LLAMA_ARG_N_CPU_MOE)\n",
            "-ngl,  --gpu-layers, --n-gpu-layers N   number of layers to store in VRAM\n",
            "                                        (env: LLAMA_ARG_N_GPU_LAYERS)\n",
            "-sm,   --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:\n",
            "                                        - none: use one GPU only\n",
            "                                        - layer (default): split layers and KV across GPUs\n",
            "                                        - row: split rows across GPUs\n",
            "                                        (env: LLAMA_ARG_SPLIT_MODE)\n",
            "-ts,   --tensor-split N0,N1,N2,...      fraction of the model to offload to each GPU, comma-separated list of\n",
            "                                        proportions, e.g. 3,1\n",
            "                                        (env: LLAMA_ARG_TENSOR_SPLIT)\n",
            "-mg,   --main-gpu INDEX                 the GPU to use for the model (with split-mode = none), or for\n",
            "                                        intermediate results and KV (with split-mode = row) (default: 0)\n",
            "                                        (env: LLAMA_ARG_MAIN_GPU)\n",
            "--check-tensors                         check model tensor data for invalid values (default: false)\n",
            "--override-kv KEY=TYPE:VALUE            advanced option to override model metadata by key. may be specified\n",
            "                                        multiple times.\n",
            "                                        types: int, float, bool, str. example: --override-kv\n",
            "                                        tokenizer.ggml.add_bos_token=bool:false\n",
            "--no-op-offload                         disable offloading host tensor operations to device (default: false)\n",
            "--lora FNAME                            path to LoRA adapter (can be repeated to use multiple adapters)\n",
            "--lora-scaled FNAME SCALE               path to LoRA adapter with user defined scaling (can be repeated to use\n",
            "                                        multiple adapters)\n",
            "--control-vector FNAME                  add a control vector\n",
            "                                        note: this argument can be repeated to add multiple control vectors\n",
            "--control-vector-scaled FNAME SCALE     add a control vector with user defined scaling SCALE\n",
            "                                        note: this argument can be repeated to add multiple scaled control\n",
            "                                        vectors\n",
            "--control-vector-layer-range START END\n",
            "                                        layer range to apply the control vector(s) to, start and end inclusive\n",
            "-m,    --model FNAME                    model path (default: `models/$filename` with filename from `--hf-file`\n",
            "                                        or `--model-url` if set, otherwise models/7B/ggml-model-f16.gguf)\n",
            "                                        (env: LLAMA_ARG_MODEL)\n",
            "-mu,   --model-url MODEL_URL            model download url (default: unused)\n",
            "                                        (env: LLAMA_ARG_MODEL_URL)\n",
            "-hf,   -hfr, --hf-repo <user>/<model>[:quant]\n",
            "                                        Hugging Face model repository; quant is optional, case-insensitive,\n",
            "                                        default to Q4_K_M, or falls back to the first file in the repo if\n",
            "                                        Q4_K_M doesn't exist.\n",
            "                                        mmproj is also downloaded automatically if available. to disable, add\n",
            "                                        --no-mmproj\n",
            "                                        example: unsloth/phi-4-GGUF:q4_k_m\n",
            "                                        (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_REPO)\n",
            "-hfd,  -hfrd, --hf-repo-draft <user>/<model>[:quant]\n",
            "                                        Same as --hf-repo, but for the draft model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HFD_REPO)\n",
            "-hff,  --hf-file FILE                   Hugging Face model file. If specified, it will override the quant in\n",
            "                                        --hf-repo (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_FILE)\n",
            "-hfv,  -hfrv, --hf-repo-v <user>/<model>[:quant]\n",
            "                                        Hugging Face model repository for the vocoder model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_REPO_V)\n",
            "-hffv, --hf-file-v FILE                 Hugging Face model file for the vocoder model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_FILE_V)\n",
            "-hft,  --hf-token TOKEN                 Hugging Face access token (default: value from HF_TOKEN environment\n",
            "                                        variable)\n",
            "                                        (env: HF_TOKEN)\n",
            "--log-disable                           Log disable\n",
            "--log-file FNAME                        Log to file\n",
            "--log-colors                            Enable colored logging\n",
            "                                        (env: LLAMA_LOG_COLORS)\n",
            "-v,    --verbose, --log-verbose         Set verbosity level to infinity (i.e. log all messages, useful for\n",
            "                                        debugging)\n",
            "--offline                               Offline mode: forces use of cache, prevents network access\n",
            "                                        (env: LLAMA_OFFLINE)\n",
            "-lv,   --verbosity, --log-verbosity N   Set the verbosity threshold. Messages with a higher verbosity will be\n",
            "                                        ignored.\n",
            "                                        (env: LLAMA_LOG_VERBOSITY)\n",
            "--log-prefix                            Enable prefix in log messages\n",
            "                                        (env: LLAMA_LOG_PREFIX)\n",
            "--log-timestamps                        Enable timestamps in log messages\n",
            "                                        (env: LLAMA_LOG_TIMESTAMPS)\n",
            "-ctkd, --cache-type-k-draft TYPE        KV cache data type for K for the draft model\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_K_DRAFT)\n",
            "-ctvd, --cache-type-v-draft TYPE        KV cache data type for V for the draft model\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_V_DRAFT)\n",
            "\n",
            "\n",
            "----- sampling params -----\n",
            "\n",
            "--samplers SAMPLERS                     samplers that will be used for generation in the order, separated by\n",
            "                                        ';'\n",
            "                                        (default:\n",
            "                                        penalties;dry;top_n_sigma;top_k;typ_p;top_p;min_p;xtc;temperature)\n",
            "-s,    --seed SEED                      RNG seed (default: -1, use random seed for -1)\n",
            "--sampling-seq, --sampler-seq SEQUENCE\n",
            "                                        simplified sequence for samplers that will be used (default:\n",
            "                                        edskypmxt)\n",
            "--ignore-eos                            ignore end of stream token and continue generating (implies\n",
            "                                        --logit-bias EOS-inf)\n",
            "--temp N                                temperature (default: 0.8)\n",
            "--top-k N                               top-k sampling (default: 40, 0 = disabled)\n",
            "--top-p N                               top-p sampling (default: 0.9, 1.0 = disabled)\n",
            "--min-p N                               min-p sampling (default: 0.1, 0.0 = disabled)\n",
            "--top-nsigma N                          top-n-sigma sampling (default: -1.0, -1.0 = disabled)\n",
            "--xtc-probability N                     xtc probability (default: 0.0, 0.0 = disabled)\n",
            "--xtc-threshold N                       xtc threshold (default: 0.1, 1.0 = disabled)\n",
            "--typical N                             locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)\n",
            "--repeat-last-n N                       last n tokens to consider for penalize (default: 64, 0 = disabled, -1\n",
            "                                        = ctx_size)\n",
            "--repeat-penalty N                      penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)\n",
            "--presence-penalty N                    repeat alpha presence penalty (default: 0.0, 0.0 = disabled)\n",
            "--frequency-penalty N                   repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)\n",
            "--dry-multiplier N                      set DRY sampling multiplier (default: 0.0, 0.0 = disabled)\n",
            "--dry-base N                            set DRY sampling base value (default: 1.75)\n",
            "--dry-allowed-length N                  set allowed length for DRY sampling (default: 2)\n",
            "--dry-penalty-last-n N                  set DRY penalty for the last n tokens (default: -1, 0 = disable, -1 =\n",
            "                                        context size)\n",
            "--dry-sequence-breaker STRING           add sequence breaker for DRY sampling, clearing out default breakers\n",
            "                                        ('\\n', ':', '\"', '*') in the process; use \"none\" to not use any\n",
            "                                        sequence breakers\n",
            "--dynatemp-range N                      dynamic temperature range (default: 0.0, 0.0 = disabled)\n",
            "--dynatemp-exp N                        dynamic temperature exponent (default: 1.0)\n",
            "--mirostat N                            use Mirostat sampling.\n",
            "                                        Top K, Nucleus and Locally Typical samplers are ignored if used.\n",
            "                                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n",
            "--mirostat-lr N                         Mirostat learning rate, parameter eta (default: 0.1)\n",
            "--mirostat-ent N                        Mirostat target entropy, parameter tau (default: 5.0)\n",
            "-l,    --logit-bias TOKEN_ID(+/-)BIAS   modifies the likelihood of token appearing in the completion,\n",
            "                                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',\n",
            "                                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'\n",
            "--grammar GRAMMAR                       BNF-like grammar to constrain generations (see samples in grammars/\n",
            "                                        dir) (default: '')\n",
            "--grammar-file FNAME                    file to read grammar from\n",
            "-j,    --json-schema SCHEMA             JSON schema to constrain generations (https://json-schema.org/), e.g.\n",
            "                                        `{}` for any JSON object\n",
            "                                        For schemas w/ external $refs, use --grammar +\n",
            "                                        example/json_schema_to_grammar.py instead\n",
            "-jf,   --json-schema-file FILE          File containing a JSON schema to constrain generations\n",
            "                                        (https://json-schema.org/), e.g. `{}` for any JSON object\n",
            "                                        For schemas w/ external $refs, use --grammar +\n",
            "                                        example/json_schema_to_grammar.py instead\n",
            "\n",
            "\n",
            "----- example-specific params -----\n",
            "\n",
            "--no-display-prompt                     don't print prompt at generation (default: false)\n",
            "-co,   --color                          colorise output to distinguish prompt and user input from generations\n",
            "                                        (default: false)\n",
            "--no-context-shift                      disables context shift on infinite text generation (default: disabled)\n",
            "                                        (env: LLAMA_ARG_NO_CONTEXT_SHIFT)\n",
            "-sys,  --system-prompt PROMPT           system prompt to use with model (if applicable, depending on chat\n",
            "                                        template)\n",
            "-sysf, --system-prompt-file FNAME       a file containing the system prompt (default: none)\n",
            "-ptc,  --print-token-count N            print token count every N tokens (default: -1)\n",
            "--prompt-cache FNAME                    file to cache prompt state for faster startup (default: none)\n",
            "--prompt-cache-all                      if specified, saves user input and generations to cache as well\n",
            "--prompt-cache-ro                       if specified, uses the prompt cache but does not update it\n",
            "-r,    --reverse-prompt PROMPT          halt generation at PROMPT, return control in interactive mode\n",
            "-sp,   --special                        special tokens output enabled (default: false)\n",
            "-cnv,  --conversation                   run in conversation mode:\n",
            "                                        - does not print special tokens and suffix/prefix\n",
            "                                        - interactive mode is also enabled\n",
            "                                        (default: auto enabled if chat template is available)\n",
            "-no-cnv, --no-conversation              force disable conversation mode (default: false)\n",
            "-st,   --single-turn                    run conversation for a single turn only, then exit when done\n",
            "                                        will not be interactive if first turn is predefined with --prompt\n",
            "                                        (default: false)\n",
            "-i,    --interactive                    run in interactive mode (default: false)\n",
            "-if,   --interactive-first              run in interactive mode and wait for input right away (default: false)\n",
            "-mli,  --multiline-input                allows you to write or paste multiple lines without ending each in '\\'\n",
            "--in-prefix-bos                         prefix BOS to user inputs, preceding the `--in-prefix` string\n",
            "--in-prefix STRING                      string to prefix user inputs with (default: empty)\n",
            "--in-suffix STRING                      string to suffix after user inputs with (default: empty)\n",
            "--no-warmup                             skip warming up the model with an empty run\n",
            "-gan,  --grp-attn-n N                   group-attention factor (default: 1)\n",
            "                                        (env: LLAMA_ARG_GRP_ATTN_N)\n",
            "-gaw,  --grp-attn-w N                   group-attention width (default: 512)\n",
            "                                        (env: LLAMA_ARG_GRP_ATTN_W)\n",
            "--jinja                                 use jinja template for chat (default: disabled)\n",
            "                                        (env: LLAMA_ARG_JINJA)\n",
            "--reasoning-format FORMAT               controls whether thought tags are allowed and/or extracted from the\n",
            "                                        response, and in which format they're returned; one of:\n",
            "                                        - none: leaves thoughts unparsed in `message.content`\n",
            "                                        - deepseek: puts thoughts in `message.reasoning_content` (except in\n",
            "                                        streaming mode, which behaves as `none`)\n",
            "                                        (default: auto)\n",
            "                                        (env: LLAMA_ARG_THINK)\n",
            "--reasoning-budget N                    controls the amount of thinking allowed; currently only one of: -1 for\n",
            "                                        unrestricted thinking budget, or 0 to disable thinking (default: -1)\n",
            "                                        (env: LLAMA_ARG_THINK_BUDGET)\n",
            "--chat-template JINJA_TEMPLATE          set custom jinja chat template (default: template taken from model's\n",
            "                                        metadata)\n",
            "                                        if suffix/prefix are specified, template will be disabled\n",
            "                                        only commonly used templates are accepted (unless --jinja is set\n",
            "                                        before this flag):\n",
            "                                        list of built-in templates:\n",
            "                                        bailing, chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2,\n",
            "                                        deepseek3, exaone3, exaone4, falcon3, gemma, gigachat, glmedge,\n",
            "                                        gpt-oss, granite, hunyuan-dense, hunyuan-moe, kimi-k2, llama2,\n",
            "                                        llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, llama4, megrez,\n",
            "                                        minicpm, mistral-v1, mistral-v3, mistral-v3-tekken, mistral-v7,\n",
            "                                        mistral-v7-tekken, monarch, openchat, orion, phi3, phi4, rwkv-world,\n",
            "                                        smolvlm, vicuna, vicuna-orca, yandex, zephyr\n",
            "                                        (env: LLAMA_ARG_CHAT_TEMPLATE)\n",
            "--chat-template-file JINJA_TEMPLATE_FILE\n",
            "                                        set custom jinja chat template file (default: template taken from\n",
            "                                        model's metadata)\n",
            "                                        if suffix/prefix are specified, template will be disabled\n",
            "                                        only commonly used templates are accepted (unless --jinja is set\n",
            "                                        before this flag):\n",
            "                                        list of built-in templates:\n",
            "                                        bailing, chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2,\n",
            "                                        deepseek3, exaone3, exaone4, falcon3, gemma, gigachat, glmedge,\n",
            "                                        gpt-oss, granite, hunyuan-dense, hunyuan-moe, kimi-k2, llama2,\n",
            "                                        llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, llama4, megrez,\n",
            "                                        minicpm, mistral-v1, mistral-v3, mistral-v3-tekken, mistral-v7,\n",
            "                                        mistral-v7-tekken, monarch, openchat, orion, phi3, phi4, rwkv-world,\n",
            "                                        smolvlm, vicuna, vicuna-orca, yandex, zephyr\n",
            "                                        (env: LLAMA_ARG_CHAT_TEMPLATE_FILE)\n",
            "--simple-io                             use basic IO for better compatibility in subprocesses and limited\n",
            "                                        consoles\n",
            "\n",
            "example usage:\n",
            "\n",
            "  text generation:     build/bin/llama-cli -m your_model.gguf -p \"I believe the meaning of life is\" -n 128 -no-cnv\n",
            "\n",
            "  chat (conversation): build/bin/llama-cli -m your_model.gguf -sys \"You are a helpful assistant\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-cli -m /content/openai_gpt-oss-20b-bf16.gguf -p \"I believe the meaning of life is\" -n 32 -no-cnv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhGfPuP7wzHf",
        "outputId": "a9b72ba4-d3ca-44aa-9710-f807d5df5c4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_backend: loaded RPC backend from /content/a/build/bin/libggml-rpc.so\n",
            "load_backend: loaded CPU backend from /content/a/build/bin/libggml-cpu-haswell.so\n",
            "build: 6096 (fd1234cb) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 459 tensors from /content/openai_gpt-oss-20b-bf16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Gpt Oss 20b\n",
            "llama_model_loader: - kv   3:                           general.basename str              = gpt-oss\n",
            "llama_model_loader: - kv   4:                         general.size_label str              = 20B\n",
            "llama_model_loader: - kv   5:                            general.license str              = apache-2.0\n",
            "llama_model_loader: - kv   6:                               general.tags arr[str,2]       = [\"vllm\", \"text-generation\"]\n",
            "llama_model_loader: - kv   7:                        gpt-oss.block_count u32              = 24\n",
            "llama_model_loader: - kv   8:                     gpt-oss.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                   gpt-oss.embedding_length u32              = 2880\n",
            "llama_model_loader: - kv  10:                gpt-oss.feed_forward_length u32              = 2880\n",
            "llama_model_loader: - kv  11:               gpt-oss.attention.head_count u32              = 64\n",
            "llama_model_loader: - kv  12:            gpt-oss.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                     gpt-oss.rope.freq_base f32              = 150000.000000\n",
            "llama_model_loader: - kv  14:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                       gpt-oss.expert_count u32              = 32\n",
            "llama_model_loader: - kv  16:                  gpt-oss.expert_used_count u32              = 4\n",
            "llama_model_loader: - kv  17:               gpt-oss.attention.key_length u32              = 64\n",
            "llama_model_loader: - kv  18:             gpt-oss.attention.value_length u32              = 64\n",
            "llama_model_loader: - kv  19:                          general.file_type u32              = 32\n",
            "llama_model_loader: - kv  20:           gpt-oss.attention.sliding_window u32              = 128\n",
            "llama_model_loader: - kv  21:         gpt-oss.expert_feed_forward_length u32              = 2880\n",
            "llama_model_loader: - kv  22:                  gpt-oss.rope.scaling.type str              = yarn\n",
            "llama_model_loader: - kv  23:                gpt-oss.rope.scaling.factor f32              = 32.000000\n",
            "llama_model_loader: - kv  24: gpt-oss.rope.scaling.original_context_length u32              = 4096\n",
            "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = gpt-4o\n",
            "llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,201088]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,446189]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 199998\n",
            "llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 200002\n",
            "llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 199999\n",
            "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {#-\\n  In addition to the normal input...\n",
            "llama_model_loader: - type  f32:  289 tensors\n",
            "llama_model_loader: - type bf16:   98 tensors\n",
            "llama_model_loader: - type mxfp4:   72 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = BF16\n",
            "print_info: file size   = 12.83 GiB (5.27 BPW) \n",
            "load: printing all EOG tokens:\n",
            "load:   - 199999 ('<|endoftext|>')\n",
            "load:   - 200002 ('<|return|>')\n",
            "load:   - 200007 ('<|end|>')\n",
            "load:   - 200012 ('<|call|>')\n",
            "load: special_eog_ids contains both '<|return|>' and '<|call|>' tokens, removing '<|end|>' token from EOG list\n",
            "load: special tokens cache size = 21\n",
            "load: token to piece cache size = 1.3332 MB\n",
            "print_info: arch             = gpt-oss\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 131072\n",
            "print_info: n_embd           = 2880\n",
            "print_info: n_layer          = 24\n",
            "print_info: n_head           = 64\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 64\n",
            "print_info: n_swa            = 128\n",
            "print_info: is_swa_any       = 1\n",
            "print_info: n_embd_head_k    = 64\n",
            "print_info: n_embd_head_v    = 64\n",
            "print_info: n_gqa            = 8\n",
            "print_info: n_embd_k_gqa     = 512\n",
            "print_info: n_embd_v_gqa     = 512\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 2880\n",
            "print_info: n_expert         = 32\n",
            "print_info: n_expert_used    = 4\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = yarn\n",
            "print_info: freq_base_train  = 150000.0\n",
            "print_info: freq_scale_train = 0.03125\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = ?B\n",
            "print_info: model params     = 20.91 B\n",
            "print_info: general.name     = Gpt Oss 20b\n",
            "print_info: n_ff_exp         = 2880\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 201088\n",
            "print_info: n_merges         = 446189\n",
            "print_info: BOS token        = 199998 '<|startoftext|>'\n",
            "print_info: EOS token        = 200002 '<|return|>'\n",
            "print_info: EOT token        = 199999 '<|endoftext|>'\n",
            "print_info: PAD token        = 199999 '<|endoftext|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: EOG token        = 199999 '<|endoftext|>'\n",
            "print_info: EOG token        = 200002 '<|return|>'\n",
            "print_info: EOG token        = 200012 '<|call|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: offloading 0 repeating layers to GPU\n",
            "load_tensors: offloaded 0/25 layers to GPU\n",
            "load_tensors:   CPU_Mapped model buffer size = 13141.28 MiB\n",
            "....................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 2048\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 150000.0\n",
            "llama_context: freq_scale    = 0.03125\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context:        CPU  output buffer size =     0.77 MiB\n",
            "llama_kv_cache_unified_iswa: creating non-SWA KV cache, size = 4096 cells\n",
            "llama_kv_cache_unified:        CPU KV buffer size =    96.00 MiB\n",
            "llama_kv_cache_unified: size =   96.00 MiB (  4096 cells,  12 layers,  1/1 seqs), K (f16):   48.00 MiB, V (f16):   48.00 MiB\n",
            "llama_kv_cache_unified_iswa: creating     SWA KV cache, size = 640 cells\n",
            "llama_kv_cache_unified:        CPU KV buffer size =    15.00 MiB\n",
            "llama_kv_cache_unified: size =   15.00 MiB (   640 cells,  12 layers,  1/1 seqs), K (f16):    7.50 MiB, V (f16):    7.50 MiB\n",
            "llama_context:        CPU compute buffer size =   552.51 MiB\n",
            "llama_context: graph nodes  = 1446\n",
            "llama_context: graph splits = 1\n",
            "common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting\n",
            "common_init_from_params: added <|endoftext|> logit bias = -inf\n",
            "common_init_from_params: added <|return|> logit bias = -inf\n",
            "common_init_from_params: added <|call|> logit bias = -inf\n",
            "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: llama threadpool init, n_threads = 1\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "\n",
            "sampler seed: 1985466929\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n",
            "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
            "generate: n_ctx = 4096, n_batch = 2048, n_predict = 32, n_keep = 0\n",
            "\n",
            "I believe the meaning of life is that we should pursue a life of virtue. The ancient Greek concept of eudaimonia was something that was in the Aristotelian sense. It was the\n",
            "\n",
            "llama_perf_sampler_print:    sampling time =      13.34 ms /    39 runs   (    0.34 ms per token,  2923.10 tokens per second)\n",
            "llama_perf_context_print:        load time =  124642.79 ms\n",
            "llama_perf_context_print: prompt eval time =   14281.36 ms /     7 tokens ( 2040.19 ms per token,     0.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =   54420.65 ms /    31 runs   ( 1755.50 ms per token,     0.57 tokens per second)\n",
            "llama_perf_context_print:       total time =   69058.48 ms /    38 tokens\n",
            "llama_perf_context_print:    graphs reused =         29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-cli -m /content/openai_gpt-oss-20b-bf16.gguf -p \"Write a Python code to print the number 8 twenty-five times.\" -n 64 -no-cnv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgmFLyVbxzKI",
        "outputId": "76dc1339-3e25-4f3c-ba0c-8992a9b89e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_backend: loaded RPC backend from /content/a/build/bin/libggml-rpc.so\n",
            "load_backend: loaded CPU backend from /content/a/build/bin/libggml-cpu-haswell.so\n",
            "build: 6096 (fd1234cb) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 459 tensors from /content/openai_gpt-oss-20b-bf16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Gpt Oss 20b\n",
            "llama_model_loader: - kv   3:                           general.basename str              = gpt-oss\n",
            "llama_model_loader: - kv   4:                         general.size_label str              = 20B\n",
            "llama_model_loader: - kv   5:                            general.license str              = apache-2.0\n",
            "llama_model_loader: - kv   6:                               general.tags arr[str,2]       = [\"vllm\", \"text-generation\"]\n",
            "llama_model_loader: - kv   7:                        gpt-oss.block_count u32              = 24\n",
            "llama_model_loader: - kv   8:                     gpt-oss.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                   gpt-oss.embedding_length u32              = 2880\n",
            "llama_model_loader: - kv  10:                gpt-oss.feed_forward_length u32              = 2880\n",
            "llama_model_loader: - kv  11:               gpt-oss.attention.head_count u32              = 64\n",
            "llama_model_loader: - kv  12:            gpt-oss.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                     gpt-oss.rope.freq_base f32              = 150000.000000\n",
            "llama_model_loader: - kv  14:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                       gpt-oss.expert_count u32              = 32\n",
            "llama_model_loader: - kv  16:                  gpt-oss.expert_used_count u32              = 4\n",
            "llama_model_loader: - kv  17:               gpt-oss.attention.key_length u32              = 64\n",
            "llama_model_loader: - kv  18:             gpt-oss.attention.value_length u32              = 64\n",
            "llama_model_loader: - kv  19:                          general.file_type u32              = 32\n",
            "llama_model_loader: - kv  20:           gpt-oss.attention.sliding_window u32              = 128\n",
            "llama_model_loader: - kv  21:         gpt-oss.expert_feed_forward_length u32              = 2880\n",
            "llama_model_loader: - kv  22:                  gpt-oss.rope.scaling.type str              = yarn\n",
            "llama_model_loader: - kv  23:                gpt-oss.rope.scaling.factor f32              = 32.000000\n",
            "llama_model_loader: - kv  24: gpt-oss.rope.scaling.original_context_length u32              = 4096\n",
            "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = gpt-4o\n",
            "llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,201088]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,446189]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 199998\n",
            "llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 200002\n",
            "llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 199999\n",
            "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {#-\\n  In addition to the normal input...\n",
            "llama_model_loader: - type  f32:  289 tensors\n",
            "llama_model_loader: - type bf16:   98 tensors\n",
            "llama_model_loader: - type mxfp4:   72 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = BF16\n",
            "print_info: file size   = 12.83 GiB (5.27 BPW) \n",
            "load: printing all EOG tokens:\n",
            "load:   - 199999 ('<|endoftext|>')\n",
            "load:   - 200002 ('<|return|>')\n",
            "load:   - 200007 ('<|end|>')\n",
            "load:   - 200012 ('<|call|>')\n",
            "load: special_eog_ids contains both '<|return|>' and '<|call|>' tokens, removing '<|end|>' token from EOG list\n",
            "load: special tokens cache size = 21\n",
            "load: token to piece cache size = 1.3332 MB\n",
            "print_info: arch             = gpt-oss\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 131072\n",
            "print_info: n_embd           = 2880\n",
            "print_info: n_layer          = 24\n",
            "print_info: n_head           = 64\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 64\n",
            "print_info: n_swa            = 128\n",
            "print_info: is_swa_any       = 1\n",
            "print_info: n_embd_head_k    = 64\n",
            "print_info: n_embd_head_v    = 64\n",
            "print_info: n_gqa            = 8\n",
            "print_info: n_embd_k_gqa     = 512\n",
            "print_info: n_embd_v_gqa     = 512\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 2880\n",
            "print_info: n_expert         = 32\n",
            "print_info: n_expert_used    = 4\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = yarn\n",
            "print_info: freq_base_train  = 150000.0\n",
            "print_info: freq_scale_train = 0.03125\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = ?B\n",
            "print_info: model params     = 20.91 B\n",
            "print_info: general.name     = Gpt Oss 20b\n",
            "print_info: n_ff_exp         = 2880\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 201088\n",
            "print_info: n_merges         = 446189\n",
            "print_info: BOS token        = 199998 '<|startoftext|>'\n",
            "print_info: EOS token        = 200002 '<|return|>'\n",
            "print_info: EOT token        = 199999 '<|endoftext|>'\n",
            "print_info: PAD token        = 199999 '<|endoftext|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: EOG token        = 199999 '<|endoftext|>'\n",
            "print_info: EOG token        = 200002 '<|return|>'\n",
            "print_info: EOG token        = 200012 '<|call|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: offloading 0 repeating layers to GPU\n",
            "load_tensors: offloaded 0/25 layers to GPU\n",
            "load_tensors:   CPU_Mapped model buffer size = 13141.28 MiB\n",
            "....................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 2048\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 150000.0\n",
            "llama_context: freq_scale    = 0.03125\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context:        CPU  output buffer size =     0.77 MiB\n",
            "llama_kv_cache_unified_iswa: creating non-SWA KV cache, size = 4096 cells\n",
            "llama_kv_cache_unified:        CPU KV buffer size =    96.00 MiB\n",
            "llama_kv_cache_unified: size =   96.00 MiB (  4096 cells,  12 layers,  1/1 seqs), K (f16):   48.00 MiB, V (f16):   48.00 MiB\n",
            "llama_kv_cache_unified_iswa: creating     SWA KV cache, size = 640 cells\n",
            "llama_kv_cache_unified:        CPU KV buffer size =    15.00 MiB\n",
            "llama_kv_cache_unified: size =   15.00 MiB (   640 cells,  12 layers,  1/1 seqs), K (f16):    7.50 MiB, V (f16):    7.50 MiB\n",
            "llama_context:        CPU compute buffer size =   552.51 MiB\n",
            "llama_context: graph nodes  = 1446\n",
            "llama_context: graph splits = 1\n",
            "common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting\n",
            "common_init_from_params: added <|endoftext|> logit bias = -inf\n",
            "common_init_from_params: added <|return|> logit bias = -inf\n",
            "common_init_from_params: added <|call|> logit bias = -inf\n",
            "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: llama threadpool init, n_threads = 1\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "\n",
            "sampler seed: 1456828453\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n",
            "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
            "generate: n_ctx = 4096, n_batch = 2048, n_predict = 64, n_keep = 0\n",
            "\n",
            "Write a Python code to print the number 8 twenty-five times. The code should be compatible with both Python 2.x and 3.x.\n",
            "\n",
            "Essentially, we want to produce a code that prints the number 8 twenty-five times. We could do something like:\n",
            "\n",
            "```\n",
            "for _ in range(25):\n",
            "    print 8\n",
            "```\n",
            "\n",
            "But that prints each on new line\n",
            "\n",
            "llama_perf_sampler_print:    sampling time =      34.02 ms /    78 runs   (    0.44 ms per token,  2292.77 tokens per second)\n",
            "llama_perf_context_print:        load time =  128984.07 ms\n",
            "llama_perf_context_print: prompt eval time =   38900.34 ms /    14 tokens ( 2778.60 ms per token,     0.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =  109299.89 ms /    63 runs   ( 1734.92 ms per token,     0.58 tokens per second)\n",
            "llama_perf_context_print:       total time =  148611.17 ms /    77 tokens\n",
            "llama_perf_context_print:    graphs reused =         60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(25):\n",
        "  print(8)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulYgFT6pzSaA",
        "outputId": "d3127490-9e2d-47dd-f56d-2b4aef3521de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!wget https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-bf16.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UP1GS_W_zT5I",
        "outputId": "3f267659-6432-4ec5-80f9-42dd14c6e644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "--2025-08-06 19:24:33--  https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-bf16.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.166.152.110, 3.166.152.44, 3.166.152.105, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.166.152.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/6890ab5d6619103494c8a56a/f4660c5052945fd227a247952563cbfa8ba2f4691c5512e1b9309dddf523540b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250806%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250806T192433Z&X-Amz-Expires=3600&X-Amz-Signature=74b0607aca44209f15d15b618b615ced38d63c12fba7c59a31795ed94982bd4a&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27tencent_Hunyuan-7B-Instruct-bf16.gguf%3B+filename%3D%22tencent_Hunyuan-7B-Instruct-bf16.gguf%22%3B&x-id=GetObject&Expires=1754511873&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NDUxMTg3M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODkwYWI1ZDY2MTkxMDM0OTRjOGE1NmEvZjQ2NjBjNTA1Mjk0NWZkMjI3YTI0Nzk1MjU2M2NiZmE4YmEyZjQ2OTFjNTUxMmUxYjkzMDlkZGRmNTIzNTQwYioifV19&Signature=IjQSYH0s0h0S2nSXzukvfsC1UPmI8aUxkwovJJZMhDIuif0-zXsF5%7EEYWpVbmEIBGKYKlPLWE6PKYHmL3htPw5nJ9eNRZG5tyRfqT85Xy1STjLdeVLokkOtnz3DSMZ2fvpeA3wmDdiBlh9QUsTUtCupgv0SYeIZmu4xp69y8ZDdFVwtlazzdo184R4UNUcxMn4sXImWUjsn9YB7cPE0p-Aqua6GaT0cXAE9GkmRGOuehfbrMm%7EhZKW-3mvI%7EwP5dCN2A18vanPot1lixECi-NAv9H83ZM9hEreA1WXkj-IAl2Zx49C5%7EaZ6tWSQGPwj%7E0nPQ5VkXAnlMNZ02RhXkeg__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-08-06 19:24:33--  https://cas-bridge.xethub.hf.co/xet-bridge-us/6890ab5d6619103494c8a56a/f4660c5052945fd227a247952563cbfa8ba2f4691c5512e1b9309dddf523540b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250806%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250806T192433Z&X-Amz-Expires=3600&X-Amz-Signature=74b0607aca44209f15d15b618b615ced38d63c12fba7c59a31795ed94982bd4a&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27tencent_Hunyuan-7B-Instruct-bf16.gguf%3B+filename%3D%22tencent_Hunyuan-7B-Instruct-bf16.gguf%22%3B&x-id=GetObject&Expires=1754511873&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NDUxMTg3M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODkwYWI1ZDY2MTkxMDM0OTRjOGE1NmEvZjQ2NjBjNTA1Mjk0NWZkMjI3YTI0Nzk1MjU2M2NiZmE4YmEyZjQ2OTFjNTUxMmUxYjkzMDlkZGRmNTIzNTQwYioifV19&Signature=IjQSYH0s0h0S2nSXzukvfsC1UPmI8aUxkwovJJZMhDIuif0-zXsF5%7EEYWpVbmEIBGKYKlPLWE6PKYHmL3htPw5nJ9eNRZG5tyRfqT85Xy1STjLdeVLokkOtnz3DSMZ2fvpeA3wmDdiBlh9QUsTUtCupgv0SYeIZmu4xp69y8ZDdFVwtlazzdo184R4UNUcxMn4sXImWUjsn9YB7cPE0p-Aqua6GaT0cXAE9GkmRGOuehfbrMm%7EhZKW-3mvI%7EwP5dCN2A18vanPot1lixECi-NAv9H83ZM9hEreA1WXkj-IAl2Zx49C5%7EaZ6tWSQGPwj%7E0nPQ5VkXAnlMNZ02RhXkeg__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.173.166.74, 18.173.166.82, 18.173.166.85, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.173.166.74|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15014548224 (14G)\n",
            "Saving to: ‘tencent_Hunyuan-7B-Instruct-bf16.gguf’\n",
            "\n",
            "tencent_Hunyuan-7B- 100%[===================>]  13.98G  73.3MB/s    in 2m 49s  \n",
            "\n",
            "2025-08-06 19:27:23 (84.7 MB/s) - ‘tencent_Hunyuan-7B-Instruct-bf16.gguf’ saved [15014548224/15014548224]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/a\n",
        "!build/bin/llama-cli -m /content/tencent_Hunyuan-7B-Instruct-bf16.gguf -p \"Write a Python code to print the number 8 twenty-five times.\" -n 64 -no-cnv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-xw8n8w16ow",
        "outputId": "7d95fb45-9a70-4826-ee22-85569aa4b465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/a\n",
            "load_backend: loaded RPC backend from /content/a/build/bin/libggml-rpc.so\n",
            "load_backend: loaded CPU backend from /content/a/build/bin/libggml-cpu-haswell.so\n",
            "build: 6096 (fd1234cb) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 354 tensors from /content/tencent_Hunyuan-7B-Instruct-bf16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = hunyuan-dense\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Hunyuan 7B Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Hunyuan\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 7B\n",
            "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   7:                  general.base_model.0.name str              = Hunyuan 7B Pretrain\n",
            "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Tencent\n",
            "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/tencent/Hunyua...\n",
            "llama_model_loader: - kv  10:                  hunyuan-dense.block_count u32              = 32\n",
            "llama_model_loader: - kv  11:               hunyuan-dense.context_length u32              = 262144\n",
            "llama_model_loader: - kv  12:             hunyuan-dense.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  13:          hunyuan-dense.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  14:         hunyuan-dense.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  15:      hunyuan-dense.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  16:               hunyuan-dense.rope.freq_base f32              = 11158840.000000\n",
            "llama_model_loader: - kv  17: hunyuan-dense.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  18:         hunyuan-dense.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  19:       hunyuan-dense.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  20:                          general.file_type u32              = 32\n",
            "llama_model_loader: - kv  21:            hunyuan-dense.rope.scaling.type str              = none\n",
            "llama_model_loader: - kv  22:          hunyuan-dense.rope.scaling.factor f32              = 1.000000\n",
            "llama_model_loader: - kv  23: hunyuan-dense.rope.scaling.original_context_length u32              = 262144\n",
            "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  25:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  26:                         tokenizer.ggml.pre str              = hunyuan\n",
            "llama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,128167]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  28:                  tokenizer.ggml.token_type arr[i32,128167]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  29:                      tokenizer.ggml.merges arr[str,127698]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 127958\n",
            "llama_model_loader: - kv  31:                tokenizer.ggml.eos_token_id u32              = 127960\n",
            "llama_model_loader: - kv  32:          tokenizer.ggml.seperator_token_id u32              = 127962\n",
            "llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 127961\n",
            "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {%- if not add_generation_prompt is d...\n",
            "llama_model_loader: - type  f32:  129 tensors\n",
            "llama_model_loader: - type bf16:  225 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = BF16\n",
            "print_info: file size   = 13.98 GiB (16.00 BPW) \n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: printing all EOG tokens:\n",
            "load:   - 127957 ('<|endoftext|>')\n",
            "load:   - 127960 ('<|eos|>')\n",
            "load: special tokens cache size = 210\n",
            "load: token to piece cache size = 0.7868 MB\n",
            "print_info: arch             = hunyuan-dense\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 262144\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 14336\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = none\n",
            "print_info: freq_base_train  = 11158840.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 262144\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 7B\n",
            "print_info: model params     = 7.50 B\n",
            "print_info: general.name     = Hunyuan 7B Instruct\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 128167\n",
            "print_info: n_merges         = 127698\n",
            "print_info: BOS token        = 127958 '<|startoftext|>'\n",
            "print_info: EOS token        = 127960 '<|eos|>'\n",
            "print_info: EOT token        = 127957 '<|endoftext|>'\n",
            "print_info: SEP token        = 127962 '<|extra_0|>'\n",
            "print_info: PAD token        = 127961 '<|pad|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: EOG token        = 127957 '<|endoftext|>'\n",
            "print_info: EOG token        = 127960 '<|eos|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: offloading 0 repeating layers to GPU\n",
            "load_tensors: offloaded 0/33 layers to GPU\n",
            "load_tensors:   CPU_Mapped model buffer size = 14314.35 MiB\n",
            "...............................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 2048\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 11158840.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (262144) -- the full capacity of the model will not be utilized\n",
            "llama_context:        CPU  output buffer size =     0.49 MiB\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   512.00 MiB\n",
            "llama_kv_cache_unified: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
            "llama_context:        CPU compute buffer size =   300.01 MiB\n",
            "llama_context: graph nodes  = 1254\n",
            "llama_context: graph splits = 1\n",
            "common_init_from_params: added <|endoftext|> logit bias = -inf\n",
            "common_init_from_params: added <|eos|> logit bias = -inf\n",
            "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: llama threadpool init, n_threads = 1\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "\n",
            "sampler seed: 799941061\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n",
            "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
            "generate: n_ctx = 4096, n_batch = 2048, n_predict = 64, n_keep = 0\n",
            "\n",
            "Write a Python code to print the number 8 twenty-five times. To To To To To To To To To To To\n",
            "llama_perf_sampler_print:    sampling time =      71.91 ms /    25 runs   (    2.88 ms per token,   347.66 tokens per second)\n",
            "llama_perf_context_print:        load time =  198352.28 ms\n",
            "llama_perf_context_print: prompt eval time =   96308.49 ms /    14 tokens ( 6879.18 ms per token,     0.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =  624270.99 ms /    10 runs   (62427.10 ms per token,     0.02 tokens per second)\n",
            "llama_perf_context_print:       total time =  721256.06 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =         10\n",
            "Interrupted by user\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aws-shell"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4Dv_Ahbs2zTw",
        "outputId": "c6604f1e-4c1d-4a9e-d6a7-abefed3e8f01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aws-shell\n",
            "  Downloading aws_shell-0.2.2-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting awscli<2.0.0,>=1.16.10 (from aws-shell)\n",
            "  Downloading awscli-1.42.4-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting prompt-toolkit<1.1.0,>=1.0.0 (from aws-shell)\n",
            "  Downloading prompt_toolkit-1.0.18-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting boto3<2.0.0,>=1.9.0 (from aws-shell)\n",
            "  Downloading boto3-1.40.4-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting configobj<6.0.0,>=5.0.6 (from aws-shell)\n",
            "  Downloading configobj-5.0.9-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: Pygments<3.0.0,>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from aws-shell) (2.19.2)\n",
            "Collecting botocore==1.40.4 (from awscli<2.0.0,>=1.16.10->aws-shell)\n",
            "  Downloading botocore-1.40.4-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting docutils<=0.19,>=0.18.1 (from awscli<2.0.0,>=1.16.10->aws-shell)\n",
            "  Downloading docutils-0.19-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from awscli<2.0.0,>=1.16.10->aws-shell)\n",
            "  Downloading s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: PyYAML<6.1,>=3.10 in /usr/local/lib/python3.11/dist-packages (from awscli<2.0.0,>=1.16.10->aws-shell) (6.0.2)\n",
            "Collecting colorama<0.4.7,>=0.2.5 (from awscli<2.0.0,>=1.16.10->aws-shell)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting rsa<4.8,>=3.1.2 (from awscli<2.0.0,>=1.16.10->aws-shell)\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from botocore==1.40.4->awscli<2.0.0,>=1.16.10->aws-shell)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore==1.40.4->awscli<2.0.0,>=1.16.10->aws-shell) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore==1.40.4->awscli<2.0.0,>=1.16.10->aws-shell) (2.5.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit<1.1.0,>=1.0.0->aws-shell) (1.17.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit<1.1.0,>=1.0.0->aws-shell) (0.2.13)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from rsa<4.8,>=3.1.2->awscli<2.0.0,>=1.16.10->aws-shell) (0.6.1)\n",
            "Downloading aws_shell-0.2.2-py2.py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading awscli-1.42.4-py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.40.4-py3-none-any.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.40.4-py3-none-any.whl (140 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading configobj-5.0.9-py2.py3-none-any.whl (35 kB)\n",
            "Downloading prompt_toolkit-1.0.18-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.4/245.4 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading docutils-0.19-py3-none-any.whl (570 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m570.5/570.5 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Downloading s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rsa, prompt-toolkit, jmespath, docutils, configobj, colorama, botocore, s3transfer, boto3, awscli, aws-shell\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.9.1\n",
            "    Uninstalling rsa-4.9.1:\n",
            "      Successfully uninstalled rsa-4.9.1\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt_toolkit 3.0.51\n",
            "    Uninstalling prompt_toolkit-3.0.51:\n",
            "      Successfully uninstalled prompt_toolkit-3.0.51\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.21.2\n",
            "    Uninstalling docutils-0.21.2:\n",
            "      Successfully uninstalled docutils-0.21.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "jupyter-console 6.1.0 requires prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0, but you have prompt-toolkit 1.0.18 which is incompatible.\n",
            "sphinx 8.2.3 requires docutils<0.22,>=0.20, but you have docutils 0.19 which is incompatible.\n",
            "ipython 7.34.0 requires prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0, but you have prompt-toolkit 1.0.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aws-shell-0.2.2 awscli-1.42.4 boto3-1.40.4 botocore-1.40.4 colorama-0.4.6 configobj-5.0.9 docutils-0.19 jmespath-1.0.1 prompt-toolkit-1.0.18 rsa-4.7.2 s3transfer-0.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "prompt_toolkit"
                ]
              },
              "id": "c7315a189ffc4dd3901cc8030b543ead"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!aws-shell"
      ],
      "metadata": {
        "id": "o8UvSrZN7iiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gS-ha0RC7wTa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}