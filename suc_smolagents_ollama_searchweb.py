# -*- coding: utf-8 -*-
"""suc_smolagents_ollama_searchweb.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ja-1JRE8I5RvL4G_WM-iYT9TqsiS8LeC
"""















"""https://dataisadope.com/blog/build-your-first-local-ai-agent/#:~:text=Learn%20how%20to%20build%20a%20powerful%20AI%20agent,problems%20-%20from%20complex%20calculations%20to%20multi-step%20reasoning."""



!pip install smolagents[litellm]

!curl -fsSL https://ollama.com/install.sh | sh

!nohup ollama serve &
!ollama pull qwen2:7b

from smolagents import LiteLLMModel, CodeAgent

model = LiteLLMModel(
    model_id="ollama_chat/qwen2:7b",
    api_base="http://127.0.0.1:11434",
    num_ctx=8192,
)

agent = CodeAgent(tools=[], model=model)
response = agent.run("Tell me a joke")
print(response)

"""### شغال بحث نت بدون مشاكل"""

import torch
from smolagents import CodeAgent, TransformersModel, DuckDuckGoSearchTool

# It's good practice to empty the cache before loading a large model
torch.cuda.empty_cache()

from smolagents import LiteLLMModel, CodeAgent

model = LiteLLMModel(
    model_id="ollama_chat/qwen2:7b",
    api_base="http://127.0.0.1:11434",
    num_ctx=8192,
)



# 2. Create the CodeAgent with the search tool
agent = CodeAgent(
    tools=[DuckDuckGoSearchTool()],
    model=model,
    add_base_tools=True
)

# 3. Run the agent with the task that includes the search instruction
# The agent will use the DuckDuckGoSearchTool based on the task description.
print("=== CodeAgent Search and Summarize Test ===")
result = agent.run("""
Search for the latest news about artificial intelligence and provide a summary.
""")

print("\n=== Final Result ===")
print(result)





















# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
tokenizer = AutoTokenizer.from_pretrained("unsloth/gpt-oss-20b-unsloth-bnb-4bit")
model = AutoModelForCausalLM.from_pretrained("unsloth/gpt-oss-20b-unsloth-bnb-4bit", torch_dtype="auto", device_map="auto")
messages = [
    {"role": "user", "content": "Who are you?"},
]
inputs = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt",
).to(model.device)

outputs = model.generate(**inputs, max_new_tokens=40)
print(tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:]))

# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("text-generation", model="unsloth/gpt-oss-20b-unsloth-bnb-4bit")
messages = [
    {"role": "user", "content": "Who are you?"},
]
pipe(messages)

# Commented out IPython magic to ensure Python compatibility.
# %pip install bitsandbytes

!pip install -U kernels

!pip install autoawq

from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer

model_name_or_path = "TheBloke/Mistral-7B-Instruct-v0.2-AWQ"

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
model = AutoModelForCausalLM.from_pretrained(
    model_name_or_path,
    low_cpu_mem_usage=True,
    device_map="cuda:0"
)

# Using the text streamer to stream output one token at a time
streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

prompt = "Tell me about AI"
prompt_template=f'''<s>[INST] {prompt} [/INST]
'''

# Convert prompt to tokens
tokens = tokenizer(
    prompt_template,
    return_tensors='pt'
).input_ids.cuda()

generation_params = {
    "do_sample": True,
    "temperature": 0.7,
    "top_p": 0.95,
    "top_k": 40,
    "max_new_tokens": 4,
    "repetition_penalty": 1.1
}

# Generate streamed output, visible one token at a time
generation_output = model.generate(
    tokens,
    streamer=streamer,
    **generation_params
)

# Generation without a streamer, which will include the prompt in the output
generation_output = model.generate(
    tokens,
    **generation_params
)

# Get the tokens from the output, decode them, print them
token_output = generation_output[0]
text_output = tokenizer.decode(token_output)
print("model.generate output: ", text_output)

# Inference is also possible via Transformers' pipeline
from transformers import pipeline

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    **generation_params
)

pipe_output = pipe(prompt_template)[0]['generated_text']
print("pipeline output: ", pipe_output)

!pip install smolagents[transformers]

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade autoawq

!pip install ddgs

from smolagents import CodeAgent, TransformersModel, DuckDuckGoSearchTool, tool
import requests
from bs4 import BeautifulSoup

# Custom tool to fetch web page content
@tool
def fetch_web_content(url: str) -> str:
    """
    Fetch web page content from a given URL and extract the text.

    Args:
        url (str): The URL of the web page to fetch content from.

    Returns:
        str: The extracted text content from the web page (first 2000 characters).
    """
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        # Remove scripts and styles
        for script in soup(["script", "style"]):
            script.decompose()
        return soup.get_text()[:2000]  # First 2000 characters only
    except Exception as e:
        return f"Error fetching content: {str(e)}"

# 1. Set up local model
model = TransformersModel(
    model_id="TheBloke/Mistral-7B-Instruct-v0.2-AWQ",
    max_new_tokens=111,  # Increased from 11 which is too small
    device_map="auto",
    temperature=0.1,
    top_p=0.9
)

# 2. Create agent with multiple tools
agent = CodeAgent(
    tools=[DuckDuckGoSearchTool(), fetch_web_content],
    model=model
)

# 3. Run agent with advanced tasks
print("=== Advanced Example: Search and Analysis ===")
result = agent.run(
    task="""
Search for the latest AI news, then analyze the results and provide a summary
of key developments in artificial intelligence
""",
    additional_args={"messages": []} # Explicitly provide empty messages
)
print(f"Result: {result}")

print("\n=== Example: Comparative Search ===")
result2 = agent.run(
    task="""
Compare the performance of GPT-4 and Claude 3 models on programming tasks
Find recent information and provide a detailed comparison
""",
    additional_args={"messages": []} # Explicitly provide empty messages
)
print(f"Result: {result2}")

!pip install smolagents[toolkit] transformers torch requests beautifulsoup4

!pip list | grep -E "(smolagents|transformers|torch)"

from smolagents import CodeAgent, LiteLLMModel, DuckDuckGoSearchTool

# 1. Set up model using Ollama
model = LiteLLMModel(
    model_id="ollama_chat/mistral:7b-instruct-v0.2-q4_0",
    api_base="http://localhost:11434",
    temperature=0.1,
    max_tokens=512
)

# 2. Create agent
agent = CodeAgent(
    tools=[DuckDuckGoSearchTool()],
    model=model,
    add_base_tools=True
)

# 3. Run agent
print("=== Ollama Model Test ===")
result = agent.run("Search for recent AI developments")
print(f"Result: {result}")

from smolagents import CodeAgent, TransformersModel, DuckDuckGoSearchTool

# 1. Set up local model with a smaller model
model = TransformersModel(
    model_id="HuggingFaceTB/SmolLM-135M-Instruct",
    max_new_tokens=512,
    device_map="auto",
    temperature=0.1,
    top_p=0.9
)

# 2. Create agent with just the search tool
agent = CodeAgent(
    tools=[DuckDuckGoSearchTool()],
    model=model,
    add_base_tools=True
)

# 3. Run agent with a simple task
print("=== Simple Search Test ===")
result = agent.run("What is the latest news about artificial intelligence?")
print(f"Result: {result}")

from smolagents import CodeAgent, TransformersModel, DuckDuckGoSearchTool, tool
import requests
from bs4 import BeautifulSoup

# Custom tool to fetch web page content
@tool
def fetch_web_content(url: str) -> str:
    """
    Fetch web page content from a given URL and extract the text.
    Args:
        url (str): The URL of the web page to fetch content from.
    Returns:
        str: The extracted text content from the web page (first 2000 characters).
    """
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        # Remove scripts and styles
        for script in soup(["script", "style"]):
            script.decompose()
        return soup.get_text()[:2000]  # First 2000 characters only
    except Exception as e:
        return f"Error fetching content: {str(e)}"

# 1. Set up local model with a different model ID
model = TransformersModel(
    model_id="HuggingFaceH4/zephyr-7b-beta",  # نموذج بديل معروف بالتوافق
    max_new_tokens=512,  # زيادة عدد الرموز المسموح بها
    device_map="auto",
    temperature=0.1,
    top_p=0.9,
    trust_remote_code=True  # إضافة هذه المعلمة للنماذج التي تتطلبها
)

# 2. Create agent with multiple tools
agent = CodeAgent(
    tools=[DuckDuckGoSearchTool(), fetch_web_content],
    model=model,
    add_base_tools=True
)

# 3. Run agent with a simple task first
print("=== Simple Test ===")
result = agent.run("Search for recent AI news and provide a brief summary")
print(f"Result: {result}")



from smolagents import CodeAgent, TransformersModel, DuckDuckGoSearchTool

# 1. Set up local model with a smaller model
model = TransformersModel(
    model_id="Qwen/Qwen3-4B-Instruct-2507",
    max_new_tokens=51,
    device_map="auto",
    temperature=0.1,
    top_p=0.9
)

# 2. Create agent with just the search tool
agent = CodeAgent(
    tools=[DuckDuckGoSearchTool()],
    model=model,
    add_base_tools=True
)

# 3. Run agent with a simple task
print("=== Simple Search Test ===")
result = agent.run("What is the latest news about artificial intelligence?")
print(f"Result: {result}")

torch.cuda.empty_cache()

from smolagents import CodeAgent, TransformersModel, DuckDuckGoSearchTool
import torch
torch.cuda.empty_cache()
# 1. Set up local model with a smaller model
model = TransformersModel(
    model_id="Qwen/Qwen3-4B-Instruct-2507",
    max_new_tokens=51,
    torch_dtype="auto",
    device_map="auto",
    temperature=0.1,
    top_p=0.9
)
torch.cuda.empty_cache()
# 2. Create agent with just the search tool
agent = CodeAgent(
    tools=[DuckDuckGoSearchTool()],
    model=model,
    add_base_tools=True
)
torch.cuda.empty_cache()
# 3. Run agent with a simple task
print("=== Simple Search Test ===")
result = agent.run("What is the latest news about artificial intelligence?")
print(f"Result: {result}")





import os
import torch
from smolagents import CodeAgent, TransformersModel, DuckDuckGoSearchTool

# إعداد متغيرات البيئة لتحسين إدارة الذاكرة
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# مسح ذاكرة الـ GPU
torch.cuda.empty_cache()

# 1. Set up local model with memory optimization
model = TransformersModel(
    model_id="HuggingFaceTB/SmolLM-135M-Instruct",  # أصغر نموذج ممكن
    max_new_tokens=128,  # تقليل عدد الرموز المسموح بها
    device_map="auto",
    temperature=0.1,
    top_p=0.9,
    torch_dtype=torch.float16  # استخدام float16 لتوفير الذاكرة
)

# 2. Create agent
agent = CodeAgent(
    tools=[DuckDuckGoSearchTool()],
    model=model,
    add_base_tools=True
)

# 3. Run agent with a very simple task
print("=== Memory Optimized Test ===")
result = agent.run("Latest AI news")
print(f"Result: {result}")

from smolagents import CodeAgent, LiteLLMModel, DuckDuckGoSearchTool

# 1. Set up model using Ollama (يحتاج تثبيت Ollama مسبقاً)
model = LiteLLMModel(
    model_id="ollama_chat/llama3:8b-instruct-q4_0",
    api_base="http://localhost:11434",
    temperature=0.1,
    max_tokens=256
)

# 2. Create agent
agent = CodeAgent(
    tools=[DuckDuckGoSearchTool()],
    model=model,
    add_base_tools=True
)

# 3. Run agent
print("=== Ollama Model Test ===")
result = agent.run("What is the latest news about artificial intelligence?")
print(f"Result: {result}")

import torch
from smolagents import CodeAgent, TransformersModel, DuckDuckGoSearchTool

# مسح ذاكرة الـ GPU
torch.cuda.empty_cache()

# 1. Set up local model
model = TransformersModel(
    model_id="HuggingFaceTB/SmolLM-135M-Instruct",  # نموذج صغير
    max_new_tokens=256,
    device_map="auto",
    temperature=0.1,
    top_p=0.9
)

# 2. Create agent
agent = CodeAgent(
    tools=[DuckDuckGoSearchTool()],
    model=model,
    add_base_tools=True
)

# 3. Run agent
print("=== Cleared GPU Memory Test ===")
result = agent.run("What is the latest news about artificial intelligence?")
print(f"Result: {result}")

from smolagents import CodeAgent, TransformersModel, DuckDuckGoSearchTool

# 1. Set up local model forcing CPU usage
model = TransformersModel(
    model_id="Qwen/Qwen3-4B-Instruct-2507",
    max_new_tokens=256,
    device_map="cpu",  # استخدام الـ CPU بدلاً من الـ GPU
    temperature=0.1,
    top_p=0.9
)

# 2. Create agent
agent = CodeAgent(
    tools=[DuckDuckGoSearchTool()],
    model=model,
    add_base_tools=True
)

# 3. Run agent
print("=== CPU-based Search Test ===")
result = agent.run("What is the latest news about artificial intelligence?")
print(f"Result: {result}")

from smolagents import CodeAgent, TransformersModel, DuckDuckGoSearchTool
import torch
torch.cuda.empty_cache()
# 1. Set up local model with a smaller model
model = TransformersModel(
    model_id="Qwen/Qwen3-4B-Instruct-2507",
    max_new_tokens=51,
    torch_dtype="auto",
    device_map="auto",
    temperature=0.1,
    top_p=0.9
)
torch.cuda.empty_cache()
# 2. Create agent with just the search tool
agent = CodeAgent(
    tools=[DuckDuckGoSearchTool()],
    model=model,
    add_base_tools=True
)
torch.cuda.empty_cache()
# 3. Run agent with a simple task
print("=== Simple Search Test ===")
result = agent.run("What is the latest news about artificial intelligence?")
print(f"Result: {result}")

andresnowak/Qwen3-0.6B-instruction-finetuned

# Install necessary libraries
!pip install -U "smol-agents" "transformers" "torch" "accelerate" "bitsandbytes"

!pip install -U "bitsandbytes"

# Install necessary libraries
#!pip install -U "smol-agents" "transformers" "torch" "accelerate" "bitsandbytes"

import torch
from smolagents import ReactAgent, TransformersModel, DuckDuckGoSearchTool

# Clear the GPU cache before loading the model
torch.cuda.empty_cache()

# 1. Set up the local model with 8-bit quantization to save memory
model = TransformersModel(
    model_id="Qwen/Qwen3-4B-Instruct-2507",
    load_in_8bit=True,  # Load in 8-bit to reduce memory footprint
    max_new_tokens=512, # Increased token limit for a more detailed summary
    temperature=0.1,
    top_p=0.9
)

# 2. Use ReactAgent for a search and summarize task
# This agent is better suited for reasoning and acting upon information from tools.
agent = ReactAgent(
    tools=[DuckDuckGoSearchTool()],
    model=model,
)

# Clear the cache again before running the agent
torch.cuda.empty_cache()

# 3. Run the agent with the specified task
print("=== Search and Summarize Test ===")
result = agent.run("What is the latest news about artificial intelligence?")
print(f"Result: {result}")

# Install necessary libraries
#!pip install -U "smol-agents" "transformers" "torch" "accelerate" "bitsandbytes"

import torch
# Corrected import for ReactAgent
from smolagents.agents import ReactAgent
from smolagents import TransformersModel, DuckDuckGoSearchTool

# Clear the GPU cache before loading the model
torch.cuda.empty_cache()

# 1. Set up the local model with 8-bit quantization to save memory
# This is crucial for T4's 15GB VRAM, especially with models like Qwen.
model = TransformersModel(
    model_id="Qwen/Qwen3-4B-Instruct-2507",
    load_in_8bit=True,  # Load in 8-bit to reduce memory footprint
    max_new_tokens=512, # Increased token limit for a more detailed summary
    temperature=0.1,
    top_p=0.9
)

# 2. Use ReactAgent for a search and summarize task
# This agent is better suited for reasoning and acting upon information from tools,
# and will be more robust in generating natural language summaries after search,
# rather than strictly requiring code execution like CodeAgent.
agent = ReactAgent(
    tools=[DuckDuckGoSearchTool()],
    model=model,
    # add_base_tools is not a parameter for ReactAgent directly.
    # ReactAgent includes reasoning capabilities by default.
)

# Clear the cache again before running the agent
torch.cuda.empty_cache()

# 3. Run the agent with the specified task
print("=== Search and Summarize Test ===")
result = agent.run("What is the latest news about artificial intelligence?")
print(f"Result: {result}")

# Install necessary libraries
!pip install -U "smol-agents" "transformers" "torch" "accelerate" "bitsandbytes"

import torch
from smolagents import TransformersModel, DuckDuckGoSearchTool
# Corrected import: import ReactAgent from smolagents.agents
from smolagents.agents import ReactAgent

# Clear the GPU cache before loading the model
torch.cuda.empty_cache()

# 1. Set up the local model with 8-bit quantization to save memory
model = TransformersModel(
    model_id="Qwen/Qwen3-4B-Instruct-2507",
    load_in_8bit=True,  # Load in 8-bit to reduce memory footprint
    max_new_tokens=512, # Increased token limit for a more detailed summary
    temperature=0.1,
    top_p=0.9
)

# 2. Use ReactAgent for a search and summarize task
# This agent is better suited for reasoning and acting upon information from tools.
agent = ReactAgent(
    tools=[DuckDuckGoSearchTool()],
    model=model,
    # add_base_tools=True # Only necessary if you want default tools like terminal/file operations
)

# Clear the cache again before running the agent
torch.cuda.empty_cache()

# 3. Run the agent with the specified task
print("=== Search and Summarize Test ===")
result = agent.run("What is the latest news about artificial intelligence?")
print(f"Result: {result}")

# Install necessary libraries if you haven't already
!pip install -U "smol-agents" "transformers" "torch" "accelerate" "bitsandbytes"

import torch
from smolagents import CodeAgent, TransformersModel, DuckDuckGoSearchTool

# It's good practice to empty the cache before loading a large model
torch.cuda.empty_cache()

# 1. Set up local model, loading in 8-bit to conserve memory
# This significantly reduces the GPU memory footprint.
model = TransformersModel(
    model_id="Qwen/Qwen3-4B-Instruct-2507",
    load_in_8bit=True,
    max_new_tokens=1024, # Increased token limit for a more complete summary
    torch_dtype="auto",
    device_map="auto",
    temperature=0.1,
    top_p=0.9
)

# 2. Create the CodeAgent with the search tool
agent = CodeAgent(
    tools=[DuckDuckGoSearchTool()],
    model=model,
    add_base_tools=True
)

# 3. Create an explicit prompt to guide the model
# By providing a clear instruction and format, we guide the model
# to produce the output the CodeAgent can successfully parse.
prompt = """
Thought: I need to find the latest news about artificial intelligence and then present it as a summary. I will use the search tool to find the information and then use a print statement in a Python code block to output the final answer.
"""
# Use the search tool to find recent articles and news about AI.
news_summary = DuckDuckGoSearchTool().run("latest news artificial intelligence summary")
# Print the findings as the final result.
print(news_summary)



# 1. Install the correct and necessary libraries
# We need transformers for the model, accelerate and bitsandbytes for efficient loading,
# and the various langchain packages for the agent framework.
!pip install -U langchain langchain_community langchain_huggingface transformers accelerate bitsandbytes torch

import torch
from langchain_community.tools import DuckDuckGoSearchRun
from langchain_huggingface import HuggingFacePipeline
from langchain import hub
from langchain.agents import create_react_agent, AgentExecutor

# It's good practice to clear the GPU cache before loading a large model
torch.cuda.empty_cache()

# 2. Set up the local model pipeline using HuggingFacePipeline
# We load the model in 8-bit precision to ensure it fits on the T4 GPU.
llm = HuggingFacePipeline.from_model_id(
    model_id="Qwen/Qwen2-1.5B-Instruct", # Using a slightly smaller Qwen model for stability
    task="text-generation",
    device_map="auto",
    model_kwargs={
        "torch_dtype": torch.bfloat16,
        "load_in_8bit": True,
    },
    pipeline_kwargs={
        "max_new_tokens": 512,
        "top_p": 0.9,
        "temperature": 0.1,
    },
)

# 3. Set up the tools the agent can use
# For langchain, the standard tool is DuckDuckGoSearchRun.
tools = [DuckDuckGoSearchRun()]

# 4. Create the Agent using a standard ReAct prompt template
# ReAct (Reasoning and Acting) is a reliable prompting strategy for agents.
# We pull a pre-built, effective prompt from the LangChain Hub.
prompt = hub.pull("hwchase17/react")

# 5. Create the agent and the agent executor
# The agent is the "brain" that decides what to do.
# The executor is what actually runs the agent and its tools.
agent = create_react_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# 6. Run the agent with a simple, direct prompt
print("=== LangChain Agent Search Test ===")
question = "What is the latest news about artificial intelligence?"
result = agent_executor.invoke({"input": question})

print("\n=== Final Result ===")
print(result)

# Install necessary libraries if you haven't already
!pip install -U "smol-agents" "transformers" "torch" "accelerate" "bitsandbytes"

import torch
from smolagents import CodeAgent, TransformersModel, DuckDuckGoSearchTool

# It's good practice to empty the cache before loading a large model
torch.cuda.empty_cache()

# 1. Set up local model, loading in 8-bit to conserve memory
# This significantly reduces the GPU memory footprint.
model = TransformersModel(
    model_id="Qwen/Qwen2-1.5B-Instruct",
    load_in_8bit=True,  # Added back load_in_8bit
    max_new_tokens=1024, # Increased token limit for a more complete summary
    torch_dtype="auto",
    device_map="auto",
    temperature=0.1,
    top_p=0.9
)

# 2. Create the CodeAgent with the search tool
agent = CodeAgent(
    tools=[DuckDuckGoSearchTool()],
    model=model,
    add_base_tools=True
)

# 3. Run the agent with the task that includes the search instruction
# The agent will use the DuckDuckGoSearchTool based on the task description.
print("=== CodeAgent Search and Summarize Test ===")
result = agent.run("""
Search for the latest news about artificial intelligence and provide a summary.
""")

print("\n=== Final Result ===")
print(result)

from smolagents import CodeAgent, TransformersModel, DuckDuckGoSearchTool, tool
import requests
from bs4 import BeautifulSoup

# Custom tool to fetch web page content
@tool
def fetch_web_content(url: str) -> str:
    """
    Fetch web page content from a given URL and extract the text.

    Args:
        url (str): The URL of the web page to fetch content from.

    Returns:
        str: The extracted text content from the web page (first 2000 characters).
    """
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        # Remove scripts and styles
        for script in soup(["script", "style"]):
            script.decompose()
        return soup.get_text()[:2000]  # First 2000 characters only
    except Exception as e:
        return f"Error fetching content: {str(e)}"

# 1. Set up local model
model = TransformersModel(
    model_id="Qwen/Qwen2-1.5B-Instruct",
    max_new_tokens=512,  # Increased from 11 which is too small
    device_map="auto",
    temperature=0.1,
    top_p=0.9
)

# 2. Create agent with multiple tools
agent = CodeAgent(
    tools=[DuckDuckGoSearchTool(), fetch_web_content],
    model=model,
    add_base_tools=True
)

# 3. Run agent with advanced tasks
print("=== Advanced Example: Search and Analysis ===")
result = agent.run("""
Search for the latest AI news, then analyze the results and provide a summary
of key developments in artificial intelligence
""")
print(f"Result: {result}")

print("\n=== Example: Comparative Search ===")
result2 = agent.run("""
Compare the performance of GPT-4 and Claude 3 models on programming tasks
Find recent information and provide a detailed comparison
""")
print(f"Result: {result2}")







"""https://dataisadope.com/blog/build-your-first-local-ai-agent/#:~:text=Learn%20how%20to%20build%20a%20powerful%20AI%20agent,problems%20-%20from%20complex%20calculations%20to%20multi-step%20reasoning."""

!pip install smolagents[litellm]

curl -fsSL https://ollama.com/install.sh | sh
nohup ollama serve &
ollama pull llama3:8b
ollama list

!curl -fsSL https://ollama.com/install.sh | sh

!nohup ollama serve &
!ollama pull qwen2:7b

from smolagents import LiteLLMModel, CodeAgent

model = LiteLLMModel(
    model_id="ollama_chat/qwen2:7b",
    api_base="http://127.0.0.1:11434",
    num_ctx=8192,
)

agent = CodeAgent(tools=[], model=model)
response = agent.run("Tell me a joke")
print(response)

"""### شغال بحث نت بدون مشاكل"""

import torch
from smolagents import CodeAgent, TransformersModel, DuckDuckGoSearchTool

# It's good practice to empty the cache before loading a large model
torch.cuda.empty_cache()

from smolagents import LiteLLMModel, CodeAgent

model = LiteLLMModel(
    model_id="ollama_chat/qwen2:7b",
    api_base="http://127.0.0.1:11434",
    num_ctx=8192,
)



# 2. Create the CodeAgent with the search tool
agent = CodeAgent(
    tools=[DuckDuckGoSearchTool()],
    model=model,
    add_base_tools=True
)

# 3. Run the agent with the task that includes the search instruction
# The agent will use the DuckDuckGoSearchTool based on the task description.
print("=== CodeAgent Search and Summarize Test ===")
result = agent.run("""
Search for the latest news about artificial intelligence and provide a summary.
""")

print("\n=== Final Result ===")
print(result)