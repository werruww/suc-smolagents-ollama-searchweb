# -*- coding: utf-8 -*-
"""suc_RAG_pdf_assistant_cpu.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iJxl29Uvcx4-DyQYtdrrt0sw5vrEWpcg
"""





curl -fsSL https://ollama.com/install.sh | sh
nohup ollama serve &
ollama pull llama3:8b
ollama list

!curl -fsSL https://ollama.com/install.sh | sh

!nohup ollama serve &

!ollama run gemma3:4b

!git clone https://github.com/miekadal4/RAG_pdf_assistant.git

# Commented out IPython magic to ensure Python compatibility.
# %cd RAG_pdf_assistant

!pip install -r requirements.txt

!pip install pyngrok

!pip install pyngrok

2qoMyvN33sChLDfKL39OR8IDIxk_2hoKPdPQp4MUdSQbDPv6d
2qoMyvN33sChLDfKL39OR8IDIxk_2hoKPdPQp4MUdSQbDPv6d

from pyngrok import ngrok
import threading
import time

ngrok_token = "2qoMyvN33sChLDfKL39OR8IDIxk_2hoKPdPQp4MUdSQbDPv6d"
ngrok.set_auth_token(ngrok_token)

# Start ngrok in a separate thread to avoid blocking
def start_ngrok():
    public_url = ngrok.connect(8501).public_url
    print(f"🚀 Ngrok Tunnel Open: {public_url}")

ngrok_thread = threading.Thread(target=start_ngrok)
ngrok_thread.start()

# Wait for ngrok to start (optional)
time.sleep(5)

# Execute your node.js script
!node hello-world.js

!nohup ollama serve &
!ollama pull nomic-embed-text

!nohup ollama serve &

!ollama list

!nohup ollama serve &




from pyngrok import ngrok
import threading
import time

ngrok_token = "2qoMyvN33sChLDfKL39OR8IDIxk_2hoKPdPQp4MUdSQbDPv6d"
ngrok.set_auth_token(ngrok_token)

# Start ngrok in a separate thread to avoid blocking
def start_ngrok():
    public_url = ngrok.connect(8501).public_url
    print(f"🚀 Ngrok Tunnel Open: {public_url}")

ngrok_thread = threading.Thread(target=start_ngrok)
ngrok_thread.start()

# Wait for ngrok to start (optional)
time.sleep(5)

# Execute your node.js script
!streamlit run /content/RAG_pdf_assistant/main.py

/content/RAG_pdf_assistant/pdf_processor.py


import hashlib
import tempfile
import streamlit as st
from streamlit_chat import message

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma

from langchain.chains import ConversationalRetrievalChain
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain.prompts import PromptTemplate
from langchain.retrievers.multi_query import MultiQueryRetriever

# Embedding model used for vectorization
EMBEDDING = "nomic-embed-text:latest"

def enhance_query(vectorstore, mode, model):
    """
    Enhance the retriever with multi-query prompting if Enhanced Mode is enabled.
    """
    if mode:
        query_prompt = PromptTemplate(input_variables=["question"],
                                      template="""
            You are an AI language model assistant.
            Your task is to generate three different versions of the given user question to retrieve relevant documents from a vector database.
            By generating multiple perspectives in the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search.
            Provide these alternative questions separated by newlines.
            Original question: {question}""")

        retriever = MultiQueryRetriever.from_llm(
            vectorstore.as_retriever(),
            model,
            prompt=query_prompt
        )
    else:
        retriever = vectorstore.as_retriever()

    return retriever

def initialize_pdf_rag_chain(pdf, mode, model):
    """
    Load PDF, embed content, and return a conversational retrieval chain.
    """
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(pdf.read())
        pdf_path = tmp_file.name
    # PDF
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=300)
    chunks = splitter.split_documents(docs)

    # Vector database and embedding.
    embeddings = OllamaEmbeddings(model=EMBEDDING)
    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings
    )
    # Retriever
    retriever = enhance_query(vectorstore, mode, model)
    # Chain
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=model,
        retriever=retriever,
        return_source_documents=False
    )
    return qa_chain

def on_input_change():
    """
    Handles new user input and updates chat history.
    """
    question = st.session_state.user_input
    if question:
        result = st.session_state.conversation(
            {"question": question, "chat_history": st.session_state.chat_history}
        )
        answer = result["answer"]
        st.session_state.chat_history.append((question, answer))
        st.session_state.user_input = ""

def on_clear():
    """
    Clears the chat history.
    """
    st.session_state.chat_history.clear()

def get_file_hash(file):
    """
    Generates a unique MD5 hash for the uploaded PDF file.
    """
    file.seek(0)
    file_bytes = file.read()
    file.seek(0)
    return hashlib.md5(file_bytes).hexdigest()

/content/RAG_pdf_assistant/main.py


from pdf_processor import *

def main():
    # Page Setup
    st.set_page_config(page_title="PDF Chat Demo", layout="wide")
    st.title("📄 Open source LLM pdf chatbot")
    st.markdown("`Upload a PDF and chat`")

    # Sidebar Settings
    with st.sidebar:
        st.header("⚙️ Settings")
        model_selection = st.selectbox("Available Models", ["arnold", "qwen3:8b", "gemma3:4b"])
        temperature = st.slider("Temperature", 0.0, 1.0, 0.7)
        mode = st.toggle(label="Enhanced Mode", value = False)
        uploaded_file = st.file_uploader(label="Upload a File to chat!", type=["pdf"])

    # Initialize session state
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    if "user_input" not in st.session_state:
        st.session_state.user_input = ""
    if "last_file_hash" not in st.session_state:
        st.session_state.last_file_hash = None
    if "conversation" not in st.session_state:
        st.session_state.conversation = None
    if "last_settings" not in st.session_state:
        st.session_state.last_settings = {
            "model": model_selection,
            "temperature": temperature,
            "mode": mode
        }

    # PDF and settings handler
    if uploaded_file:
        file_hash = get_file_hash(uploaded_file)

        # Detects if new PDF changed
        is_new_pdf = st.session_state.last_file_hash != file_hash
        # Detects if settings changed:
        settings_changed = (
            st.session_state.last_settings["model"] != model_selection or
            st.session_state.last_settings["temperature"] != temperature or
            st.session_state.last_settings["mode"] != mode
        )

        # Conditions to reinitialize the chain:
        if is_new_pdf or settings_changed:
            st.session_state.last_file_hash = file_hash
            st.session_state.last_settings = {
                "model": model_selection,
                "temperature": temperature,
                "mode": mode
            }
            # Build the model
            model = ChatOllama(model=model_selection, temperature=temperature)

            if is_new_pdf:
                st.session_state.chat_history=[]

            uploaded_file.seek(0)
            st.session_state.conversation = initialize_pdf_rag_chain(uploaded_file, mode, model)

    # Chat UI
    with st.container():
        for i, (q, a) in enumerate(st.session_state.chat_history):
            message(q, is_user=True, key=f"user_{i}", avatar_style="micah")
            message(a, key=f"bot_{i}", avatar_style="identicon")

    # Input + Clear button
    st.text_input("Ask something about the PDF:", key="user_input", on_change=on_input_change)
    st.button("Clear Chat", on_click=on_clear)

if __name__ == "__main__":
    main()

/content/RAG_pdf_assistant/custom_llm_arnold.py


import ollama
"""
This script demonstrates how to interact with the Ollama API.

Functionality:
- Defines a custom assistant model "arnold" based on the "gemma3:4b" model.
- Customizes the assistant's system prompt to be concise, focused, and informative.
- Sets a custom temperature parameter to control response creativity (though not passed in create here).
"""

model = "gemma3:4b"
custom_system = "You are a highly intelligent assistant. Always provide the most important and relevant information to answer my questions directly. Avoid unnecessary explanations, opinions, or details I didn’t ask for. Be concise and focused"
custom_parameter = "temperature 0.7"
ollama.create(model="arnold", from_=model, system=custom_system)

/content/RAG_pdf_assistant/requirements.txt


streamlit>=1.33.0
streamlit-chat
langchain>=0.1.0
langchain-community>=0.0.26
langchain-ollama>=0.1.0
pypdf
chromadb>=0.4.22

